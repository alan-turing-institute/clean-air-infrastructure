{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CleanAir \u00b6 Welcome to our docs site","title":"Cleanair"},{"location":"#cleanair","text":"Welcome to our docs site","title":"CleanAir"},{"location":"accessing-production-database/","text":"Access CleanAir Production Database \u00b6 To access the production database you will need an Azure account and be given access by one of the database adminstrators . You should discuss what your access requirements are (e.g. do you need write access).To access the database first login to Azure from the terminal. You can then request an access token. The token will be valid for between 5 minutes and 1 hour. Set the token as an environment variable: export PGPASSWORD = $( az account get-access-token --resource-type oss-rdbms --query accessToken -o tsv ) Connect using psql \u00b6 Once your IP has been whitelisted (ask the database adminstrators ), you will be able to access the database using psql: psql \"host=cleanair-inputs-server.postgres.database.azure.com port=5432 dbname=cleanair_inputs_db user=<your-turing-credentials>@cleanair-inputs-server sslmode=require\" replacing <your-turing-credentials> with your turing credentials (e.g. jblogs@turing.ac.uk ). Create secret file to connect using CleanAir package \u00b6 To connect to the database using the CleanAir package you will need to create another secret file: echo '{ \"username\": \"<your-turing-credentials>@cleanair-inputs-server\", \"host\": \"cleanair-inputs-server.postgres.database.azure.com\", \"port\": 5432, \"db_name\": \"cleanair_inputs_db\", \"ssl_mode\": \"require\" }' >> .secrets/db_secrets_ad.json Make sure you then replace <your-turing-credentials> with your full Turing username (e.g. jblogs@turing.ac.uk@cleanair-inputs-server ).","title":"Access CleanAir Production Database"},{"location":"accessing-production-database/#access-cleanair-production-database","text":"To access the production database you will need an Azure account and be given access by one of the database adminstrators . You should discuss what your access requirements are (e.g. do you need write access).To access the database first login to Azure from the terminal. You can then request an access token. The token will be valid for between 5 minutes and 1 hour. Set the token as an environment variable: export PGPASSWORD = $( az account get-access-token --resource-type oss-rdbms --query accessToken -o tsv )","title":"Access CleanAir Production Database"},{"location":"accessing-production-database/#connect-using-psql","text":"Once your IP has been whitelisted (ask the database adminstrators ), you will be able to access the database using psql: psql \"host=cleanair-inputs-server.postgres.database.azure.com port=5432 dbname=cleanair_inputs_db user=<your-turing-credentials>@cleanair-inputs-server sslmode=require\" replacing <your-turing-credentials> with your turing credentials (e.g. jblogs@turing.ac.uk ).","title":"Connect using psql"},{"location":"accessing-production-database/#create-secret-file-to-connect-using-cleanair-package","text":"To connect to the database using the CleanAir package you will need to create another secret file: echo '{ \"username\": \"<your-turing-credentials>@cleanair-inputs-server\", \"host\": \"cleanair-inputs-server.postgres.database.azure.com\", \"port\": 5432, \"db_name\": \"cleanair_inputs_db\", \"ssl_mode\": \"require\" }' >> .secrets/db_secrets_ad.json Make sure you then replace <your-turing-credentials> with your full Turing username (e.g. jblogs@turing.ac.uk@cleanair-inputs-server ).","title":"Create secret file to connect using CleanAir package"},{"location":"developer-guide/","text":"Developer guide \u00b6 Style guide \u00b6 Writing Documentation \u00b6 Before being accepted into master all code should have well writen documentation. Please use Google Style Python Docstrings We would like to move towards adding type hints so you may optionally add types to your code. In which case you do not need to include types in your google style docstrings. Adding and updating existing documentation is highly encouraged. Gitmoji \u00b6 We like gitmoji for an emoji guide to our commit messages. You might consider (entirly optional) to use the gitmoji-cli as a hook when writing commit messages. Working on an issue \u00b6 The general workflow for contributing to the project is to first choose and issue (or create one) to work on and assign yourself to the issues. You can find issues that need work on by searching by the Needs assignment label. If you decide to move onto something else or wonder what you've got yourself into please unassign yourself, leave a comment about why you dropped the issue (e.g. got bored, blocked by something etc) and re-add the Needs assignment label. You are encouraged to open a pull request earlier rather than later (either a draft pull request or add WIP to the title) so others know what you are working on. How you label branches is optional, but we encourage using iss_<issue-number>_<description_of_issue> where <issue-number> is the github issue number and <description_of_issue> is a very short description of the issue. For example iss_928_add_api_docs . Running tests \u00b6 Tests should be written where possible before code is accepted into master. Contributing tests to existing code is highly desirable. Tests will also be run on travis (see the travis configuration ). All tests can be found in the containers/tests/ directory. We already ran some tests to check our local database was set up. To run the full test suite against the local database run SECRETS = $( pwd ) /.secrets/.db_secrets_offline.json pytest containers --secretfile $SECRETS Writing tests \u00b6 The following shows an example test: def test_scoot_reading_empty ( secretfile , connection ): conn = DBWriter ( secretfile = secretfile , initialise_tables = True , connection = connection ) with conn . dbcnxn . open_session () as session : assert session . query ( ScootReading ) . count () == 0 It uses the DBWriter class to connect to the database. In general when interacting with a database we write a class which inherits from either DBWriter or DBReader . Both classes take a secretfile as an argument which provides database connection secrets. Critically, we also pass a special connection fixture when initialising any class that interacts with the database . This fixture ensures that all interactions with the database take place within a transaction . At the end of the test the transaction is rolled back leaving the database in the same state it was in before the test was run, even if commit is called on the database.","title":"Developer guide"},{"location":"developer-guide/#developer-guide","text":"","title":"Developer guide"},{"location":"developer-guide/#style-guide","text":"","title":"Style guide"},{"location":"developer-guide/#writing-documentation","text":"Before being accepted into master all code should have well writen documentation. Please use Google Style Python Docstrings We would like to move towards adding type hints so you may optionally add types to your code. In which case you do not need to include types in your google style docstrings. Adding and updating existing documentation is highly encouraged.","title":"Writing Documentation"},{"location":"developer-guide/#gitmoji","text":"We like gitmoji for an emoji guide to our commit messages. You might consider (entirly optional) to use the gitmoji-cli as a hook when writing commit messages.","title":"Gitmoji"},{"location":"developer-guide/#working-on-an-issue","text":"The general workflow for contributing to the project is to first choose and issue (or create one) to work on and assign yourself to the issues. You can find issues that need work on by searching by the Needs assignment label. If you decide to move onto something else or wonder what you've got yourself into please unassign yourself, leave a comment about why you dropped the issue (e.g. got bored, blocked by something etc) and re-add the Needs assignment label. You are encouraged to open a pull request earlier rather than later (either a draft pull request or add WIP to the title) so others know what you are working on. How you label branches is optional, but we encourage using iss_<issue-number>_<description_of_issue> where <issue-number> is the github issue number and <description_of_issue> is a very short description of the issue. For example iss_928_add_api_docs .","title":"Working on an issue"},{"location":"developer-guide/#running-tests","text":"Tests should be written where possible before code is accepted into master. Contributing tests to existing code is highly desirable. Tests will also be run on travis (see the travis configuration ). All tests can be found in the containers/tests/ directory. We already ran some tests to check our local database was set up. To run the full test suite against the local database run SECRETS = $( pwd ) /.secrets/.db_secrets_offline.json pytest containers --secretfile $SECRETS","title":"Running tests"},{"location":"developer-guide/#writing-tests","text":"The following shows an example test: def test_scoot_reading_empty ( secretfile , connection ): conn = DBWriter ( secretfile = secretfile , initialise_tables = True , connection = connection ) with conn . dbcnxn . open_session () as session : assert session . query ( ScootReading ) . count () == 0 It uses the DBWriter class to connect to the database. In general when interacting with a database we write a class which inherits from either DBWriter or DBReader . Both classes take a secretfile as an argument which provides database connection secrets. Critically, we also pass a special connection fixture when initialising any class that interacts with the database . This fixture ensures that all interactions with the database take place within a transaction . At the end of the test the transaction is rolled back leaving the database in the same state it was in before the test was run, even if commit is called on the database.","title":"Writing tests"},{"location":"entry-points/","text":"Entry points \u00b6 Running entry points \u00b6 The directory containers/entrypoints contains Python scripts which are then built into Docker images in containers/dockerfiles . You can run them locally. These are scripts which collect and insert data into the database. To see what arguments they take you can call any of the files with the argument -h , for example: python containers/entrypoints/inputs/input_laqn_readings.py -h Entry point with local database \u00b6 The entrypoints will need to connect to a database. To do so you can pass one or more of the following arguments: --secretfile : Full path to one of the secret .json files you created in the .secrets directory. --secret-dict : A set of parameters to override the values in --secretfile . For example you could alter the port and ssl parameters as --secret-dict port=5411 ssl_mode=prefer Entry point with production database \u00b6 You will notice that the db_secrets_ad.json file we created does not contain a password. To run an entrypoint against a production database you must run: az login export PGPASSWORD = $( az account get-access-token --resource-type oss-rdbms --query accessToken -o tsv ) When you run an entrypoint script the CleanAir package will read the PGPASSWORD environment variable. This will also take precedence over any value provided in the --secret-dict argument. Docker entry point \u00b6 To run an entry point from a docker file we first need to build a docker image. Here shown for the satellite input entry point: docker build -t input_satellite:local -f containers/dockerfiles/input_satellite_readings.Dockerfile containers To run we need to set a few more environment variables. The first is the directory with secret files in: SECRET_DIR = $( pwd ) /.secrets Now get a new token: export PGPASSWORD = $( az account get-access-token --resource-type oss-rdbms --query accessToken -o tsv ) Finally you can run the docker image, passing PGPASSWORD as an environment variable (:warning: this writes data into the online database) docker run -e PGPASSWORD -v $SECRET_DIR :/secrets input_satellite:local -s 'db_secrets_ad.json' -k <copernicus-key> Here we also provided the copernicus api key which is stored in the cleanair-secrets Azure's keyvault . If you want to run that example with the local database you can do so by: COPERNICUS_KEY = $( az keyvault secret show --vault-name cleanair-secrets --name satellite-copernicus-key -o tsv --query value ) # OSX or Windows: change \"localhost\" to host.docker.internal on your db_secrets_offline.json docker run -e PGPASSWORD -v $SECRET_DIR :/secrets input_satellite:local -s 'db_secrets_offline.json' -k $COPERNICUS_KEY # Linux: docker run --network host -e PGPASSWORD -v $SECRET_DIR :/secrets input_satellite:local -s 'db_secrets_offline.json' -k $COPERNICUS_KEY","title":"Entry points"},{"location":"entry-points/#entry-points","text":"","title":"Entry points"},{"location":"entry-points/#running-entry-points","text":"The directory containers/entrypoints contains Python scripts which are then built into Docker images in containers/dockerfiles . You can run them locally. These are scripts which collect and insert data into the database. To see what arguments they take you can call any of the files with the argument -h , for example: python containers/entrypoints/inputs/input_laqn_readings.py -h","title":"Running entry points"},{"location":"entry-points/#entry-point-with-local-database","text":"The entrypoints will need to connect to a database. To do so you can pass one or more of the following arguments: --secretfile : Full path to one of the secret .json files you created in the .secrets directory. --secret-dict : A set of parameters to override the values in --secretfile . For example you could alter the port and ssl parameters as --secret-dict port=5411 ssl_mode=prefer","title":"Entry point with local database"},{"location":"entry-points/#entry-point-with-production-database","text":"You will notice that the db_secrets_ad.json file we created does not contain a password. To run an entrypoint against a production database you must run: az login export PGPASSWORD = $( az account get-access-token --resource-type oss-rdbms --query accessToken -o tsv ) When you run an entrypoint script the CleanAir package will read the PGPASSWORD environment variable. This will also take precedence over any value provided in the --secret-dict argument.","title":"Entry point with production database"},{"location":"entry-points/#docker-entry-point","text":"To run an entry point from a docker file we first need to build a docker image. Here shown for the satellite input entry point: docker build -t input_satellite:local -f containers/dockerfiles/input_satellite_readings.Dockerfile containers To run we need to set a few more environment variables. The first is the directory with secret files in: SECRET_DIR = $( pwd ) /.secrets Now get a new token: export PGPASSWORD = $( az account get-access-token --resource-type oss-rdbms --query accessToken -o tsv ) Finally you can run the docker image, passing PGPASSWORD as an environment variable (:warning: this writes data into the online database) docker run -e PGPASSWORD -v $SECRET_DIR :/secrets input_satellite:local -s 'db_secrets_ad.json' -k <copernicus-key> Here we also provided the copernicus api key which is stored in the cleanair-secrets Azure's keyvault . If you want to run that example with the local database you can do so by: COPERNICUS_KEY = $( az keyvault secret show --vault-name cleanair-secrets --name satellite-copernicus-key -o tsv --query value ) # OSX or Windows: change \"localhost\" to host.docker.internal on your db_secrets_offline.json docker run -e PGPASSWORD -v $SECRET_DIR :/secrets input_satellite:local -s 'db_secrets_offline.json' -k $COPERNICUS_KEY # Linux: docker run --network host -e PGPASSWORD -v $SECRET_DIR :/secrets input_satellite:local -s 'db_secrets_offline.json' -k $COPERNICUS_KEY","title":"Docker entry point"},{"location":"infrastructure-development/","text":"Infrastructure Deployment \u00b6 The following steps are needed to setup the Clean Air cloud infrastructure. Only infrastrucure administrator should deploy Login to Travis CLI \u00b6 Login to Travis with your github credentials, making sure you are in the Clean Air repository (Travis automatically detects your repository): travis login --pro Create an Azure service principal using the documentation for the Azure CLI or with Powershell , ensuring that you keep track of the NAME , ID and PASSWORD/SECRET for the service principal, as these will be needed later. Setup Terraform with Python \u00b6 Terraform uses a backend to keep track of the infrastructure state. We keep the backend in Azure storage so that everyone has a synchronised version of the state. You can download the `tfstate` file with `az` though you won't need it. cd terraform az storage blob download -c terraformbackend -f terraform.tfstate -n terraform.tfstate --account-name terraformstorage924roouq --auth-mode key To enable this, we have to create an initial Terraform configuration by running (from the root directory): python cleanair_setup/initialise_terraform.py -i $AWS_KEY_ID -k $AWS_KEY -n $SERVICE_PRINCIPAL_NAME -s $SERVICE_PRINCIPAL_ID -p $SERVICE_PRINCIPAL_PASSWORD Where AWS_KEY_ID and AWS_KEY are the secure key information needed to access TfL's SCOOT data on Amazon Web Services. AWS_KEY = $( az keyvault secret show --vault-name terraform-configuration --name scoot-aws-key -o tsv --query value ) AWS_KEY_ID = $( az keyvault secret show --vault-name terraform-configuration --name scoot-aws-key-id -o tsv --query value ) And SERVICE_PRINCIPAL 's NAME , ID and PASSWORD are also available in the terraform-configuration keyvault. SERVICE_PRINCIPAL_NAME = $( az keyvault secret show --vault-name terraform-configuration --name azure-service-principal-name -o tsv --query value ) SERVICE_PRINCIPAL_ID = $( az keyvault secret show --vault-name terraform-configuration --name azure-service-principal-id -o tsv --query value ) SERVICE_PRINCIPAL_PASSWORD = $( az keyvault secret show --vault-name terraform-configuration --name azure-service-principal-password -o tsv --query value ) This will only need to be run once (by anyone), but it's not a problem if you run it multiple times. Building the Clean Air infrastructure with Terraform \u00b6 To build the Terraform infrastructure go to the terraform directory cd terraform and run: terraform init If you want to, you can look at the backend_config.tf file, which should contain various details of your Azure subscription. NB. It is important that this file is in .gitignore . Do not push this file to the remote repository Then run: terraform plan which creates an execution plan. Check this matches your expectations. If you are happy then run: terraform apply to set up the Clean Air infrastructure on Azure using Terraform . You should be able to see this on the Azure portal. Creating A Record for cleanair API (DO THIS BEFORE RUNNING AZURE PIPELINES) \u00b6 Terraform created a DNS Zone in the kubernetes cluster resource group ( RG_CLEANAIR_KUBERNETES_CLUSTER ). Navigate to the DNS Zone on the Azure portal and copy the four nameservers in the \u201cNS\u201d record. Send the nameserver to Turing IT Services. Ask them to add the subdomain\u2019s DNS record as an NS record for urbanair in the turing.ac.uk DNS zone record. When viewing the DNS zone on the Azure Portal, click + Record set In the Name field, enter urbanair . Set Alias record set to \u201cYes\u201d and this will bring up some new options. We can now set up Azure pipelines. Once the cleanair api has been deployed on kubernetes you can update the alias record to point to the ip address of the cleanair-api on the cluster. Initialising the input databases \u00b6 Terraform will now have created a number of databases. We need to add the datasets to the database. This is done using Docker images from the Azure container registry. You will need the username, password and server name for the Azure container registry. All of these will be stored as secrets in the RG_CLEANAIR_INFRASTRUCTURE > cleanair-secrets Azure KeyVault. Setting up Azure pipelines \u00b6 These Docker images are built by an Azure pipeline whenever commits are made to the master branch of the GitHub repository. Ensure that you have configured Azure pipelines to use this GitHub repository . You will need to add Service Connections to GitHub and to Azure (the Azure one should be called cleanair-scn ). Currently a pipeline is set up here . To run the next steps we need to ensure that this pipeline runs a build in order to add the Docker images to the Azure container registry created by Terraform. Either push to the GitHub repository, or rerun the last build by going to the Azure pipeline page and clicking Run pipeline on the right-hand context menu. This will build all of the Docker images and add them to the registry. Now go to Azure and update the A-record to point to the ip address of the cleanair-api on the cluster. Add static datasets \u00b6 To add static datasets follow the Static data insert instructions but use the production database credentials Adding live datasets \u00b6 The live datasets (like LAQN or AQE) are populated using regular jobs that create an Azure container instance and add the most recent data to the database. These are run automatically through Kubernetes and the Azure pipeline above is used to keep track of which version of the code to use. Kubernetes deployment with GPU support \u00b6 The azure pipeline will deploy the cleanair helm chart to the azure kubernetes cluster we deployed with terraform. If you deployed GPU enabled machines on Azure (current default in the terraform script) then you need to install the nvidia device plugin daemonset. The manifest for this is adapted from the Azure docs . However, as our GPU machines have taints applied we have to add tolerations to the manifest, otherwise the nodes will block the daemonset. To install the custom manifest run, kubectl apply -f kubernetes/gpu_resources/nvidia-device-plugin-ds.yaml Removing Terraform infrastructure \u00b6 To destroy all the resources created by Terraform run: terraform destroy You can check everything was removed on the Azure portal. Then login to TravisCI and delete the Azure Container repo environment variables.","title":"Infrastructure Deployment"},{"location":"infrastructure-development/#infrastructure-deployment","text":"The following steps are needed to setup the Clean Air cloud infrastructure. Only infrastrucure administrator should deploy","title":"Infrastructure Deployment"},{"location":"infrastructure-development/#login-to-travis-cli","text":"Login to Travis with your github credentials, making sure you are in the Clean Air repository (Travis automatically detects your repository): travis login --pro Create an Azure service principal using the documentation for the Azure CLI or with Powershell , ensuring that you keep track of the NAME , ID and PASSWORD/SECRET for the service principal, as these will be needed later.","title":"Login to Travis CLI"},{"location":"infrastructure-development/#setup-terraform-with-python","text":"Terraform uses a backend to keep track of the infrastructure state. We keep the backend in Azure storage so that everyone has a synchronised version of the state. You can download the `tfstate` file with `az` though you won't need it. cd terraform az storage blob download -c terraformbackend -f terraform.tfstate -n terraform.tfstate --account-name terraformstorage924roouq --auth-mode key To enable this, we have to create an initial Terraform configuration by running (from the root directory): python cleanair_setup/initialise_terraform.py -i $AWS_KEY_ID -k $AWS_KEY -n $SERVICE_PRINCIPAL_NAME -s $SERVICE_PRINCIPAL_ID -p $SERVICE_PRINCIPAL_PASSWORD Where AWS_KEY_ID and AWS_KEY are the secure key information needed to access TfL's SCOOT data on Amazon Web Services. AWS_KEY = $( az keyvault secret show --vault-name terraform-configuration --name scoot-aws-key -o tsv --query value ) AWS_KEY_ID = $( az keyvault secret show --vault-name terraform-configuration --name scoot-aws-key-id -o tsv --query value ) And SERVICE_PRINCIPAL 's NAME , ID and PASSWORD are also available in the terraform-configuration keyvault. SERVICE_PRINCIPAL_NAME = $( az keyvault secret show --vault-name terraform-configuration --name azure-service-principal-name -o tsv --query value ) SERVICE_PRINCIPAL_ID = $( az keyvault secret show --vault-name terraform-configuration --name azure-service-principal-id -o tsv --query value ) SERVICE_PRINCIPAL_PASSWORD = $( az keyvault secret show --vault-name terraform-configuration --name azure-service-principal-password -o tsv --query value ) This will only need to be run once (by anyone), but it's not a problem if you run it multiple times.","title":"Setup Terraform with Python"},{"location":"infrastructure-development/#building-the-clean-air-infrastructure-with-terraform","text":"To build the Terraform infrastructure go to the terraform directory cd terraform and run: terraform init If you want to, you can look at the backend_config.tf file, which should contain various details of your Azure subscription. NB. It is important that this file is in .gitignore . Do not push this file to the remote repository Then run: terraform plan which creates an execution plan. Check this matches your expectations. If you are happy then run: terraform apply to set up the Clean Air infrastructure on Azure using Terraform . You should be able to see this on the Azure portal.","title":"Building the Clean Air infrastructure with Terraform"},{"location":"infrastructure-development/#creating-a-record-for-cleanair-api-do-this-before-running-azure-pipelines","text":"Terraform created a DNS Zone in the kubernetes cluster resource group ( RG_CLEANAIR_KUBERNETES_CLUSTER ). Navigate to the DNS Zone on the Azure portal and copy the four nameservers in the \u201cNS\u201d record. Send the nameserver to Turing IT Services. Ask them to add the subdomain\u2019s DNS record as an NS record for urbanair in the turing.ac.uk DNS zone record. When viewing the DNS zone on the Azure Portal, click + Record set In the Name field, enter urbanair . Set Alias record set to \u201cYes\u201d and this will bring up some new options. We can now set up Azure pipelines. Once the cleanair api has been deployed on kubernetes you can update the alias record to point to the ip address of the cleanair-api on the cluster.","title":"Creating A Record for cleanair API (DO THIS BEFORE RUNNING AZURE PIPELINES)"},{"location":"infrastructure-development/#initialising-the-input-databases","text":"Terraform will now have created a number of databases. We need to add the datasets to the database. This is done using Docker images from the Azure container registry. You will need the username, password and server name for the Azure container registry. All of these will be stored as secrets in the RG_CLEANAIR_INFRASTRUCTURE > cleanair-secrets Azure KeyVault.","title":"Initialising the input databases"},{"location":"infrastructure-development/#setting-up-azure-pipelines","text":"These Docker images are built by an Azure pipeline whenever commits are made to the master branch of the GitHub repository. Ensure that you have configured Azure pipelines to use this GitHub repository . You will need to add Service Connections to GitHub and to Azure (the Azure one should be called cleanair-scn ). Currently a pipeline is set up here . To run the next steps we need to ensure that this pipeline runs a build in order to add the Docker images to the Azure container registry created by Terraform. Either push to the GitHub repository, or rerun the last build by going to the Azure pipeline page and clicking Run pipeline on the right-hand context menu. This will build all of the Docker images and add them to the registry. Now go to Azure and update the A-record to point to the ip address of the cleanair-api on the cluster.","title":"Setting up Azure pipelines"},{"location":"infrastructure-development/#add-static-datasets","text":"To add static datasets follow the Static data insert instructions but use the production database credentials","title":"Add static datasets"},{"location":"infrastructure-development/#adding-live-datasets","text":"The live datasets (like LAQN or AQE) are populated using regular jobs that create an Azure container instance and add the most recent data to the database. These are run automatically through Kubernetes and the Azure pipeline above is used to keep track of which version of the code to use.","title":"Adding live datasets"},{"location":"infrastructure-development/#kubernetes-deployment-with-gpu-support","text":"The azure pipeline will deploy the cleanair helm chart to the azure kubernetes cluster we deployed with terraform. If you deployed GPU enabled machines on Azure (current default in the terraform script) then you need to install the nvidia device plugin daemonset. The manifest for this is adapted from the Azure docs . However, as our GPU machines have taints applied we have to add tolerations to the manifest, otherwise the nodes will block the daemonset. To install the custom manifest run, kubectl apply -f kubernetes/gpu_resources/nvidia-device-plugin-ds.yaml","title":"Kubernetes deployment with GPU support"},{"location":"infrastructure-development/#removing-terraform-infrastructure","text":"To destroy all the resources created by Terraform run: terraform destroy You can check everything was removed on the Azure portal. Then login to TravisCI and delete the Azure Container repo environment variables.","title":"Removing Terraform infrastructure"},{"location":"researcher-guide/","text":"Researcher guide \u00b6 The following steps provide useful tools for researchers to use, for example jupyter notebooks. Setup notebook \u00b6 First install jupyter with conda (you can also use pip). pip install jupyter You can start the notebook: jupyter notebook Environment variables \u00b6 To access the database, the notebooks need access to the PGPASSWORD environment variable. It is also recommended to set the SECRETS variable. We will create a .env file within you notebook directory path/to/notebook where you will be storing environment variables. Note : if you are using a shared system or scientific cluster, do not follow these steps and do not store your password in a file . Run the below command to create a .env file, replacing path/to/secretfile with the path to your db_secrets . echo ' SECRETS=\"path/to/secretfile\" PGPASSWORD= ' > path/to/notebook/.env To set the PGPASSWORD , run the following command. This will create a new password using the azure cli and replace the line in .env that contains PGPASSWORD with the new password. Remember to replace path/to/notebook with the path to your notebook directory. sed -i '' \"s/.*PGPASSWORD.*/PGPASSWORD= $( az account get-access-token --resource-type oss-rdbms --query accessToken -o tsv ) /g\" path/to/notebook/.env If you need to store other environment variables and access them in your notebook, simply add them to the .env file. To access the environment variables, include the following lines at the top of your jupyter notebook: % load_ext dotenv % dotenv You can now access the value of these variables as follows: secretfile = os . getenv ( \"SECRETS\" , None ) Remember that the PGPASSWORD token will only be valid for ~1h.","title":"Researcher guide"},{"location":"researcher-guide/#researcher-guide","text":"The following steps provide useful tools for researchers to use, for example jupyter notebooks.","title":"Researcher guide"},{"location":"researcher-guide/#setup-notebook","text":"First install jupyter with conda (you can also use pip). pip install jupyter You can start the notebook: jupyter notebook","title":"Setup notebook"},{"location":"researcher-guide/#environment-variables","text":"To access the database, the notebooks need access to the PGPASSWORD environment variable. It is also recommended to set the SECRETS variable. We will create a .env file within you notebook directory path/to/notebook where you will be storing environment variables. Note : if you are using a shared system or scientific cluster, do not follow these steps and do not store your password in a file . Run the below command to create a .env file, replacing path/to/secretfile with the path to your db_secrets . echo ' SECRETS=\"path/to/secretfile\" PGPASSWORD= ' > path/to/notebook/.env To set the PGPASSWORD , run the following command. This will create a new password using the azure cli and replace the line in .env that contains PGPASSWORD with the new password. Remember to replace path/to/notebook with the path to your notebook directory. sed -i '' \"s/.*PGPASSWORD.*/PGPASSWORD= $( az account get-access-token --resource-type oss-rdbms --query accessToken -o tsv ) /g\" path/to/notebook/.env If you need to store other environment variables and access them in your notebook, simply add them to the .env file. To access the environment variables, include the following lines at the top of your jupyter notebook: % load_ext dotenv % dotenv You can now access the value of these variables as follows: secretfile = os . getenv ( \"SECRETS\" , None ) Remember that the PGPASSWORD token will only be valid for ~1h.","title":"Environment variables"},{"location":"urbanair-api/","text":"UrbanAir API \u00b6 The UrbanAir RESTFUL API is a Flask application. To run it in locally you must configure the following steps: Configure CleanAir database secrets \u00b6 Ensure you have configured a secrets file for the CleanAir database as documented above . You will also need to set the PGPASSWORD environment variable export DATABASE_SECRETFILE = $( pwd ) /.secrets/.db_secrets_ad.json Enable Flask development server \u00b6 export FLASK_ENV = development You can now run the API python containers/urbanair/wsgi.py","title":"UrbanAir API"},{"location":"urbanair-api/#urbanair-api","text":"The UrbanAir RESTFUL API is a Flask application. To run it in locally you must configure the following steps:","title":"UrbanAir API"},{"location":"urbanair-api/#configure-cleanair-database-secrets","text":"Ensure you have configured a secrets file for the CleanAir database as documented above . You will also need to set the PGPASSWORD environment variable export DATABASE_SECRETFILE = $( pwd ) /.secrets/.db_secrets_ad.json","title":"Configure CleanAir database secrets"},{"location":"urbanair-api/#enable-flask-development-server","text":"export FLASK_ENV = development You can now run the API python containers/urbanair/wsgi.py","title":"Enable Flask development server"},{"location":"API/cleanair/dashboard/","text":"Dashboard \u00b6 \u00b6 A dashboard for validation and model fit. apps \u00b6 Create dash apps. get_model_data_fit_app ( model_data , sensor_scores_df , temporal_scores_df , mapbox_access_token , ** kwargs ) \u00b6 Return an app showing the scores for a model data fit. Parameters model_data : ModelData Model data object with updated predictions. sensor_scores_df : pd.DataFrame Metric scores over sensors for this model fit. temporal_scores_df : pd.DataFrame Scores over time for this model fit. mapbox_access_token : str The API token for MapBox. Returns App Dash app. Other Parameters evaluate_training : bool, optional Default is False. Show the metrics over the training period. evaluate_testing : bool, optional Default is True Show the metrics over the testing period. all_metrics : list, optional List of metrics to show in the dashboard, e.g. r2_score, mae. callbacks \u00b6 Function to handle callbacks from apps. interest_point_mapbox_callback ( mapbox_fig , sensor_scores_df , metric_key , pollutant ) \u00b6 Update the map of sensors when a different pollutant or metric is chosen. ip_timeseries_callback ( hover_data , point_groupby , pollutant = 'NO2' ) \u00b6 When hovering over a point, update the timeseries showing the prediction. components \u00b6 Meta components that make up the dashboard. ModelFitComponent \u00b6 Collect all the components of a model fit and its layout in the app. __init__ ( self , instance_id , model_data , sensor_scores_df , temporal_scores_df , ** kwargs ) special \u00b6 Initialise with a model data object and the scores for the fit. get_interest_points_map ( self , metric_key , pollutant ) \u00b6 Get a map with interest points plotted and the colour of points is the metric score. get_interest_points_timeseries ( self , point_id , pollutant ) \u00b6 Get a map with interest points plotted. get_temporal_metrics_timeseries ( self , metric_key , pollutant ) \u00b6 Get a timeseries of the score for a given metric over the prediction period. get_metric_dropdown ( component_id , metric_keys ) \u00b6 Get a dropdown menu with all available metrics. get_model_data_fit_intro () \u00b6 Get the markdown for the introduction text for a model fit. get_pollutant_dropdown ( component_id , species ) \u00b6 Get a dropdown menu with all possible pollutants inside. timeseries \u00b6 Plotly figures for timeseries data. get_pollutant_point_trace ( point_pred_df , col = 'NO2_mean' , name = None ) \u00b6 Return a plotly trace dict for a timeseries of the mean prediction on a sensor.","title":"Dashboard"},{"location":"API/cleanair/dashboard/#dashboard","text":"","title":"Dashboard"},{"location":"API/cleanair/dashboard/#cleanair.dashboard","text":"A dashboard for validation and model fit.","title":"cleanair.dashboard"},{"location":"API/cleanair/dashboard/#cleanair.dashboard.apps","text":"Create dash apps.","title":"apps"},{"location":"API/cleanair/dashboard/#cleanair.dashboard.apps.get_model_data_fit_app","text":"Return an app showing the scores for a model data fit. Parameters model_data : ModelData Model data object with updated predictions. sensor_scores_df : pd.DataFrame Metric scores over sensors for this model fit. temporal_scores_df : pd.DataFrame Scores over time for this model fit. mapbox_access_token : str The API token for MapBox. Returns App Dash app. Other Parameters evaluate_training : bool, optional Default is False. Show the metrics over the training period. evaluate_testing : bool, optional Default is True Show the metrics over the testing period. all_metrics : list, optional List of metrics to show in the dashboard, e.g. r2_score, mae.","title":"get_model_data_fit_app()"},{"location":"API/cleanair/dashboard/#cleanair.dashboard.callbacks","text":"Function to handle callbacks from apps.","title":"callbacks"},{"location":"API/cleanair/dashboard/#cleanair.dashboard.callbacks.interest_point_mapbox_callback","text":"Update the map of sensors when a different pollutant or metric is chosen.","title":"interest_point_mapbox_callback()"},{"location":"API/cleanair/dashboard/#cleanair.dashboard.callbacks.ip_timeseries_callback","text":"When hovering over a point, update the timeseries showing the prediction.","title":"ip_timeseries_callback()"},{"location":"API/cleanair/dashboard/#cleanair.dashboard.components","text":"Meta components that make up the dashboard.","title":"components"},{"location":"API/cleanair/dashboard/#cleanair.dashboard.components.ModelFitComponent","text":"Collect all the components of a model fit and its layout in the app.","title":"ModelFitComponent"},{"location":"API/cleanair/dashboard/#cleanair.dashboard.components.ModelFitComponent.__init__","text":"Initialise with a model data object and the scores for the fit.","title":"__init__()"},{"location":"API/cleanair/dashboard/#cleanair.dashboard.components.ModelFitComponent.get_interest_points_map","text":"Get a map with interest points plotted and the colour of points is the metric score.","title":"get_interest_points_map()"},{"location":"API/cleanair/dashboard/#cleanair.dashboard.components.ModelFitComponent.get_interest_points_timeseries","text":"Get a map with interest points plotted.","title":"get_interest_points_timeseries()"},{"location":"API/cleanair/dashboard/#cleanair.dashboard.components.ModelFitComponent.get_temporal_metrics_timeseries","text":"Get a timeseries of the score for a given metric over the prediction period.","title":"get_temporal_metrics_timeseries()"},{"location":"API/cleanair/dashboard/#cleanair.dashboard.components.get_metric_dropdown","text":"Get a dropdown menu with all available metrics.","title":"get_metric_dropdown()"},{"location":"API/cleanair/dashboard/#cleanair.dashboard.components.get_model_data_fit_intro","text":"Get the markdown for the introduction text for a model fit.","title":"get_model_data_fit_intro()"},{"location":"API/cleanair/dashboard/#cleanair.dashboard.components.get_pollutant_dropdown","text":"Get a dropdown menu with all possible pollutants inside.","title":"get_pollutant_dropdown()"},{"location":"API/cleanair/dashboard/#cleanair.dashboard.timeseries","text":"Plotly figures for timeseries data.","title":"timeseries"},{"location":"API/cleanair/dashboard/#cleanair.dashboard.timeseries.get_pollutant_point_trace","text":"Return a plotly trace dict for a timeseries of the mean prediction on a sensor.","title":"get_pollutant_point_trace()"},{"location":"API/cleanair/databases/","text":"Databases \u00b6 \u00b6 Module for interacting with the Azure Postgres database base \u00b6 Declarative base class and table initialisation connector \u00b6 Class for connecting to Azure databases Connector \u00b6 Base class for connecting to databases with sqlalchemy engine property readonly \u00b6 Access the class-level sqlalchemy engine sessionfactory property readonly \u00b6 Access the class-level sqlalchemy sessionfactory __init__ ( self , secretfile , connection = None , ** kwargs ) special \u00b6 Parameters: Name Type Description Default connection sqlalchemy.engine.Connection Pass an sqlalchemy connection object. Useful when testing to allow operations with a transaction None check_internet_connection ( self , url = 'http://www.google.com/' , timeout = 5 , interval = 10 ) \u00b6 Check that the internet is accessible Repeated checks within interval seconds will be skipped check_schema_exists ( self , schema_name ) \u00b6 Check if a schema exists ensure_database_exists ( self ) \u00b6 Ensure the database exists ensure_extensions ( self ) \u00b6 Ensure required extensions are installed publicly ensure_schema ( self , schema_name ) \u00b6 Ensure that requested schema exists initialise_tables ( self ) \u00b6 Ensure that all table connections exist open_session ( self , skip_check = False ) \u00b6 Create a session as a context manager which will thereby self-close db_config \u00b6 Class for configuring database schema DBConfig \u00b6 Class to manage database configuration database_name property readonly \u00b6 Return name of database in config file roles property readonly \u00b6 Return a list of roles schema property readonly \u00b6 Return a list of schemas from the database __init__ ( self , config_file , * args , ** kwargs ) special \u00b6 Loads a database configuration file Parameters: Name Type Description Default config_file str Location of configuration file required Returns: Type Description None assign_role_connect ( self , role_name ) \u00b6 Allow a role to connect to the database Parameters: Name Type Description Default role_name str Role name required Returns: Type Description None assign_role_schema_default_privilege ( self , role_name , schema_name , privileges ) \u00b6 Assign a role default privilegs Parameters: Name Type Description Default role_name str Role name required schema_name str Schema name required privileges Lis[str] A list of privileges to assign required Returns: Type Description None assign_role_schema_default_sequences ( self , role_name , schema_name ) \u00b6 Assign a role default sequences on a schema Parameters: Name Type Description Default role_name str Role name required schema_name str Schema name required Returns: Type Description None assign_role_schema_privilege ( self , role_name , schema_name , privileges ) \u00b6 Assign a role a list of privileges Parameters: Name Type Description Default role_name str Role name required schema_name str Schema name required privileges List[str] A list of privileges to assign required Returns: Type Description None assign_role_schema_sequences ( self , role_name , schema_name ) \u00b6 Assign a role to all sequences on schema Parameters: Name Type Description Default role_name str Role name required schema_name str Schema name required Returns: Type Description None assign_role_schema_usage ( self , role_name , schema_name , create = False ) \u00b6 Assign a role to all schemas Parameters: Name Type Description Default role_name str Role name required schema_name str Schema name required create bool Give the user CREATE on the schema (i.e. they can create objects) False Returns: Type Description None configure_all_roles ( self ) \u00b6 Configure roles as defined in the configuration file create_role ( self , role_name ) \u00b6 Create a new role Parameters: Name Type Description Default role_name str Role name required Returns: Type Description None create_schema ( self ) \u00b6 Create schemas on database create_user ( self , username , password ) \u00b6 Create a new user and assign to a role Parameters: Name Type Description Default username str A username required password str A user's password required Returns: Type Description None grant_role_to_user ( self , username , role ) \u00b6 Parameters: Name Type Description Default username str A username required roles List[str] A list of roles to assign a user to required Returns: Type Description None list_roles ( self ) \u00b6 Return a list of existing database roles read_config ( config_file ) staticmethod \u00b6 Read a database configuration file Parameters: Name Type Description Default config_file str Location of configuration file required Returns: Type Description dict A dictionary of the required configuration db_interactor \u00b6 Table reader/writer DBInteractor \u00b6 Base class for interacting with tables in the Azure database __init__ ( self , secretfile , initialise_tables = True , connection = None , secret_dict = None ) special \u00b6 Init method for connecting to database Parameters: Name Type Description Default secretfile str Path to a secret file (json). Can be the full path to secrets file or a filename if the secret is in a directory called '/secrets' required initialise_tables bool Create all tables. Default to False. Requires admin privileges on database. True connection sqlalchemy.engine.Connection Pass an sqlalchemy connection object. Useful when testing to allow operations with a transaction. Defaults to None. None secret_dict dict A dictionary of login secrets. Will override variables in the json secrets file if both provided None db_reader \u00b6 Table reader DBReader \u00b6 Base class for reading from the Azure database db_writer \u00b6 Table writer DBWriter \u00b6 Base class for writing to the Azure database commit_records ( self , records , on_conflict , table = None ) \u00b6 Commit records to the database Parameters: Name Type Description Default session a session object required records Either a list of sqlalchemy records, list of dictionaries (table arg must be provided) or an sqlalchemy subquery object (table arg must be provided) required on_conflict \"overwrite\" or \"ignore\". required table Optional. sqlalchemy table. If table provide sqlalchemy core used for insert None If table is provided it will insert using sqlalchemy's core rather than the ORM. update_remote_tables ( self ) \u00b6 Update all relevant tables on the remote database mixins special \u00b6 Mixins for databases and tables. instance_tables_mixin \u00b6 Table that summerises an instance (model + data + result). DataConfigMixin \u00b6 Table of model parameters. InstanceTableMixin \u00b6 Table of Instances. ModelTableMixin \u00b6 Table of model parameters. tables special \u00b6 Module for interacting with tables in the Azure Postgres database aqe_tables \u00b6 Tables for AQE data source AQEReading \u00b6 Table of AQE readings build_entry ( reading_dict , return_dict = False ) staticmethod \u00b6 Create an AQEReading entry, replacing empty strings with None If return_dict then return a dictionary rather than and entry, to allow inserting via sqlalchemy core AQESite \u00b6 Table of AQE sites build_entry ( site_dict ) staticmethod \u00b6 Create an AQESite entry, replacing empty strings with None features_tables \u00b6 Tables for intersection between datasource and interest points DynamicFeature \u00b6 Any model features that vary over time (and therefore need a start-time column) build_entry ( feature_name , reading_tuple ) staticmethod \u00b6 Create a DynamicFeature entry and return it StaticFeature \u00b6 Any model features that are static (and therefore do not need a start-time column) build_entry ( feature_name , reading_tuple ) staticmethod \u00b6 Create a StaticFeature entry and return it gla_scoot_tables \u00b6 Tables for GLA lockdown scoot ScootPercentChange \u00b6 Table of LAQN sites hexgrid_table \u00b6 Table for HexGrid static data HexGrid \u00b6 Table of static hexagonal grids jamcam_tables \u00b6 Tables for jamcam results JamCamFrameStats \u00b6 Table of LAQN sites JamCamVideoStats \u00b6 Table of LAQN sites laqn_tables \u00b6 Tables for LAQN data source LAQNReading \u00b6 Table of LAQN readings build_entry ( reading_dict , return_dict = False ) staticmethod \u00b6 Create an LAQNReading entry, replacing empty strings with None If return_dict then return a dictionary rather than and entry, to allow inserting via sqlalchemy core LAQNSite \u00b6 Table of LAQN sites build_entry ( site_dict ) staticmethod \u00b6 Create an LAQNSite entry, replacing empty strings with None londonboundary_table \u00b6 Table for London boundary static data LondonBoundary \u00b6 Table containing London boundary data meta_point_table \u00b6 Table for interest points MetaPoint \u00b6 Table of interest points build_entry ( source , latitude = None , longitude = None , geometry = None ) staticmethod \u00b6 Create an MetaPoint entry from a source and position details build_ewkt ( latitude , longitude ) staticmethod \u00b6 Create an EWKT geometry string from latitude and longitude model_results_table \u00b6 Tables for model results ModelResult \u00b6 Table of AQE sites oshighway_table \u00b6 Table for OS highways static data OSHighway \u00b6 Table of static OS highways data rectgrid_table \u00b6 Tables for AQE data source RectGrid \u00b6 Table of grid points build_entry ( gridcell_dict ) staticmethod \u00b6 Create a RectGrid entry and return it RectGrid100 \u00b6 Table of 100m grid points satellite_tables \u00b6 Tables for Satellite data SatelliteBox \u00b6 Locations of the discretised satellite locations build_box_ewkt ( latitude , longitude , half_grid ) staticmethod \u00b6 Create an EWKT geometry string from latitude and longitude and half the grid size build_entry ( lat , lon , half_grid ) staticmethod \u00b6 Create a SatelliteBox entry and return it build_ewkt ( latitude , longitude ) staticmethod \u00b6 Create an EWKT geometry string from latitude and longitude SatelliteForecast \u00b6 Table of Satellite forecasts SatelliteGrid \u00b6 Locations of the discretised satellite locations build_entry ( point_id , box_id ) staticmethod \u00b6 Create a SatelliteGrid entry and return it scoot_tables \u00b6 Tables for SCOOT data source ScootDetector \u00b6 Table of Scoot detectors ScootForecast \u00b6 Table of Scoot forecasts ScootReading \u00b6 Table of Scoot readings ScootRoadForecast \u00b6 Table of SCOOT forecasts for each road segment ScootRoadMatch \u00b6 Table of all roads and their associated SCOOT sensors ScootRoadReading \u00b6 Table of SCOOT readings for each road segment street_canyon_tables \u00b6 Table for OS highways static data StreetCanyon \u00b6 Table of static OS highways data traffic_modelling_tables \u00b6 Tables for the traffic modelling schema. TrafficDataTable \u00b6 Storing settings for traffic data. TrafficInstanceTable \u00b6 Store a traffic instance. TrafficMetricTable \u00b6 A table for storing metrics from traffic models. TrafficModelTable \u00b6 Storing model parameters and information. ukmap_tables \u00b6 Table for UKMap static data UKMap \u00b6 Table of static UKMap data urban_village_tables \u00b6 Table for urban village static data UrbanVillage \u00b6 Table of urban village data","title":"Databases"},{"location":"API/cleanair/databases/#databases","text":"","title":"Databases"},{"location":"API/cleanair/databases/#cleanair.databases","text":"Module for interacting with the Azure Postgres database","title":"cleanair.databases"},{"location":"API/cleanair/databases/#cleanair.databases.base","text":"Declarative base class and table initialisation","title":"base"},{"location":"API/cleanair/databases/#cleanair.databases.connector","text":"Class for connecting to Azure databases","title":"connector"},{"location":"API/cleanair/databases/#cleanair.databases.connector.Connector","text":"Base class for connecting to databases with sqlalchemy","title":"Connector"},{"location":"API/cleanair/databases/#cleanair.databases.connector.Connector.engine","text":"Access the class-level sqlalchemy engine","title":"engine"},{"location":"API/cleanair/databases/#cleanair.databases.connector.Connector.sessionfactory","text":"Access the class-level sqlalchemy sessionfactory","title":"sessionfactory"},{"location":"API/cleanair/databases/#cleanair.databases.connector.Connector.__init__","text":"Parameters: Name Type Description Default connection sqlalchemy.engine.Connection Pass an sqlalchemy connection object. Useful when testing to allow operations with a transaction None","title":"__init__()"},{"location":"API/cleanair/databases/#cleanair.databases.connector.Connector.check_internet_connection","text":"Check that the internet is accessible Repeated checks within interval seconds will be skipped","title":"check_internet_connection()"},{"location":"API/cleanair/databases/#cleanair.databases.connector.Connector.check_schema_exists","text":"Check if a schema exists","title":"check_schema_exists()"},{"location":"API/cleanair/databases/#cleanair.databases.connector.Connector.ensure_database_exists","text":"Ensure the database exists","title":"ensure_database_exists()"},{"location":"API/cleanair/databases/#cleanair.databases.connector.Connector.ensure_extensions","text":"Ensure required extensions are installed publicly","title":"ensure_extensions()"},{"location":"API/cleanair/databases/#cleanair.databases.connector.Connector.ensure_schema","text":"Ensure that requested schema exists","title":"ensure_schema()"},{"location":"API/cleanair/databases/#cleanair.databases.connector.Connector.initialise_tables","text":"Ensure that all table connections exist","title":"initialise_tables()"},{"location":"API/cleanair/databases/#cleanair.databases.connector.Connector.open_session","text":"Create a session as a context manager which will thereby self-close","title":"open_session()"},{"location":"API/cleanair/databases/#cleanair.databases.db_config","text":"Class for configuring database schema","title":"db_config"},{"location":"API/cleanair/databases/#cleanair.databases.db_config.DBConfig","text":"Class to manage database configuration","title":"DBConfig"},{"location":"API/cleanair/databases/#cleanair.databases.db_config.DBConfig.database_name","text":"Return name of database in config file","title":"database_name"},{"location":"API/cleanair/databases/#cleanair.databases.db_config.DBConfig.roles","text":"Return a list of roles","title":"roles"},{"location":"API/cleanair/databases/#cleanair.databases.db_config.DBConfig.schema","text":"Return a list of schemas from the database","title":"schema"},{"location":"API/cleanair/databases/#cleanair.databases.db_config.DBConfig.__init__","text":"Loads a database configuration file Parameters: Name Type Description Default config_file str Location of configuration file required Returns: Type Description None","title":"__init__()"},{"location":"API/cleanair/databases/#cleanair.databases.db_config.DBConfig.assign_role_connect","text":"Allow a role to connect to the database Parameters: Name Type Description Default role_name str Role name required Returns: Type Description None","title":"assign_role_connect()"},{"location":"API/cleanair/databases/#cleanair.databases.db_config.DBConfig.assign_role_schema_default_privilege","text":"Assign a role default privilegs Parameters: Name Type Description Default role_name str Role name required schema_name str Schema name required privileges Lis[str] A list of privileges to assign required Returns: Type Description None","title":"assign_role_schema_default_privilege()"},{"location":"API/cleanair/databases/#cleanair.databases.db_config.DBConfig.assign_role_schema_default_sequences","text":"Assign a role default sequences on a schema Parameters: Name Type Description Default role_name str Role name required schema_name str Schema name required Returns: Type Description None","title":"assign_role_schema_default_sequences()"},{"location":"API/cleanair/databases/#cleanair.databases.db_config.DBConfig.assign_role_schema_privilege","text":"Assign a role a list of privileges Parameters: Name Type Description Default role_name str Role name required schema_name str Schema name required privileges List[str] A list of privileges to assign required Returns: Type Description None","title":"assign_role_schema_privilege()"},{"location":"API/cleanair/databases/#cleanair.databases.db_config.DBConfig.assign_role_schema_sequences","text":"Assign a role to all sequences on schema Parameters: Name Type Description Default role_name str Role name required schema_name str Schema name required Returns: Type Description None","title":"assign_role_schema_sequences()"},{"location":"API/cleanair/databases/#cleanair.databases.db_config.DBConfig.assign_role_schema_usage","text":"Assign a role to all schemas Parameters: Name Type Description Default role_name str Role name required schema_name str Schema name required create bool Give the user CREATE on the schema (i.e. they can create objects) False Returns: Type Description None","title":"assign_role_schema_usage()"},{"location":"API/cleanair/databases/#cleanair.databases.db_config.DBConfig.configure_all_roles","text":"Configure roles as defined in the configuration file","title":"configure_all_roles()"},{"location":"API/cleanair/databases/#cleanair.databases.db_config.DBConfig.create_role","text":"Create a new role Parameters: Name Type Description Default role_name str Role name required Returns: Type Description None","title":"create_role()"},{"location":"API/cleanair/databases/#cleanair.databases.db_config.DBConfig.create_schema","text":"Create schemas on database","title":"create_schema()"},{"location":"API/cleanair/databases/#cleanair.databases.db_config.DBConfig.create_user","text":"Create a new user and assign to a role Parameters: Name Type Description Default username str A username required password str A user's password required Returns: Type Description None","title":"create_user()"},{"location":"API/cleanair/databases/#cleanair.databases.db_config.DBConfig.grant_role_to_user","text":"Parameters: Name Type Description Default username str A username required roles List[str] A list of roles to assign a user to required Returns: Type Description None","title":"grant_role_to_user()"},{"location":"API/cleanair/databases/#cleanair.databases.db_config.DBConfig.list_roles","text":"Return a list of existing database roles","title":"list_roles()"},{"location":"API/cleanair/databases/#cleanair.databases.db_config.DBConfig.read_config","text":"Read a database configuration file Parameters: Name Type Description Default config_file str Location of configuration file required Returns: Type Description dict A dictionary of the required configuration","title":"read_config()"},{"location":"API/cleanair/databases/#cleanair.databases.db_interactor","text":"Table reader/writer","title":"db_interactor"},{"location":"API/cleanair/databases/#cleanair.databases.db_interactor.DBInteractor","text":"Base class for interacting with tables in the Azure database","title":"DBInteractor"},{"location":"API/cleanair/databases/#cleanair.databases.db_interactor.DBInteractor.__init__","text":"Init method for connecting to database Parameters: Name Type Description Default secretfile str Path to a secret file (json). Can be the full path to secrets file or a filename if the secret is in a directory called '/secrets' required initialise_tables bool Create all tables. Default to False. Requires admin privileges on database. True connection sqlalchemy.engine.Connection Pass an sqlalchemy connection object. Useful when testing to allow operations with a transaction. Defaults to None. None secret_dict dict A dictionary of login secrets. Will override variables in the json secrets file if both provided None","title":"__init__()"},{"location":"API/cleanair/databases/#cleanair.databases.db_reader","text":"Table reader","title":"db_reader"},{"location":"API/cleanair/databases/#cleanair.databases.db_reader.DBReader","text":"Base class for reading from the Azure database","title":"DBReader"},{"location":"API/cleanair/databases/#cleanair.databases.db_writer","text":"Table writer","title":"db_writer"},{"location":"API/cleanair/databases/#cleanair.databases.db_writer.DBWriter","text":"Base class for writing to the Azure database","title":"DBWriter"},{"location":"API/cleanair/databases/#cleanair.databases.db_writer.DBWriter.commit_records","text":"Commit records to the database Parameters: Name Type Description Default session a session object required records Either a list of sqlalchemy records, list of dictionaries (table arg must be provided) or an sqlalchemy subquery object (table arg must be provided) required on_conflict \"overwrite\" or \"ignore\". required table Optional. sqlalchemy table. If table provide sqlalchemy core used for insert None If table is provided it will insert using sqlalchemy's core rather than the ORM.","title":"commit_records()"},{"location":"API/cleanair/databases/#cleanair.databases.db_writer.DBWriter.update_remote_tables","text":"Update all relevant tables on the remote database","title":"update_remote_tables()"},{"location":"API/cleanair/databases/#cleanair.databases.mixins","text":"Mixins for databases and tables.","title":"mixins"},{"location":"API/cleanair/databases/#cleanair.databases.mixins.instance_tables_mixin","text":"Table that summerises an instance (model + data + result).","title":"instance_tables_mixin"},{"location":"API/cleanair/databases/#cleanair.databases.mixins.instance_tables_mixin.DataConfigMixin","text":"Table of model parameters.","title":"DataConfigMixin"},{"location":"API/cleanair/databases/#cleanair.databases.mixins.instance_tables_mixin.InstanceTableMixin","text":"Table of Instances.","title":"InstanceTableMixin"},{"location":"API/cleanair/databases/#cleanair.databases.mixins.instance_tables_mixin.ModelTableMixin","text":"Table of model parameters.","title":"ModelTableMixin"},{"location":"API/cleanair/databases/#cleanair.databases.tables","text":"Module for interacting with tables in the Azure Postgres database","title":"tables"},{"location":"API/cleanair/databases/#cleanair.databases.tables.aqe_tables","text":"Tables for AQE data source","title":"aqe_tables"},{"location":"API/cleanair/databases/#cleanair.databases.tables.aqe_tables.AQEReading","text":"Table of AQE readings","title":"AQEReading"},{"location":"API/cleanair/databases/#cleanair.databases.tables.aqe_tables.AQEReading.build_entry","text":"Create an AQEReading entry, replacing empty strings with None If return_dict then return a dictionary rather than and entry, to allow inserting via sqlalchemy core","title":"build_entry()"},{"location":"API/cleanair/databases/#cleanair.databases.tables.aqe_tables.AQESite","text":"Table of AQE sites","title":"AQESite"},{"location":"API/cleanair/databases/#cleanair.databases.tables.aqe_tables.AQESite.build_entry","text":"Create an AQESite entry, replacing empty strings with None","title":"build_entry()"},{"location":"API/cleanair/databases/#cleanair.databases.tables.features_tables","text":"Tables for intersection between datasource and interest points","title":"features_tables"},{"location":"API/cleanair/databases/#cleanair.databases.tables.features_tables.DynamicFeature","text":"Any model features that vary over time (and therefore need a start-time column)","title":"DynamicFeature"},{"location":"API/cleanair/databases/#cleanair.databases.tables.features_tables.DynamicFeature.build_entry","text":"Create a DynamicFeature entry and return it","title":"build_entry()"},{"location":"API/cleanair/databases/#cleanair.databases.tables.features_tables.StaticFeature","text":"Any model features that are static (and therefore do not need a start-time column)","title":"StaticFeature"},{"location":"API/cleanair/databases/#cleanair.databases.tables.features_tables.StaticFeature.build_entry","text":"Create a StaticFeature entry and return it","title":"build_entry()"},{"location":"API/cleanair/databases/#cleanair.databases.tables.gla_scoot_tables","text":"Tables for GLA lockdown scoot","title":"gla_scoot_tables"},{"location":"API/cleanair/databases/#cleanair.databases.tables.gla_scoot_tables.ScootPercentChange","text":"Table of LAQN sites","title":"ScootPercentChange"},{"location":"API/cleanair/databases/#cleanair.databases.tables.hexgrid_table","text":"Table for HexGrid static data","title":"hexgrid_table"},{"location":"API/cleanair/databases/#cleanair.databases.tables.hexgrid_table.HexGrid","text":"Table of static hexagonal grids","title":"HexGrid"},{"location":"API/cleanair/databases/#cleanair.databases.tables.jamcam_tables","text":"Tables for jamcam results","title":"jamcam_tables"},{"location":"API/cleanair/databases/#cleanair.databases.tables.jamcam_tables.JamCamFrameStats","text":"Table of LAQN sites","title":"JamCamFrameStats"},{"location":"API/cleanair/databases/#cleanair.databases.tables.jamcam_tables.JamCamVideoStats","text":"Table of LAQN sites","title":"JamCamVideoStats"},{"location":"API/cleanair/databases/#cleanair.databases.tables.laqn_tables","text":"Tables for LAQN data source","title":"laqn_tables"},{"location":"API/cleanair/databases/#cleanair.databases.tables.laqn_tables.LAQNReading","text":"Table of LAQN readings","title":"LAQNReading"},{"location":"API/cleanair/databases/#cleanair.databases.tables.laqn_tables.LAQNReading.build_entry","text":"Create an LAQNReading entry, replacing empty strings with None If return_dict then return a dictionary rather than and entry, to allow inserting via sqlalchemy core","title":"build_entry()"},{"location":"API/cleanair/databases/#cleanair.databases.tables.laqn_tables.LAQNSite","text":"Table of LAQN sites","title":"LAQNSite"},{"location":"API/cleanair/databases/#cleanair.databases.tables.laqn_tables.LAQNSite.build_entry","text":"Create an LAQNSite entry, replacing empty strings with None","title":"build_entry()"},{"location":"API/cleanair/databases/#cleanair.databases.tables.londonboundary_table","text":"Table for London boundary static data","title":"londonboundary_table"},{"location":"API/cleanair/databases/#cleanair.databases.tables.londonboundary_table.LondonBoundary","text":"Table containing London boundary data","title":"LondonBoundary"},{"location":"API/cleanair/databases/#cleanair.databases.tables.meta_point_table","text":"Table for interest points","title":"meta_point_table"},{"location":"API/cleanair/databases/#cleanair.databases.tables.meta_point_table.MetaPoint","text":"Table of interest points","title":"MetaPoint"},{"location":"API/cleanair/databases/#cleanair.databases.tables.meta_point_table.MetaPoint.build_entry","text":"Create an MetaPoint entry from a source and position details","title":"build_entry()"},{"location":"API/cleanair/databases/#cleanair.databases.tables.meta_point_table.MetaPoint.build_ewkt","text":"Create an EWKT geometry string from latitude and longitude","title":"build_ewkt()"},{"location":"API/cleanair/databases/#cleanair.databases.tables.model_results_table","text":"Tables for model results","title":"model_results_table"},{"location":"API/cleanair/databases/#cleanair.databases.tables.model_results_table.ModelResult","text":"Table of AQE sites","title":"ModelResult"},{"location":"API/cleanair/databases/#cleanair.databases.tables.oshighway_table","text":"Table for OS highways static data","title":"oshighway_table"},{"location":"API/cleanair/databases/#cleanair.databases.tables.oshighway_table.OSHighway","text":"Table of static OS highways data","title":"OSHighway"},{"location":"API/cleanair/databases/#cleanair.databases.tables.rectgrid_table","text":"Tables for AQE data source","title":"rectgrid_table"},{"location":"API/cleanair/databases/#cleanair.databases.tables.rectgrid_table.RectGrid","text":"Table of grid points","title":"RectGrid"},{"location":"API/cleanair/databases/#cleanair.databases.tables.rectgrid_table.RectGrid.build_entry","text":"Create a RectGrid entry and return it","title":"build_entry()"},{"location":"API/cleanair/databases/#cleanair.databases.tables.rectgrid_table.RectGrid100","text":"Table of 100m grid points","title":"RectGrid100"},{"location":"API/cleanair/databases/#cleanair.databases.tables.satellite_tables","text":"Tables for Satellite data","title":"satellite_tables"},{"location":"API/cleanair/databases/#cleanair.databases.tables.satellite_tables.SatelliteBox","text":"Locations of the discretised satellite locations","title":"SatelliteBox"},{"location":"API/cleanair/databases/#cleanair.databases.tables.satellite_tables.SatelliteBox.build_box_ewkt","text":"Create an EWKT geometry string from latitude and longitude and half the grid size","title":"build_box_ewkt()"},{"location":"API/cleanair/databases/#cleanair.databases.tables.satellite_tables.SatelliteBox.build_entry","text":"Create a SatelliteBox entry and return it","title":"build_entry()"},{"location":"API/cleanair/databases/#cleanair.databases.tables.satellite_tables.SatelliteBox.build_ewkt","text":"Create an EWKT geometry string from latitude and longitude","title":"build_ewkt()"},{"location":"API/cleanair/databases/#cleanair.databases.tables.satellite_tables.SatelliteForecast","text":"Table of Satellite forecasts","title":"SatelliteForecast"},{"location":"API/cleanair/databases/#cleanair.databases.tables.satellite_tables.SatelliteGrid","text":"Locations of the discretised satellite locations","title":"SatelliteGrid"},{"location":"API/cleanair/databases/#cleanair.databases.tables.satellite_tables.SatelliteGrid.build_entry","text":"Create a SatelliteGrid entry and return it","title":"build_entry()"},{"location":"API/cleanair/databases/#cleanair.databases.tables.scoot_tables","text":"Tables for SCOOT data source","title":"scoot_tables"},{"location":"API/cleanair/databases/#cleanair.databases.tables.scoot_tables.ScootDetector","text":"Table of Scoot detectors","title":"ScootDetector"},{"location":"API/cleanair/databases/#cleanair.databases.tables.scoot_tables.ScootForecast","text":"Table of Scoot forecasts","title":"ScootForecast"},{"location":"API/cleanair/databases/#cleanair.databases.tables.scoot_tables.ScootReading","text":"Table of Scoot readings","title":"ScootReading"},{"location":"API/cleanair/databases/#cleanair.databases.tables.scoot_tables.ScootRoadForecast","text":"Table of SCOOT forecasts for each road segment","title":"ScootRoadForecast"},{"location":"API/cleanair/databases/#cleanair.databases.tables.scoot_tables.ScootRoadMatch","text":"Table of all roads and their associated SCOOT sensors","title":"ScootRoadMatch"},{"location":"API/cleanair/databases/#cleanair.databases.tables.scoot_tables.ScootRoadReading","text":"Table of SCOOT readings for each road segment","title":"ScootRoadReading"},{"location":"API/cleanair/databases/#cleanair.databases.tables.street_canyon_tables","text":"Table for OS highways static data","title":"street_canyon_tables"},{"location":"API/cleanair/databases/#cleanair.databases.tables.street_canyon_tables.StreetCanyon","text":"Table of static OS highways data","title":"StreetCanyon"},{"location":"API/cleanair/databases/#cleanair.databases.tables.traffic_modelling_tables","text":"Tables for the traffic modelling schema.","title":"traffic_modelling_tables"},{"location":"API/cleanair/databases/#cleanair.databases.tables.traffic_modelling_tables.TrafficDataTable","text":"Storing settings for traffic data.","title":"TrafficDataTable"},{"location":"API/cleanair/databases/#cleanair.databases.tables.traffic_modelling_tables.TrafficInstanceTable","text":"Store a traffic instance.","title":"TrafficInstanceTable"},{"location":"API/cleanair/databases/#cleanair.databases.tables.traffic_modelling_tables.TrafficMetricTable","text":"A table for storing metrics from traffic models.","title":"TrafficMetricTable"},{"location":"API/cleanair/databases/#cleanair.databases.tables.traffic_modelling_tables.TrafficModelTable","text":"Storing model parameters and information.","title":"TrafficModelTable"},{"location":"API/cleanair/databases/#cleanair.databases.tables.ukmap_tables","text":"Table for UKMap static data","title":"ukmap_tables"},{"location":"API/cleanair/databases/#cleanair.databases.tables.ukmap_tables.UKMap","text":"Table of static UKMap data","title":"UKMap"},{"location":"API/cleanair/databases/#cleanair.databases.tables.urban_village_tables","text":"Table for urban village static data","title":"urban_village_tables"},{"location":"API/cleanair/databases/#cleanair.databases.tables.urban_village_tables.UrbanVillage","text":"Table of urban village data","title":"UrbanVillage"},{"location":"API/cleanair/decorators/","text":"Decorators \u00b6 \u00b6 Module for interacting with the Azure Postgres database api \u00b6 API decorators robust_api ( api_call ) \u00b6 Kwargs n_repeat: Maximum number of calls to the API that can be made sleep_time: Time to sleep between api calls in seconds db \u00b6 DB decorators EmptyQueryError \u00b6 Raised when a database query returns no rows check_empty_df ( data_frame , raise_error = True ) \u00b6 Check a dataframe is not empty and raise and error if it is db_query ( query_f ) \u00b6 Kwargs output_type: Either 'query', 'subquery', 'df' or 'list'. list returns the first column of the query output \u00b6 Output suppression context manager from https://github.com/facebook/prophet/issues/223 SuppressStdoutStderr \u00b6 A context manager for doing a \"deep suppression\" of stdout and stderr in Python, i.e. will suppress all print, even if the print originates in a compiled C/Fortran sub-function. This will not suppress raised exceptions, since exceptions are printed to stderr just before a script exits, and after the context manager has exited (at least, I think that is why it lets exceptions through).","title":"Decorators"},{"location":"API/cleanair/decorators/#decorators","text":"","title":"Decorators"},{"location":"API/cleanair/decorators/#cleanair.decorators","text":"Module for interacting with the Azure Postgres database","title":"cleanair.decorators"},{"location":"API/cleanair/decorators/#cleanair.decorators.api","text":"API decorators","title":"api"},{"location":"API/cleanair/decorators/#cleanair.decorators.api.robust_api","text":"Kwargs n_repeat: Maximum number of calls to the API that can be made sleep_time: Time to sleep between api calls in seconds","title":"robust_api()"},{"location":"API/cleanair/decorators/#cleanair.decorators.db","text":"DB decorators","title":"db"},{"location":"API/cleanair/decorators/#cleanair.decorators.db.EmptyQueryError","text":"Raised when a database query returns no rows","title":"EmptyQueryError"},{"location":"API/cleanair/decorators/#cleanair.decorators.db.check_empty_df","text":"Check a dataframe is not empty and raise and error if it is","title":"check_empty_df()"},{"location":"API/cleanair/decorators/#cleanair.decorators.db.db_query","text":"Kwargs output_type: Either 'query', 'subquery', 'df' or 'list'. list returns the first column of the query","title":"db_query()"},{"location":"API/cleanair/decorators/#cleanair.decorators.output","text":"Output suppression context manager from https://github.com/facebook/prophet/issues/223","title":"output"},{"location":"API/cleanair/decorators/#cleanair.decorators.output.SuppressStdoutStderr","text":"A context manager for doing a \"deep suppression\" of stdout and stderr in Python, i.e. will suppress all print, even if the print originates in a compiled C/Fortran sub-function. This will not suppress raised exceptions, since exceptions are printed to stderr just before a script exits, and after the context manager has exited (at least, I think that is why it lets exceptions through).","title":"SuppressStdoutStderr"},{"location":"API/cleanair/features/","text":"Features \u00b6 \u00b6 Module for feature extraction feature_extractor \u00b6 Feature extraction Base class FeatureExtractor \u00b6 Extract features which are near to a given set of MetaPoints and inside London features property readonly \u00b6 A dictionary of features of the kind: { \"building_height\": { \"type\": \"value\", \"feature_dict\": { \"calculated_height_of_building\": [\"*\"] }, \"aggfunc\": max_ } } table property readonly \u00b6 Either returns an sql table instance or a subquery __init__ ( self , dynamic = False , batch_size = 1000 , sources = None , ** kwargs ) special \u00b6 Base class for extracting features. Parameters: Name Type Description Default dynamic Boolean. Set whether feature is dynamic (e.g. varies over time) Time must always be named measurement_start_utc False query_features ( self , feature_name , feature_type , agg_func ) \u00b6 For a given features, produce a query containing the full feature processing stage. This avoids complications when trying to work out which interest points have already been processed (easy for static features but very complicated for dynamic features). As we do not filter interest points as part of the query, it will stay the same size on repeated calls and can therefore be sliced for batch operations. query_input_geometries ( self , feature_name ) \u00b6 Query inputs selecting all input geometries matching the requirements in self.feature_dict query_meta_points ( self ) \u00b6 Query MetaPoints, selecting all matching sources. We do not filter these in order to ensure that repeated calls will return the same set of points. update_remote_tables ( self ) \u00b6 For each interest point location, for each feature, extract the geometry for that feature in each of the buffer radii then apply the appropriate aggregation function to extract a value for each buffer size. feature_funcs \u00b6 Functions for feature extractions avg_ ( x ) \u00b6 avg of x max_ ( x ) \u00b6 max of x min_ ( x ) \u00b6 min of x sum_ ( x ) \u00b6 sum of x sum_area ( geom ) \u00b6 Function to calculate the total area of a geometry sum_length ( geom ) \u00b6 Function to calculate the total length of linestring geometries os_highway_features \u00b6 OS Highway feature extraction OSHighwayFeatures \u00b6 Extract features for OSHighways scoot_features \u00b6 Scoot feature extraction ScootFeaturesBase \u00b6 Process SCOOT values into model features table property readonly \u00b6 Join the geometry column from OSHighway onto the relevant SCOOT table for feature extraction __init__ ( self , table_class , value_type , ** kwargs ) special \u00b6 Base class for extracting features. Parameters: Name Type Description Default dynamic Boolean. Set whether feature is dynamic (e.g. varies over time) Time must always be named measurement_start_utc required ScootForecastFeatures \u00b6 Process SCOOT forecasts into model features ScootReadingFeatures \u00b6 Process SCOOT readings into model features streetcanyon_features \u00b6 Street Canyon feature extraction StreetCanyonFeatures \u00b6 Extract features for StreetCanyon ukmap_features \u00b6 UKMAP feature extraction UKMapFeatures \u00b6 Extract features for UKMap","title":"Features"},{"location":"API/cleanair/features/#features","text":"","title":"Features"},{"location":"API/cleanair/features/#cleanair.features","text":"Module for feature extraction","title":"cleanair.features"},{"location":"API/cleanair/features/#cleanair.features.feature_extractor","text":"Feature extraction Base class","title":"feature_extractor"},{"location":"API/cleanair/features/#cleanair.features.feature_extractor.FeatureExtractor","text":"Extract features which are near to a given set of MetaPoints and inside London","title":"FeatureExtractor"},{"location":"API/cleanair/features/#cleanair.features.feature_extractor.FeatureExtractor.features","text":"A dictionary of features of the kind: { \"building_height\": { \"type\": \"value\", \"feature_dict\": { \"calculated_height_of_building\": [\"*\"] }, \"aggfunc\": max_ } }","title":"features"},{"location":"API/cleanair/features/#cleanair.features.feature_extractor.FeatureExtractor.table","text":"Either returns an sql table instance or a subquery","title":"table"},{"location":"API/cleanair/features/#cleanair.features.feature_extractor.FeatureExtractor.__init__","text":"Base class for extracting features. Parameters: Name Type Description Default dynamic Boolean. Set whether feature is dynamic (e.g. varies over time) Time must always be named measurement_start_utc False","title":"__init__()"},{"location":"API/cleanair/features/#cleanair.features.feature_extractor.FeatureExtractor.query_features","text":"For a given features, produce a query containing the full feature processing stage. This avoids complications when trying to work out which interest points have already been processed (easy for static features but very complicated for dynamic features). As we do not filter interest points as part of the query, it will stay the same size on repeated calls and can therefore be sliced for batch operations.","title":"query_features()"},{"location":"API/cleanair/features/#cleanair.features.feature_extractor.FeatureExtractor.query_input_geometries","text":"Query inputs selecting all input geometries matching the requirements in self.feature_dict","title":"query_input_geometries()"},{"location":"API/cleanair/features/#cleanair.features.feature_extractor.FeatureExtractor.query_meta_points","text":"Query MetaPoints, selecting all matching sources. We do not filter these in order to ensure that repeated calls will return the same set of points.","title":"query_meta_points()"},{"location":"API/cleanair/features/#cleanair.features.feature_extractor.FeatureExtractor.update_remote_tables","text":"For each interest point location, for each feature, extract the geometry for that feature in each of the buffer radii then apply the appropriate aggregation function to extract a value for each buffer size.","title":"update_remote_tables()"},{"location":"API/cleanair/features/#cleanair.features.feature_funcs","text":"Functions for feature extractions","title":"feature_funcs"},{"location":"API/cleanair/features/#cleanair.features.feature_funcs.avg_","text":"avg of x","title":"avg_()"},{"location":"API/cleanair/features/#cleanair.features.feature_funcs.max_","text":"max of x","title":"max_()"},{"location":"API/cleanair/features/#cleanair.features.feature_funcs.min_","text":"min of x","title":"min_()"},{"location":"API/cleanair/features/#cleanair.features.feature_funcs.sum_","text":"sum of x","title":"sum_()"},{"location":"API/cleanair/features/#cleanair.features.feature_funcs.sum_area","text":"Function to calculate the total area of a geometry","title":"sum_area()"},{"location":"API/cleanair/features/#cleanair.features.feature_funcs.sum_length","text":"Function to calculate the total length of linestring geometries","title":"sum_length()"},{"location":"API/cleanair/features/#cleanair.features.os_highway_features","text":"OS Highway feature extraction","title":"os_highway_features"},{"location":"API/cleanair/features/#cleanair.features.os_highway_features.OSHighwayFeatures","text":"Extract features for OSHighways","title":"OSHighwayFeatures"},{"location":"API/cleanair/features/#cleanair.features.scoot_features","text":"Scoot feature extraction","title":"scoot_features"},{"location":"API/cleanair/features/#cleanair.features.scoot_features.ScootFeaturesBase","text":"Process SCOOT values into model features","title":"ScootFeaturesBase"},{"location":"API/cleanair/features/#cleanair.features.scoot_features.ScootFeaturesBase.table","text":"Join the geometry column from OSHighway onto the relevant SCOOT table for feature extraction","title":"table"},{"location":"API/cleanair/features/#cleanair.features.scoot_features.ScootFeaturesBase.__init__","text":"Base class for extracting features. Parameters: Name Type Description Default dynamic Boolean. Set whether feature is dynamic (e.g. varies over time) Time must always be named measurement_start_utc required","title":"__init__()"},{"location":"API/cleanair/features/#cleanair.features.scoot_features.ScootForecastFeatures","text":"Process SCOOT forecasts into model features","title":"ScootForecastFeatures"},{"location":"API/cleanair/features/#cleanair.features.scoot_features.ScootReadingFeatures","text":"Process SCOOT readings into model features","title":"ScootReadingFeatures"},{"location":"API/cleanair/features/#cleanair.features.streetcanyon_features","text":"Street Canyon feature extraction","title":"streetcanyon_features"},{"location":"API/cleanair/features/#cleanair.features.streetcanyon_features.StreetCanyonFeatures","text":"Extract features for StreetCanyon","title":"StreetCanyonFeatures"},{"location":"API/cleanair/features/#cleanair.features.ukmap_features","text":"UKMAP feature extraction","title":"ukmap_features"},{"location":"API/cleanair/features/#cleanair.features.ukmap_features.UKMapFeatures","text":"Extract features for UKMap","title":"UKMapFeatures"},{"location":"API/cleanair/inputs/","text":"Inputs \u00b6 \u00b6 Module for input datasources aqe_writer \u00b6 Get data from the AQE network via the API AQEWriter \u00b6 Manage interactions with the AQE table on Azure __init__ ( self , ** kwargs ) special \u00b6 Init method for connecting to database Parameters: Name Type Description Default secretfile str Path to a secret file (json). Can be the full path to secrets file or a filename if the secret is in a directory called '/secrets' required initialise_tables bool Create all tables. Default to False. Requires admin privileges on database. required connection sqlalchemy.engine.Connection Pass an sqlalchemy connection object. Useful when testing to allow operations with a transaction. Defaults to None. required secret_dict dict A dictionary of login secrets. Will override variables in the json secrets file if both provided required request_site_entries ( self ) \u00b6 Request all AQE sites Remove any that do not have an opening date request_site_readings ( self , start_date , end_date , site_code ) \u00b6 Request all readings for {site_code} between {start_date} and {end_date} Remove duplicates and add the site_code update_reading_table ( self , usecore = True ) \u00b6 Update the readings table with new sensor readings. update_remote_tables ( self ) \u00b6 Update all relevant tables on the remote database update_site_list_table ( self ) \u00b6 Update the aqe_site table laqn_writer \u00b6 LAQN LAQNWriter \u00b6 Get data from the LAQN network via the API maintained by Kings College London: (https://www.londonair.org.uk/Londonair/API/) __init__ ( self , ** kwargs ) special \u00b6 Init method for connecting to database Parameters: Name Type Description Default secretfile str Path to a secret file (json). Can be the full path to secrets file or a filename if the secret is in a directory called '/secrets' required initialise_tables bool Create all tables. Default to False. Requires admin privileges on database. required connection sqlalchemy.engine.Connection Pass an sqlalchemy connection object. Useful when testing to allow operations with a transaction. Defaults to None. required secret_dict dict A dictionary of login secrets. Will override variables in the json secrets file if both provided required request_site_entries ( self ) \u00b6 Request all LAQN sites Remove any that do not have an opening date request_site_readings ( self , start_date , end_date , site_code ) \u00b6 Request all readings for {site_code} between {start_date} and {end_date} Remove duplicates and add the site_code update_reading_table ( self , usecore = True ) \u00b6 Update the readings table with new sensor readings. update_remote_tables ( self ) \u00b6 Update all relevant tables on the remote database update_site_list_table ( self ) \u00b6 Update the laqn_site table rectgrid_writer \u00b6 Get data from the AQE network via the API RectGridWriter \u00b6 Manage interactions with the RectGrid table on Azure build_cell ( self , latitude , longitude ) \u00b6 Build a rectangular cell around a given latitude and longitude update_remote_tables ( self ) \u00b6 Upload grid data satellite_writer \u00b6 Satellite SatelliteWriter \u00b6 Get Satellite data from (https://download.regional.atmosphere.copernicus.eu/services/CAMS50) API INFO: https://www.regional.atmosphere.copernicus.eu/doc/Guide_Numerical_Data_CAMS_new.pdf IMPORTANT: Satellite forecast data should become available on API at: 06:30 UTC for 0-48 hours. 08:30 UTC for 49-72 hours. __init__ ( self , copernicus_key , ** kwargs ) special \u00b6 Init method for connecting to database Parameters: Name Type Description Default secretfile str Path to a secret file (json). Can be the full path to secrets file or a filename if the secret is in a directory called '/secrets' required initialise_tables bool Create all tables. Default to False. Requires admin privileges on database. required connection sqlalchemy.engine.Connection Pass an sqlalchemy connection object. Useful when testing to allow operations with a transaction. Defaults to None. required secret_dict dict A dictionary of login secrets. Will override variables in the json secrets file if both provided required build_satellite_grid ( self , satellite_boxes_df ) \u00b6 Build a dataframe of satellite grid points given a dataframe of satellite boxes get_response ( api_endpoint , params = None , timeout = 60.0 ) staticmethod \u00b6 Return the response from an API grib_to_df ( self , satellite_bytes , period , species ) \u00b6 Take satellite bytes and load into a pandas dataframe, converting NO2 units if required read_grib_file ( self , filecontent , period ) \u00b6 Read a grib file into a pandas dataframe request_satellite_data ( self , start_date , period , pollutant ) \u00b6 Request satellite data. Will make multiple attempts to read data and raise an exception if it fails Parameters: Name Type Description Default start_date Date to collect data from required period The time periods to request data for: 0H24H, 25H48H, 49H72H required pollutant 'NO2', 'PM25', 'PM10', 'O3' required update_interest_points ( self ) \u00b6 Create interest points and insert into the database update_reading_table ( self ) \u00b6 Update the satellite reading table update_remote_tables ( self ) \u00b6 Update all relevant tables on the remote database scoot_writer \u00b6 Scoot ScootWriter \u00b6 Class to get data from the Scoot traffic detector network via the S3 bucket maintained by TfL: (https://s3.console.aws.amazon.com/s3/buckets/surface.data.tfl.gov.uk) __init__ ( self , aws_key_id , aws_key , detector_ids = None , ** kwargs ) special \u00b6 Init method for connecting to database Parameters: Name Type Description Default secretfile str Path to a secret file (json). Can be the full path to secrets file or a filename if the secret is in a directory called '/secrets' required initialise_tables bool Create all tables. Default to False. Requires admin privileges on database. required connection sqlalchemy.engine.Connection Pass an sqlalchemy connection object. Useful when testing to allow operations with a transaction. Defaults to None. required secret_dict dict A dictionary of login secrets. Will override variables in the json secrets file if both provided required aggregate_scoot_data ( self , df_readings ) \u00b6 Aggregate scoot data by detector ID into hourly chunks combine_by_detector_id ( self , input_df ) \u00b6 Aggregate measurements by detector ID across several readings get_existing_scoot_data ( self ) \u00b6 Get all the SCOOT readings already in the database for the given time range and set of detector IDs get_remote_filenames ( start_datetime , end_datetime ) staticmethod \u00b6 Get all possible remote file details for the period in question request_remote_data ( self , start_datetime , end_datetime , detector_ids ) \u00b6 Request all readings between {start_date} and {end_date}. Remove readings with unknown detector ID or detector faults. request_site_entries ( self ) \u00b6 Get list of known detectors update_remote_tables ( self ) \u00b6 Update the database with new Scoot traffic data. static_writer \u00b6 Upload static data currently held in geodatabase/shape file format in Azure Convert to PostgreSQL using ogr2ogr and upload to the inputs DB StaticWriter \u00b6 Manage interactions with the static database on Azure schema_table property readonly \u00b6 Get schema and table where the current dataset will live __init__ ( self , target_file , schema , table , ** kwargs ) special \u00b6 Create a StaticWrite instance for writing static datasets to a database Parameters: Name Type Description Default target_file str Either the path to a target file, or a directory if a shape file required schema str Name of the database schema to write to required table str Name of the table to write to required configure_tables ( self ) \u00b6 Tidy up the databases by doing the following: Ensuring there is an index on the geometry column Dropping (some) duplicate rows Dropping duplicate/unnecessary columns Converting some column types Adding a primary key update_remote_tables ( self ) \u00b6 Attempt to upload static files and configure the tables if successful upload_static_files ( self ) \u00b6 Upload static data to the inputs database","title":"Inputs"},{"location":"API/cleanair/inputs/#inputs","text":"","title":"Inputs"},{"location":"API/cleanair/inputs/#cleanair.inputs","text":"Module for input datasources","title":"cleanair.inputs"},{"location":"API/cleanair/inputs/#cleanair.inputs.aqe_writer","text":"Get data from the AQE network via the API","title":"aqe_writer"},{"location":"API/cleanair/inputs/#cleanair.inputs.aqe_writer.AQEWriter","text":"Manage interactions with the AQE table on Azure","title":"AQEWriter"},{"location":"API/cleanair/inputs/#cleanair.inputs.aqe_writer.AQEWriter.__init__","text":"Init method for connecting to database Parameters: Name Type Description Default secretfile str Path to a secret file (json). Can be the full path to secrets file or a filename if the secret is in a directory called '/secrets' required initialise_tables bool Create all tables. Default to False. Requires admin privileges on database. required connection sqlalchemy.engine.Connection Pass an sqlalchemy connection object. Useful when testing to allow operations with a transaction. Defaults to None. required secret_dict dict A dictionary of login secrets. Will override variables in the json secrets file if both provided required","title":"__init__()"},{"location":"API/cleanair/inputs/#cleanair.inputs.aqe_writer.AQEWriter.request_site_entries","text":"Request all AQE sites Remove any that do not have an opening date","title":"request_site_entries()"},{"location":"API/cleanair/inputs/#cleanair.inputs.aqe_writer.AQEWriter.request_site_readings","text":"Request all readings for {site_code} between {start_date} and {end_date} Remove duplicates and add the site_code","title":"request_site_readings()"},{"location":"API/cleanair/inputs/#cleanair.inputs.aqe_writer.AQEWriter.update_reading_table","text":"Update the readings table with new sensor readings.","title":"update_reading_table()"},{"location":"API/cleanair/inputs/#cleanair.inputs.aqe_writer.AQEWriter.update_remote_tables","text":"Update all relevant tables on the remote database","title":"update_remote_tables()"},{"location":"API/cleanair/inputs/#cleanair.inputs.aqe_writer.AQEWriter.update_site_list_table","text":"Update the aqe_site table","title":"update_site_list_table()"},{"location":"API/cleanair/inputs/#cleanair.inputs.laqn_writer","text":"LAQN","title":"laqn_writer"},{"location":"API/cleanair/inputs/#cleanair.inputs.laqn_writer.LAQNWriter","text":"Get data from the LAQN network via the API maintained by Kings College London: (https://www.londonair.org.uk/Londonair/API/)","title":"LAQNWriter"},{"location":"API/cleanair/inputs/#cleanair.inputs.laqn_writer.LAQNWriter.__init__","text":"Init method for connecting to database Parameters: Name Type Description Default secretfile str Path to a secret file (json). Can be the full path to secrets file or a filename if the secret is in a directory called '/secrets' required initialise_tables bool Create all tables. Default to False. Requires admin privileges on database. required connection sqlalchemy.engine.Connection Pass an sqlalchemy connection object. Useful when testing to allow operations with a transaction. Defaults to None. required secret_dict dict A dictionary of login secrets. Will override variables in the json secrets file if both provided required","title":"__init__()"},{"location":"API/cleanair/inputs/#cleanair.inputs.laqn_writer.LAQNWriter.request_site_entries","text":"Request all LAQN sites Remove any that do not have an opening date","title":"request_site_entries()"},{"location":"API/cleanair/inputs/#cleanair.inputs.laqn_writer.LAQNWriter.request_site_readings","text":"Request all readings for {site_code} between {start_date} and {end_date} Remove duplicates and add the site_code","title":"request_site_readings()"},{"location":"API/cleanair/inputs/#cleanair.inputs.laqn_writer.LAQNWriter.update_reading_table","text":"Update the readings table with new sensor readings.","title":"update_reading_table()"},{"location":"API/cleanair/inputs/#cleanair.inputs.laqn_writer.LAQNWriter.update_remote_tables","text":"Update all relevant tables on the remote database","title":"update_remote_tables()"},{"location":"API/cleanair/inputs/#cleanair.inputs.laqn_writer.LAQNWriter.update_site_list_table","text":"Update the laqn_site table","title":"update_site_list_table()"},{"location":"API/cleanair/inputs/#cleanair.inputs.rectgrid_writer","text":"Get data from the AQE network via the API","title":"rectgrid_writer"},{"location":"API/cleanair/inputs/#cleanair.inputs.rectgrid_writer.RectGridWriter","text":"Manage interactions with the RectGrid table on Azure","title":"RectGridWriter"},{"location":"API/cleanair/inputs/#cleanair.inputs.rectgrid_writer.RectGridWriter.build_cell","text":"Build a rectangular cell around a given latitude and longitude","title":"build_cell()"},{"location":"API/cleanair/inputs/#cleanair.inputs.rectgrid_writer.RectGridWriter.update_remote_tables","text":"Upload grid data","title":"update_remote_tables()"},{"location":"API/cleanair/inputs/#cleanair.inputs.satellite_writer","text":"Satellite","title":"satellite_writer"},{"location":"API/cleanair/inputs/#cleanair.inputs.satellite_writer.SatelliteWriter","text":"Get Satellite data from (https://download.regional.atmosphere.copernicus.eu/services/CAMS50) API INFO: https://www.regional.atmosphere.copernicus.eu/doc/Guide_Numerical_Data_CAMS_new.pdf IMPORTANT: Satellite forecast data should become available on API at: 06:30 UTC for 0-48 hours. 08:30 UTC for 49-72 hours.","title":"SatelliteWriter"},{"location":"API/cleanair/inputs/#cleanair.inputs.satellite_writer.SatelliteWriter.__init__","text":"Init method for connecting to database Parameters: Name Type Description Default secretfile str Path to a secret file (json). Can be the full path to secrets file or a filename if the secret is in a directory called '/secrets' required initialise_tables bool Create all tables. Default to False. Requires admin privileges on database. required connection sqlalchemy.engine.Connection Pass an sqlalchemy connection object. Useful when testing to allow operations with a transaction. Defaults to None. required secret_dict dict A dictionary of login secrets. Will override variables in the json secrets file if both provided required","title":"__init__()"},{"location":"API/cleanair/inputs/#cleanair.inputs.satellite_writer.SatelliteWriter.build_satellite_grid","text":"Build a dataframe of satellite grid points given a dataframe of satellite boxes","title":"build_satellite_grid()"},{"location":"API/cleanair/inputs/#cleanair.inputs.satellite_writer.SatelliteWriter.get_response","text":"Return the response from an API","title":"get_response()"},{"location":"API/cleanair/inputs/#cleanair.inputs.satellite_writer.SatelliteWriter.grib_to_df","text":"Take satellite bytes and load into a pandas dataframe, converting NO2 units if required","title":"grib_to_df()"},{"location":"API/cleanair/inputs/#cleanair.inputs.satellite_writer.SatelliteWriter.read_grib_file","text":"Read a grib file into a pandas dataframe","title":"read_grib_file()"},{"location":"API/cleanair/inputs/#cleanair.inputs.satellite_writer.SatelliteWriter.request_satellite_data","text":"Request satellite data. Will make multiple attempts to read data and raise an exception if it fails Parameters: Name Type Description Default start_date Date to collect data from required period The time periods to request data for: 0H24H, 25H48H, 49H72H required pollutant 'NO2', 'PM25', 'PM10', 'O3' required","title":"request_satellite_data()"},{"location":"API/cleanair/inputs/#cleanair.inputs.satellite_writer.SatelliteWriter.update_interest_points","text":"Create interest points and insert into the database","title":"update_interest_points()"},{"location":"API/cleanair/inputs/#cleanair.inputs.satellite_writer.SatelliteWriter.update_reading_table","text":"Update the satellite reading table","title":"update_reading_table()"},{"location":"API/cleanair/inputs/#cleanair.inputs.satellite_writer.SatelliteWriter.update_remote_tables","text":"Update all relevant tables on the remote database","title":"update_remote_tables()"},{"location":"API/cleanair/inputs/#cleanair.inputs.scoot_writer","text":"Scoot","title":"scoot_writer"},{"location":"API/cleanair/inputs/#cleanair.inputs.scoot_writer.ScootWriter","text":"Class to get data from the Scoot traffic detector network via the S3 bucket maintained by TfL: (https://s3.console.aws.amazon.com/s3/buckets/surface.data.tfl.gov.uk)","title":"ScootWriter"},{"location":"API/cleanair/inputs/#cleanair.inputs.scoot_writer.ScootWriter.__init__","text":"Init method for connecting to database Parameters: Name Type Description Default secretfile str Path to a secret file (json). Can be the full path to secrets file or a filename if the secret is in a directory called '/secrets' required initialise_tables bool Create all tables. Default to False. Requires admin privileges on database. required connection sqlalchemy.engine.Connection Pass an sqlalchemy connection object. Useful when testing to allow operations with a transaction. Defaults to None. required secret_dict dict A dictionary of login secrets. Will override variables in the json secrets file if both provided required","title":"__init__()"},{"location":"API/cleanair/inputs/#cleanair.inputs.scoot_writer.ScootWriter.aggregate_scoot_data","text":"Aggregate scoot data by detector ID into hourly chunks","title":"aggregate_scoot_data()"},{"location":"API/cleanair/inputs/#cleanair.inputs.scoot_writer.ScootWriter.combine_by_detector_id","text":"Aggregate measurements by detector ID across several readings","title":"combine_by_detector_id()"},{"location":"API/cleanair/inputs/#cleanair.inputs.scoot_writer.ScootWriter.get_existing_scoot_data","text":"Get all the SCOOT readings already in the database for the given time range and set of detector IDs","title":"get_existing_scoot_data()"},{"location":"API/cleanair/inputs/#cleanair.inputs.scoot_writer.ScootWriter.get_remote_filenames","text":"Get all possible remote file details for the period in question","title":"get_remote_filenames()"},{"location":"API/cleanair/inputs/#cleanair.inputs.scoot_writer.ScootWriter.request_remote_data","text":"Request all readings between {start_date} and {end_date}. Remove readings with unknown detector ID or detector faults.","title":"request_remote_data()"},{"location":"API/cleanair/inputs/#cleanair.inputs.scoot_writer.ScootWriter.request_site_entries","text":"Get list of known detectors","title":"request_site_entries()"},{"location":"API/cleanair/inputs/#cleanair.inputs.scoot_writer.ScootWriter.update_remote_tables","text":"Update the database with new Scoot traffic data.","title":"update_remote_tables()"},{"location":"API/cleanair/inputs/#cleanair.inputs.static_writer","text":"Upload static data currently held in geodatabase/shape file format in Azure Convert to PostgreSQL using ogr2ogr and upload to the inputs DB","title":"static_writer"},{"location":"API/cleanair/inputs/#cleanair.inputs.static_writer.StaticWriter","text":"Manage interactions with the static database on Azure","title":"StaticWriter"},{"location":"API/cleanair/inputs/#cleanair.inputs.static_writer.StaticWriter.schema_table","text":"Get schema and table where the current dataset will live","title":"schema_table"},{"location":"API/cleanair/inputs/#cleanair.inputs.static_writer.StaticWriter.__init__","text":"Create a StaticWrite instance for writing static datasets to a database Parameters: Name Type Description Default target_file str Either the path to a target file, or a directory if a shape file required schema str Name of the database schema to write to required table str Name of the table to write to required","title":"__init__()"},{"location":"API/cleanair/inputs/#cleanair.inputs.static_writer.StaticWriter.configure_tables","text":"Tidy up the databases by doing the following: Ensuring there is an index on the geometry column Dropping (some) duplicate rows Dropping duplicate/unnecessary columns Converting some column types Adding a primary key","title":"configure_tables()"},{"location":"API/cleanair/inputs/#cleanair.inputs.static_writer.StaticWriter.update_remote_tables","text":"Attempt to upload static files and configure the tables if successful","title":"update_remote_tables()"},{"location":"API/cleanair/inputs/#cleanair.inputs.static_writer.StaticWriter.upload_static_files","text":"Upload static data to the inputs database","title":"upload_static_files()"},{"location":"API/cleanair/instance/","text":"Instance \u00b6 \u00b6 Instances that store models, data and parameter summeries. instance \u00b6 Instances of models and data. Instance \u00b6 An instance is one model trained and fitted on some data. cluster_id: str property writable \u00b6 The id of the machine this instance was executed on. data_id: str property writable \u00b6 Data id of configuration of input data. fit_start_time: str property writable \u00b6 The datetime when the model started fitting. git_hash: str property writable \u00b6 A hash of the code version. Note there must exist a .git directory if you do not pass a git hash in the init. instance_id: str property writable \u00b6 A unique id created by hashing the model_name, param_id, data_id and git_hash model_name: str property writable \u00b6 Name of the model. param_id: str property writable \u00b6 Parameter id of the model. tag: str property writable \u00b6 A tag to categorise the instance. __init__ ( self , model_name , param_id , data_id , cluster_id , tag , fit_start_time , git_hash = None , secretfile = None ) special \u00b6 The instance id is created using the model_name, param_id, data_id and git_hash. hash ( self ) \u00b6 Hash the model name, param id, data id and git hash return a unique id. hash_dict ( value ) staticmethod \u00b6 Dumps a dictionary to json string then hashes that string. Parameters: Name Type Description Default value dict A dictionary to hash. Keys and values must be compliant with json types. required Returns: Type Description str The hash of dictionary. Notes Any lists within the dictionary are sorted. This means the following two dictionaries A and B will be hashed to the same string: A = dict(key=[\"a\", \"b\"]) B = dict(key=[\"b\", \"a\"]) hash_fn ( hash_string ) staticmethod \u00b6 Uses sha256 to hash the given string. Parameters: Name Type Description Default hash_string str The string to hash. required Returns: Type Description str The hash of the given string. to_dict ( self ) \u00b6 Returns a dictionary of the attributes of the Instance. Returns: Type Description dict Contains instance id, param id, data id, cluster id, fit start time, tag, git hash and model name as keys. Values are all strings. update_remote_tables ( self ) \u00b6 Update the instance table in the database.","title":"Instance"},{"location":"API/cleanair/instance/#instance","text":"","title":"Instance"},{"location":"API/cleanair/instance/#cleanair.instance","text":"Instances that store models, data and parameter summeries.","title":"cleanair.instance"},{"location":"API/cleanair/instance/#cleanair.instance.instance","text":"Instances of models and data.","title":"instance"},{"location":"API/cleanair/instance/#cleanair.instance.instance.Instance","text":"An instance is one model trained and fitted on some data.","title":"Instance"},{"location":"API/cleanair/instance/#cleanair.instance.instance.Instance.cluster_id","text":"The id of the machine this instance was executed on.","title":"cluster_id"},{"location":"API/cleanair/instance/#cleanair.instance.instance.Instance.data_id","text":"Data id of configuration of input data.","title":"data_id"},{"location":"API/cleanair/instance/#cleanair.instance.instance.Instance.fit_start_time","text":"The datetime when the model started fitting.","title":"fit_start_time"},{"location":"API/cleanair/instance/#cleanair.instance.instance.Instance.git_hash","text":"A hash of the code version. Note there must exist a .git directory if you do not pass a git hash in the init.","title":"git_hash"},{"location":"API/cleanair/instance/#cleanair.instance.instance.Instance.instance_id","text":"A unique id created by hashing the model_name, param_id, data_id and git_hash","title":"instance_id"},{"location":"API/cleanair/instance/#cleanair.instance.instance.Instance.model_name","text":"Name of the model.","title":"model_name"},{"location":"API/cleanair/instance/#cleanair.instance.instance.Instance.param_id","text":"Parameter id of the model.","title":"param_id"},{"location":"API/cleanair/instance/#cleanair.instance.instance.Instance.tag","text":"A tag to categorise the instance.","title":"tag"},{"location":"API/cleanair/instance/#cleanair.instance.instance.Instance.__init__","text":"The instance id is created using the model_name, param_id, data_id and git_hash.","title":"__init__()"},{"location":"API/cleanair/instance/#cleanair.instance.instance.Instance.hash","text":"Hash the model name, param id, data id and git hash return a unique id.","title":"hash()"},{"location":"API/cleanair/instance/#cleanair.instance.instance.Instance.hash_dict","text":"Dumps a dictionary to json string then hashes that string. Parameters: Name Type Description Default value dict A dictionary to hash. Keys and values must be compliant with json types. required Returns: Type Description str The hash of dictionary. Notes Any lists within the dictionary are sorted. This means the following two dictionaries A and B will be hashed to the same string: A = dict(key=[\"a\", \"b\"]) B = dict(key=[\"b\", \"a\"])","title":"hash_dict()"},{"location":"API/cleanair/instance/#cleanair.instance.instance.Instance.hash_fn","text":"Uses sha256 to hash the given string. Parameters: Name Type Description Default hash_string str The string to hash. required Returns: Type Description str The hash of the given string.","title":"hash_fn()"},{"location":"API/cleanair/instance/#cleanair.instance.instance.Instance.to_dict","text":"Returns a dictionary of the attributes of the Instance. Returns: Type Description dict Contains instance id, param id, data id, cluster id, fit start time, tag, git hash and model name as keys. Values are all strings.","title":"to_dict()"},{"location":"API/cleanair/instance/#cleanair.instance.instance.Instance.update_remote_tables","text":"Update the instance table in the database.","title":"update_remote_tables()"},{"location":"API/cleanair/loggers/","text":"Loggers \u00b6 \u00b6 Loggers logcolours \u00b6 log colours bold ( text ) \u00b6 Make text bold green ( text ) \u00b6 Make text green red ( text ) \u00b6 Make text red logsettings \u00b6 Central management of logger settings get_logger ( name ) \u00b6 Return a logger with the appropriate name initialise_logging ( verbosity ) \u00b6 Initialise logging for this process logutils \u00b6 Useful logging utilities duration ( start_time , end_time ) \u00b6 Get a human-readable duration from a start and end time in seconds duration_from_seconds ( seconds ) \u00b6 Get a human-readable duration from a number of seconds","title":"Loggers"},{"location":"API/cleanair/loggers/#loggers","text":"","title":"Loggers"},{"location":"API/cleanair/loggers/#cleanair.loggers","text":"Loggers","title":"cleanair.loggers"},{"location":"API/cleanair/loggers/#cleanair.loggers.logcolours","text":"log colours","title":"logcolours"},{"location":"API/cleanair/loggers/#cleanair.loggers.logcolours.bold","text":"Make text bold","title":"bold()"},{"location":"API/cleanair/loggers/#cleanair.loggers.logcolours.green","text":"Make text green","title":"green()"},{"location":"API/cleanair/loggers/#cleanair.loggers.logcolours.red","text":"Make text red","title":"red()"},{"location":"API/cleanair/loggers/#cleanair.loggers.logsettings","text":"Central management of logger settings","title":"logsettings"},{"location":"API/cleanair/loggers/#cleanair.loggers.logsettings.get_logger","text":"Return a logger with the appropriate name","title":"get_logger()"},{"location":"API/cleanair/loggers/#cleanair.loggers.logsettings.initialise_logging","text":"Initialise logging for this process","title":"initialise_logging()"},{"location":"API/cleanair/loggers/#cleanair.loggers.logutils","text":"Useful logging utilities","title":"logutils"},{"location":"API/cleanair/loggers/#cleanair.loggers.logutils.duration","text":"Get a human-readable duration from a start and end time in seconds","title":"duration()"},{"location":"API/cleanair/loggers/#cleanair.loggers.logutils.duration_from_seconds","text":"Get a human-readable duration from a number of seconds","title":"duration_from_seconds()"},{"location":"API/cleanair/metrics/","text":"Metrics \u00b6 \u00b6 Methods for evaluating metrics of a model fit. evaluate \u00b6 Methods for evaluating a model data fit. __remove_rows_with_nans ( pred_df , ** kwargs ) \u00b6 Remove rows with NaN as an observation. concat_static_features ( scores_df , pred_df , static_features = None ) \u00b6 Concatenate the sensor scores dataframe with static features of the sensor. evaluate_model_data ( model_data , metric_methods , ** kwargs ) \u00b6 Given a model data object, evaluate the predictions. Parameters model_data : ModelData A model data object with updated predictions. metric_methods : dict A dictionary where keys are the name of a metric and values are a function that takes two numpy arrays of the same shape. kwargs : dict See Other Parameters. Returns sensor_scores_df : pd.DataFrame For every instance of the experiment, we calculate the score of a sensor over the whole prediction time period. temporal_scores_df : pd.DataFrame For every instance of an experiment, we calculate the scores over all sensors given a slice in time. Other Parameters evaluate_testing : bool, optional If true, this function will evaluate the predictions made on the testing set of data. evaluate_training : bool, optional If true, this function will evaluate the predictions made on the training set of data. pred_cols : list, optional Columns containing predictions. Default is [\"NO2_mean\"]. test_cols : list, optional Columns containing observations during prediction period. Default is [\"NO2\"]. Raises ValueError If value of kwargs are not valid values. evaluate_spatio_temporal_scores ( pred_df , metric_methods , sensor_col = 'point_id' , temporal_col = 'measurement_start_utc' , ** kwargs ) \u00b6 Given a prediction dataframe, measure scores over sensors and time. get_metric_methods ( r2_score = True , mae = True , mse = True , ** kwargs ) \u00b6 Get a dictionary of metric keys and methods. Parameters r2_score : bool, optional Whether to use the r2_score score. mae : bool, optional Whether to use the mae score. mse : bool, optional Whether to use mean squared error. kwargs : dict, optional Keys are names of metrics, values are a function that takes two numpy arrays of the same shape. Returns dict Key is a string of the metric name. Value is a metric evaluation function that takes two equally sized numpy arrays as parameters. measure_scores_by_hour ( pred_df , metric_methods , datetime_col = 'measurement_start_utc' , ** kwargs ) \u00b6 Measure metric scores for each hour of prediction. Parameters pred_df : DataFrame Indexed by datetime. Must have a column of testing data and a column from the predicted air quality at the same points as the testing data. metric_methods : dict Keys are name of metric. Values are functions that take in two numpy/series and compute the score. datetime_col : str, optional Name of the datetime columns in the dataframe. Returns scores : DataFrame Indexed by datetime. Each column is a metric in metric_methods. Other Parameters pred_cols : list, optional Names of the column that are predictions. Length must match test_cols . test_cols : list, optional Names of columns in the dataframe that are the true observations. measure_scores_by_sensor ( pred_df , metric_methods , groupby_col = 'point_id' , ** kwargs ) \u00b6 Group the pred_df by sensor then measure scores on each sensor. Parameters pred_df : DataFrame Indexed by datetime. Must have a column of testing data and a column from the predicted air quality at the same points as the testing data. metric_methods : dict Keys are name of metric. Values are functions that take in two numpy/series and compute the score. groupby_col : str Name of the column containing the sensor ids. Returns pd.DataFrame Dataframe of scores by sensor. Other Parameters pred_cols : list, optional Names of the column that are predictions. Length must match test_cols . test_cols : list, optional Names of columns in the dataframe that are the true observations. pop_kwarg ( kwargs , key , default ) \u00b6 Pop the value of key if its in kwargs, else use the default value.","title":"Metrics"},{"location":"API/cleanair/metrics/#metrics","text":"","title":"Metrics"},{"location":"API/cleanair/metrics/#cleanair.metrics","text":"Methods for evaluating metrics of a model fit.","title":"cleanair.metrics"},{"location":"API/cleanair/metrics/#cleanair.metrics.evaluate","text":"Methods for evaluating a model data fit.","title":"evaluate"},{"location":"API/cleanair/metrics/#cleanair.metrics.evaluate.__remove_rows_with_nans","text":"Remove rows with NaN as an observation.","title":"__remove_rows_with_nans()"},{"location":"API/cleanair/metrics/#cleanair.metrics.evaluate.concat_static_features","text":"Concatenate the sensor scores dataframe with static features of the sensor.","title":"concat_static_features()"},{"location":"API/cleanair/metrics/#cleanair.metrics.evaluate.evaluate_model_data","text":"Given a model data object, evaluate the predictions. Parameters model_data : ModelData A model data object with updated predictions. metric_methods : dict A dictionary where keys are the name of a metric and values are a function that takes two numpy arrays of the same shape. kwargs : dict See Other Parameters. Returns sensor_scores_df : pd.DataFrame For every instance of the experiment, we calculate the score of a sensor over the whole prediction time period. temporal_scores_df : pd.DataFrame For every instance of an experiment, we calculate the scores over all sensors given a slice in time. Other Parameters evaluate_testing : bool, optional If true, this function will evaluate the predictions made on the testing set of data. evaluate_training : bool, optional If true, this function will evaluate the predictions made on the training set of data. pred_cols : list, optional Columns containing predictions. Default is [\"NO2_mean\"]. test_cols : list, optional Columns containing observations during prediction period. Default is [\"NO2\"]. Raises ValueError If value of kwargs are not valid values.","title":"evaluate_model_data()"},{"location":"API/cleanair/metrics/#cleanair.metrics.evaluate.evaluate_spatio_temporal_scores","text":"Given a prediction dataframe, measure scores over sensors and time.","title":"evaluate_spatio_temporal_scores()"},{"location":"API/cleanair/metrics/#cleanair.metrics.evaluate.get_metric_methods","text":"Get a dictionary of metric keys and methods. Parameters r2_score : bool, optional Whether to use the r2_score score. mae : bool, optional Whether to use the mae score. mse : bool, optional Whether to use mean squared error. kwargs : dict, optional Keys are names of metrics, values are a function that takes two numpy arrays of the same shape. Returns dict Key is a string of the metric name. Value is a metric evaluation function that takes two equally sized numpy arrays as parameters.","title":"get_metric_methods()"},{"location":"API/cleanair/metrics/#cleanair.metrics.evaluate.measure_scores_by_hour","text":"Measure metric scores for each hour of prediction. Parameters pred_df : DataFrame Indexed by datetime. Must have a column of testing data and a column from the predicted air quality at the same points as the testing data. metric_methods : dict Keys are name of metric. Values are functions that take in two numpy/series and compute the score. datetime_col : str, optional Name of the datetime columns in the dataframe. Returns scores : DataFrame Indexed by datetime. Each column is a metric in metric_methods. Other Parameters pred_cols : list, optional Names of the column that are predictions. Length must match test_cols . test_cols : list, optional Names of columns in the dataframe that are the true observations.","title":"measure_scores_by_hour()"},{"location":"API/cleanair/metrics/#cleanair.metrics.evaluate.measure_scores_by_sensor","text":"Group the pred_df by sensor then measure scores on each sensor. Parameters pred_df : DataFrame Indexed by datetime. Must have a column of testing data and a column from the predicted air quality at the same points as the testing data. metric_methods : dict Keys are name of metric. Values are functions that take in two numpy/series and compute the score. groupby_col : str Name of the column containing the sensor ids. Returns pd.DataFrame Dataframe of scores by sensor. Other Parameters pred_cols : list, optional Names of the column that are predictions. Length must match test_cols . test_cols : list, optional Names of columns in the dataframe that are the true observations.","title":"measure_scores_by_sensor()"},{"location":"API/cleanair/metrics/#cleanair.metrics.evaluate.pop_kwarg","text":"Pop the value of key if its in kwargs, else use the default value.","title":"pop_kwarg()"},{"location":"API/cleanair/mixins/","text":"Mixins \u00b6 \u00b6 Mixins for adding functionality api_request_mixin \u00b6 Mixin for all datasources that obtain their data by calling a web API APIRequestMixin \u00b6 Manage interactions with an external API get_available_datetimes ( site , start_date , end_date ) staticmethod \u00b6 Get the dates that data is available for a site between start_date and end_date If no data is available between these dates returns None get_readings_by_site ( self , site_list_query , start_date , end_date ) \u00b6 Request a list of readings for a list of sites between start_date and end_date get_response ( api_endpoint , params = None , timeout = 60.0 ) staticmethod \u00b6 Return the response from an API request_site_readings ( self , start_date , end_date , site_code ) \u00b6 Request all readings between {start_date} and {end_date}, removing duplicates. database_query_mixin \u00b6 Mixin for useful database queries DBQueryMixin \u00b6 Common database queries. Child classes must also inherit from DBWriter get_aqe_readings ( self , start_date , end_date ) \u00b6 Get AQE readings from database get_available_dynamic_features ( self , start_date , end_date ) \u00b6 Return a list of the available dynamic features in the database. Only returns features that are available between start_date and end_date get_available_interest_points ( self , sources , point_ids = None ) \u00b6 Return the available interest points for a list of sources, excluding any LAQN or AQE sites that are closed. Only returns points withing the London boundary Satellite returns features outside of london boundary, while laqn and aqe do not. Parameters: Name Type Description Default sources A list of sources to include required point_ids A list of point_ids to include. Default of None returns all points None get_available_sources ( self ) \u00b6 Return the available interest point sources in a database get_available_static_features ( self ) \u00b6 Return available static features from the CleanAir database get_laqn_readings ( self , start_date , end_date ) \u00b6 Get LAQN readings from database get_satellite_readings_pred ( self , start_date , end_date , species ) \u00b6 Get Satellite data for the prediction period Gets up to 72 hours of predicted data from the satellite readings from the same reference_start_utc date as start_date get_satellite_readings_training ( self , start_date , end_date , species ) \u00b6 Get Satellite data for the training period As we get 72 hours of Satellite forecast on each day, here we only get Satellite data where the reference date is the same as the forecast time. i.e. Get data between start_datetime and end_datetime which consists of the first 24 hours of forecasts on each of those days query_london_boundary ( self ) \u00b6 Query LondonBoundary to obtain the bounding geometry for London ScootQueryMixin \u00b6 Queries for the scoot dataset. create_day_of_week_daterange ( day_of_week , start_date , end_date ) staticmethod \u00b6 Create a list of tuples (start date, end date) where each start_date is the same day of the week. Parameters: Name Type Description Default day_of_week int Day of the week. 0=Mon, 1=Tue, etc. required start_date str ISO formatted date. All dates in returned list will be at least this date. required end_date str ISO formatted date. All dates in the returned list will be at most this date. required Returns List of date tuples. The first item in the tuple will be exactly one day before the last item in the tuple. get_scoot_by_dow ( self , day_of_week , start_date , end_date = None , detectors = None ) \u00b6 Get scoot readings for days between start_date and end_date filtered by day_of_week. Parameters: Name Type Description Default day_of_week int Day of the week. 0=Mon, 1=Tue, etc. required start_date str Start datetime. required end_date str End datetime (exclusive). None detectors List Subset of detectors to get readings for. None get_scoot_detectors ( self , offset = None , limit = None ) \u00b6 Get all scoot detectors from the interest point schema. Parameters: Name Type Description Default offset int Start selecting detectors from this integer index. None limit int Select at most this many detectors. None get_scoot_with_location ( self , start_time , end_time = None , detectors = None ) \u00b6 Get scoot data with lat and long positions. Parameters: Name Type Description Default start_time str Start datetime. required end_time str End datetime (exclusive). None detectors List Subset of detectors to get readings for. None date_range_mixin \u00b6 Mixin for classes that need to keep track of date ranges DateRangeMixin \u00b6 Manage data ranges db_connection_mixin \u00b6 Mixin for loading database loggin info and creating connection strings DBConnectionMixin \u00b6 Create database connection strings connection_keys property readonly \u00b6 Return valid connection keys connection_string property readonly \u00b6 Get a connection string __init__ ( self , secretfile , secret_dict = None , allow_env_pass = True ) special \u00b6 Generates connection strings for postgresql database. First loads connection information from secretfile. Secondly overwrites with values from secret_dict if any are provided. Finally check if PGPASSWORD is an environment variable and overwrites with this. Parameters: Name Type Description Default secretfile str Path to a secret file (json). Can be the full path to secrets file or a filename if the secret is in a directory called '/secrets' required secret_dict dict A dictionary of login secrets. Will override variables in the json secrets file if both provided None allow_env_pass bool Allow password to be overwritten by environment variable PGPASSWORD True load_connection_info ( self , secret_file ) \u00b6 Loads database secrets from a json file. Check file system is accessable from docker and return database login info read_environment_password () staticmethod \u00b6 Check if PGPASSWORD exists as an environment variable and return its values if it does Returns: Type Description dict A dictionary of containing the value of the environment value PGPASSWORD if it exists. Else None replace_connection_values ( self , connection_info , secret_dict ) \u00b6 Replace values in connection_info with those in secret_dict Parameters: Name Type Description Default connection_info dict A dictionary of connection parameters required secret_dict dict A dictionary of connection parameters to replace matching parameters in connection_info required Returns: Type Description dict A dictionary of connection parameters instance_query_mixin \u00b6 Mixin class for querying instances. InstanceQueryMixin \u00b6 Class for querying instances. Notes Any object that inherits this mixin must assign the above attributes. get_instances ( self , tag = None , instance_ids = None , data_ids = None , param_ids = None , models = None , fit_start_time = None ) \u00b6 Get traffic instances and optionally filter by parameters. Parameters: Name Type Description Default tag str String to group model fits, e.g. 'test', 'validation'. None instance_ids list Filter by instance ids in the list. None data_ids list Filter by data ids in the list. None param_ids list Filter by model parameter ids in the list. None models list Filter by names of models in the list. None fit_start_time str Filter by models that were fit on or after this timestamp. None get_instances_with_params ( self , tag = None , instance_ids = None , data_ids = None , param_ids = None , models = None , fit_start_time = None ) \u00b6 Get all traffic instances and join the json parameters. Parameters: Name Type Description Default tag str String to group model fits, e.g. 'test', 'validation'. None instance_ids list Filter by instance ids in the list. None data_ids list Filter by data ids in the list. None param_ids list Filter by model parameter ids in the list. None models list Filter by names of models in the list. None fit_start_time str Filter by models that were fit on or after this timestamp. None parser_mixins \u00b6 Mixins which are used by multiple argument parsers DurationParserMixin \u00b6 Parser for any entrypoint which needs a duration parse_args ( self , args = None , namespace = None ) \u00b6 Raise an exception if the provided arguments are invalid ParseSecretDict \u00b6 Parse items into a dictionary SecretFileParserMixin \u00b6 Parser for any entrypoint which needs a secrets file SourcesMixin \u00b6 Parser for any entrypoint which allows verbosity to be set VerbosityMixin \u00b6 Parser for any entrypoint which allows verbosity to be set","title":"Mixins"},{"location":"API/cleanair/mixins/#mixins","text":"","title":"Mixins"},{"location":"API/cleanair/mixins/#cleanair.mixins","text":"Mixins for adding functionality","title":"cleanair.mixins"},{"location":"API/cleanair/mixins/#cleanair.mixins.api_request_mixin","text":"Mixin for all datasources that obtain their data by calling a web API","title":"api_request_mixin"},{"location":"API/cleanair/mixins/#cleanair.mixins.api_request_mixin.APIRequestMixin","text":"Manage interactions with an external API","title":"APIRequestMixin"},{"location":"API/cleanair/mixins/#cleanair.mixins.api_request_mixin.APIRequestMixin.get_available_datetimes","text":"Get the dates that data is available for a site between start_date and end_date If no data is available between these dates returns None","title":"get_available_datetimes()"},{"location":"API/cleanair/mixins/#cleanair.mixins.api_request_mixin.APIRequestMixin.get_readings_by_site","text":"Request a list of readings for a list of sites between start_date and end_date","title":"get_readings_by_site()"},{"location":"API/cleanair/mixins/#cleanair.mixins.api_request_mixin.APIRequestMixin.get_response","text":"Return the response from an API","title":"get_response()"},{"location":"API/cleanair/mixins/#cleanair.mixins.api_request_mixin.APIRequestMixin.request_site_readings","text":"Request all readings between {start_date} and {end_date}, removing duplicates.","title":"request_site_readings()"},{"location":"API/cleanair/mixins/#cleanair.mixins.database_query_mixin","text":"Mixin for useful database queries","title":"database_query_mixin"},{"location":"API/cleanair/mixins/#cleanair.mixins.database_query_mixin.DBQueryMixin","text":"Common database queries. Child classes must also inherit from DBWriter","title":"DBQueryMixin"},{"location":"API/cleanair/mixins/#cleanair.mixins.database_query_mixin.DBQueryMixin.get_aqe_readings","text":"Get AQE readings from database","title":"get_aqe_readings()"},{"location":"API/cleanair/mixins/#cleanair.mixins.database_query_mixin.DBQueryMixin.get_available_dynamic_features","text":"Return a list of the available dynamic features in the database. Only returns features that are available between start_date and end_date","title":"get_available_dynamic_features()"},{"location":"API/cleanair/mixins/#cleanair.mixins.database_query_mixin.DBQueryMixin.get_available_interest_points","text":"Return the available interest points for a list of sources, excluding any LAQN or AQE sites that are closed. Only returns points withing the London boundary Satellite returns features outside of london boundary, while laqn and aqe do not. Parameters: Name Type Description Default sources A list of sources to include required point_ids A list of point_ids to include. Default of None returns all points None","title":"get_available_interest_points()"},{"location":"API/cleanair/mixins/#cleanair.mixins.database_query_mixin.DBQueryMixin.get_available_sources","text":"Return the available interest point sources in a database","title":"get_available_sources()"},{"location":"API/cleanair/mixins/#cleanair.mixins.database_query_mixin.DBQueryMixin.get_available_static_features","text":"Return available static features from the CleanAir database","title":"get_available_static_features()"},{"location":"API/cleanair/mixins/#cleanair.mixins.database_query_mixin.DBQueryMixin.get_laqn_readings","text":"Get LAQN readings from database","title":"get_laqn_readings()"},{"location":"API/cleanair/mixins/#cleanair.mixins.database_query_mixin.DBQueryMixin.get_satellite_readings_pred","text":"Get Satellite data for the prediction period Gets up to 72 hours of predicted data from the satellite readings from the same reference_start_utc date as start_date","title":"get_satellite_readings_pred()"},{"location":"API/cleanair/mixins/#cleanair.mixins.database_query_mixin.DBQueryMixin.get_satellite_readings_training","text":"Get Satellite data for the training period As we get 72 hours of Satellite forecast on each day, here we only get Satellite data where the reference date is the same as the forecast time. i.e. Get data between start_datetime and end_datetime which consists of the first 24 hours of forecasts on each of those days","title":"get_satellite_readings_training()"},{"location":"API/cleanair/mixins/#cleanair.mixins.database_query_mixin.DBQueryMixin.query_london_boundary","text":"Query LondonBoundary to obtain the bounding geometry for London","title":"query_london_boundary()"},{"location":"API/cleanair/mixins/#cleanair.mixins.database_query_mixin.ScootQueryMixin","text":"Queries for the scoot dataset.","title":"ScootQueryMixin"},{"location":"API/cleanair/mixins/#cleanair.mixins.database_query_mixin.ScootQueryMixin.create_day_of_week_daterange","text":"Create a list of tuples (start date, end date) where each start_date is the same day of the week. Parameters: Name Type Description Default day_of_week int Day of the week. 0=Mon, 1=Tue, etc. required start_date str ISO formatted date. All dates in returned list will be at least this date. required end_date str ISO formatted date. All dates in the returned list will be at most this date. required Returns List of date tuples. The first item in the tuple will be exactly one day before the last item in the tuple.","title":"create_day_of_week_daterange()"},{"location":"API/cleanair/mixins/#cleanair.mixins.database_query_mixin.ScootQueryMixin.get_scoot_by_dow","text":"Get scoot readings for days between start_date and end_date filtered by day_of_week. Parameters: Name Type Description Default day_of_week int Day of the week. 0=Mon, 1=Tue, etc. required start_date str Start datetime. required end_date str End datetime (exclusive). None detectors List Subset of detectors to get readings for. None","title":"get_scoot_by_dow()"},{"location":"API/cleanair/mixins/#cleanair.mixins.database_query_mixin.ScootQueryMixin.get_scoot_detectors","text":"Get all scoot detectors from the interest point schema. Parameters: Name Type Description Default offset int Start selecting detectors from this integer index. None limit int Select at most this many detectors. None","title":"get_scoot_detectors()"},{"location":"API/cleanair/mixins/#cleanair.mixins.database_query_mixin.ScootQueryMixin.get_scoot_with_location","text":"Get scoot data with lat and long positions. Parameters: Name Type Description Default start_time str Start datetime. required end_time str End datetime (exclusive). None detectors List Subset of detectors to get readings for. None","title":"get_scoot_with_location()"},{"location":"API/cleanair/mixins/#cleanair.mixins.date_range_mixin","text":"Mixin for classes that need to keep track of date ranges","title":"date_range_mixin"},{"location":"API/cleanair/mixins/#cleanair.mixins.date_range_mixin.DateRangeMixin","text":"Manage data ranges","title":"DateRangeMixin"},{"location":"API/cleanair/mixins/#cleanair.mixins.db_connection_mixin","text":"Mixin for loading database loggin info and creating connection strings","title":"db_connection_mixin"},{"location":"API/cleanair/mixins/#cleanair.mixins.db_connection_mixin.DBConnectionMixin","text":"Create database connection strings","title":"DBConnectionMixin"},{"location":"API/cleanair/mixins/#cleanair.mixins.db_connection_mixin.DBConnectionMixin.connection_keys","text":"Return valid connection keys","title":"connection_keys"},{"location":"API/cleanair/mixins/#cleanair.mixins.db_connection_mixin.DBConnectionMixin.connection_string","text":"Get a connection string","title":"connection_string"},{"location":"API/cleanair/mixins/#cleanair.mixins.db_connection_mixin.DBConnectionMixin.__init__","text":"Generates connection strings for postgresql database. First loads connection information from secretfile. Secondly overwrites with values from secret_dict if any are provided. Finally check if PGPASSWORD is an environment variable and overwrites with this. Parameters: Name Type Description Default secretfile str Path to a secret file (json). Can be the full path to secrets file or a filename if the secret is in a directory called '/secrets' required secret_dict dict A dictionary of login secrets. Will override variables in the json secrets file if both provided None allow_env_pass bool Allow password to be overwritten by environment variable PGPASSWORD True","title":"__init__()"},{"location":"API/cleanair/mixins/#cleanair.mixins.db_connection_mixin.DBConnectionMixin.load_connection_info","text":"Loads database secrets from a json file. Check file system is accessable from docker and return database login info","title":"load_connection_info()"},{"location":"API/cleanair/mixins/#cleanair.mixins.db_connection_mixin.DBConnectionMixin.read_environment_password","text":"Check if PGPASSWORD exists as an environment variable and return its values if it does Returns: Type Description dict A dictionary of containing the value of the environment value PGPASSWORD if it exists. Else None","title":"read_environment_password()"},{"location":"API/cleanair/mixins/#cleanair.mixins.db_connection_mixin.DBConnectionMixin.replace_connection_values","text":"Replace values in connection_info with those in secret_dict Parameters: Name Type Description Default connection_info dict A dictionary of connection parameters required secret_dict dict A dictionary of connection parameters to replace matching parameters in connection_info required Returns: Type Description dict A dictionary of connection parameters","title":"replace_connection_values()"},{"location":"API/cleanair/mixins/#cleanair.mixins.instance_query_mixin","text":"Mixin class for querying instances.","title":"instance_query_mixin"},{"location":"API/cleanair/mixins/#cleanair.mixins.instance_query_mixin.InstanceQueryMixin","text":"Class for querying instances. Notes Any object that inherits this mixin must assign the above attributes.","title":"InstanceQueryMixin"},{"location":"API/cleanair/mixins/#cleanair.mixins.instance_query_mixin.InstanceQueryMixin.get_instances","text":"Get traffic instances and optionally filter by parameters. Parameters: Name Type Description Default tag str String to group model fits, e.g. 'test', 'validation'. None instance_ids list Filter by instance ids in the list. None data_ids list Filter by data ids in the list. None param_ids list Filter by model parameter ids in the list. None models list Filter by names of models in the list. None fit_start_time str Filter by models that were fit on or after this timestamp. None","title":"get_instances()"},{"location":"API/cleanair/mixins/#cleanair.mixins.instance_query_mixin.InstanceQueryMixin.get_instances_with_params","text":"Get all traffic instances and join the json parameters. Parameters: Name Type Description Default tag str String to group model fits, e.g. 'test', 'validation'. None instance_ids list Filter by instance ids in the list. None data_ids list Filter by data ids in the list. None param_ids list Filter by model parameter ids in the list. None models list Filter by names of models in the list. None fit_start_time str Filter by models that were fit on or after this timestamp. None","title":"get_instances_with_params()"},{"location":"API/cleanair/mixins/#cleanair.mixins.parser_mixins","text":"Mixins which are used by multiple argument parsers","title":"parser_mixins"},{"location":"API/cleanair/mixins/#cleanair.mixins.parser_mixins.DurationParserMixin","text":"Parser for any entrypoint which needs a duration","title":"DurationParserMixin"},{"location":"API/cleanair/mixins/#cleanair.mixins.parser_mixins.DurationParserMixin.parse_args","text":"Raise an exception if the provided arguments are invalid","title":"parse_args()"},{"location":"API/cleanair/mixins/#cleanair.mixins.parser_mixins.ParseSecretDict","text":"Parse items into a dictionary","title":"ParseSecretDict"},{"location":"API/cleanair/mixins/#cleanair.mixins.parser_mixins.SecretFileParserMixin","text":"Parser for any entrypoint which needs a secrets file","title":"SecretFileParserMixin"},{"location":"API/cleanair/mixins/#cleanair.mixins.parser_mixins.SourcesMixin","text":"Parser for any entrypoint which allows verbosity to be set","title":"SourcesMixin"},{"location":"API/cleanair/mixins/#cleanair.mixins.parser_mixins.VerbosityMixin","text":"Parser for any entrypoint which allows verbosity to be set","title":"VerbosityMixin"},{"location":"API/cleanair/models/","text":"Models \u00b6 \u00b6 Model fitting classes model \u00b6 The interface for London air quality models. Model \u00b6 A base class for models. All other air quality models should extend this class. __init__ ( self , model_params = None , tasks = None , ** kwargs ) special \u00b6 Initialise a model with parameters and settings. Parameters model_params : dict, optional Parameters to run the model. You may wish to pass parameters for the optimizer, kernel, etc. tasks : list, optional The name of the tasks (pollutants) we are modelling. Default is ['NO2']. Other Parameters log : bool, optional Print logs. Default is True. restore : bool, optional Restore the model from a file. check_model_params_are_valid ( self ) \u00b6 Check the model parameters are valid for the model. Parameters model_params : dict A dictionary of model parameters. Raises KeyError If the model parameters are not sufficient. check_test_set_is_valid ( x_test ) staticmethod \u00b6 Check the format of x_test dictionary is correct. check_training_set_is_valid ( x_train , y_train ) staticmethod \u00b6 Check the format of x_train and y_train dictionaries are correct. Parameters x_train : dict Dictionary containing X training data. y_train : dict Dictionary containing Y training data. Raises KeyError If there are no keys in x_train or y_train. ValueError If the shape of x_train or y_train are incorrect. fit ( self , x_train , y_train , ** kwargs ) \u00b6 Fit the model to some training data. Parameters x_train : dict Keys are sources. Values are numpy arrays. y_train : dict Keys are sources. Values are a dict of pollutants and numpys. Examples x_train = { 'laqn' : x_laqn, 'satellite' : x_satellite } y_train = { 'laqn' : { 'NO2' : y_laqn_no2, 'PM10' : y_laqn_pm10 }, 'satellite' : { 'NO2' : y_satellite_no2, 'PM10' : y_satellite_pm10 } } model.fit(x_train, y_train) Notes Every value (e.g. x_laqn , y_satellite_no2 , etc) is a numpy array. The shapes are given in the table below: +-------------------+---------+ | x_laqn | (NxD) | +-------------------+---------+ | x_satellite | (MxSxD) | +-------------------+---------+ | y_laqn_* | (Nx1) | +-------------------+---------+ | y_satellite_* | (Mx1) | +-------------------+---------+ where N is the number of laqn observations, D is the number of features, S is the discretization amount, M is the number of satellite observations, and * represents a pollutant name. get_default_model_params ( self ) \u00b6 The default model parameters if none are supplied. Returns dict Dictionary of parameters. predict ( self , x_test ) \u00b6 Predict using the model. Parameters x_test : dict Keys are sources. Values are numpy arrays. Returns y_pred : dict Keys are sources. Values are dicts of pollutants for keys and dict for values. Examples x_test = { 'laqn' : np.array, 'satellite' : np.array } y_pred = model.predict(x_test) json.dumps(y_pred) { 'laqn' : { 'NO2' : { 'mean' : np.array, 'var' : np.array }, 'PM10' : { 'mean' : np.array, 'var' : np.array } } } model_data \u00b6 Vizualise available sensor data for a model fit ModelData \u00b6 Read data from multiple database tables in order to get data for model fitting norm_stats property readonly \u00b6 Get the mean and sd used for data normalisation x_names_norm property readonly \u00b6 Get the normalised x names __init__ ( self , config = None , config_dir = None , ** kwargs ) special \u00b6 Initialise the ModelData object with a config file Parameters: Name Type Description Default config A config dictionary None config_dir A directory containing config files (created by ModelData.save_config_state()) None get_df_from_pred_dict ( self , data_df , data_dict , pred_dict , fit_start_time , ** kwargs ) \u00b6 Return a new dataframe with columns updated from pred_dict. get_pred_data_arrays ( self , sources = 'all' , species = 'all' , return_y = False , dropna = False ) \u00b6 The pred data arrays. Parameters: Name Type Description Default return_y Return the sensor data if in the database for the prediction dates False dropna Drop any rows which contain NaN False Notes Satellite is never included as a key, value for prediction arrays because it is considered a training source only. get_pred_data_inputs ( self ) \u00b6 Query the database for inputs for model prediction get_training_data_arrays ( self , sources = 'all' , species = 'all' , return_y = True , dropna = False ) \u00b6 The training data arrays. Notes If the include_satellite flag is set to True in config , then satellite is always returned as a source. get_training_data_inputs ( self ) \u00b6 Query the database to get inputs for model fitting. get_training_satellite_inputs ( self ) \u00b6 Get satellite inputs restore_config_state ( self , dir_path ) \u00b6 Reload configuration state saved to disk by ModelData.save_config_state() save_config_state ( self , dir_path ) \u00b6 Save the full configuration and training/prediction data to disk: Parameters: Name Type Description Default dir_path Directory path in which to save the config files required update_model_results_df ( self , predict_data_dict , y_pred , model_fit_info ) \u00b6 Update the model results data frame with model predictions. update_remote_tables ( self ) \u00b6 Update the model results table with the model results update_test_df_with_preds ( self , test_pred_dict , fit_start_time ) \u00b6 Update the normalised_pred_data_df with predictions for all pred sources. Parameters test_pred_dict : dict Dictionary with first level keys for pred_sources (e.g. 'laqn', 'aqe'). Second level keys are species (e.g. 'NO2', 'PM10'). Third level keys are either 'mean' or 'var'. Values are numpy arrays of predictions for a source and specie. fit_start_time : datetime Start time of the model fit. update_training_df_with_preds ( self , training_pred_dict , fit_start_time ) \u00b6 Updated the normalised_training_data_df with predictions on the training set. svgp \u00b6 Sparse Variational Gaussian Process (LAQN ONLY) SVGP \u00b6 Sparse Variational Gaussian Process for air quality. __init__ ( self , model_params = None , tasks = None , ** kwargs ) special \u00b6 SVGP. Parameters model_params : dict, optional See get_default_model_params for more info. tasks : list, optional See super class. **kwargs : kwargs See parent class and other parameters (below). Other Parameters batch_size : int, optional Default is 100. disable_tf_warnings : bool, optional Don't print out warnings from tensorflow if True. refresh : bool, optional How often to print out the ELBO. batch_predict ( self , x_array ) \u00b6 Split up prediction into indepedent batchs. Parameters x_array : np.array N x D numpy array of locations to predict at. Returns y_mean : np.array N x D numpy array of means. y_var : np.array N x D numpy array of variances. clean_data ( self , x_array , y_array ) \u00b6 Remove nans and missing data for use in GPflow Parameters: Name Type Description Default x_array N x D numpy array, required y_array N x 1 numpy array required elbo_logger ( self , logger_arg ) \u00b6 Log optimisation progress. Parameters logger_arg : unknown Argument passed as a callback from GPFlow optimiser. fit ( self , x_train , y_train , ** kwargs ) \u00b6 Fit the SVGP. Parameters x_train : dict See Model.fit method in the base class for further details. NxM numpy array of N observations of M covariates. Only the 'laqn' key is used in this fit method, so all observations come from this source. y_train : dict Only y_train['laqn']['NO2'] is used for fitting. The size of this array is NX1 with N sensor observations from 'laqn'. See Model.fit method in the base class for further details. Other Parameters save_model_state : bool, optional Save the model to file so that it can be restored at a later date. Default is False. get_default_model_params ( self ) \u00b6 The default model parameters if none are supplied. Returns dict Dictionary of parameters. predict ( self , x_test ) \u00b6 Predict using the model at the laqn sites for NO2. Parameters x_test : dict See Model.predict for further details. Returns dict See Model.predict for further details. The shape for each pollutant will be (n, 1). setup_model ( self , x_array , y_array , inducing_locations , num_input_dimensions ) \u00b6 Create GPFlow sparse variational Gaussian Processes Parameters x_array : np.array N x D numpy array - observations input. y_array : np.array N x 1 numpy array - observations output. inducing_locations : np.array M x D numpy array - inducing locations. num_input_dimensions : int Number of input dimensions.","title":"Models"},{"location":"API/cleanair/models/#models","text":"","title":"Models"},{"location":"API/cleanair/models/#cleanair.models","text":"Model fitting classes","title":"cleanair.models"},{"location":"API/cleanair/models/#cleanair.models.model","text":"The interface for London air quality models.","title":"model"},{"location":"API/cleanair/models/#cleanair.models.model.Model","text":"A base class for models. All other air quality models should extend this class.","title":"Model"},{"location":"API/cleanair/models/#cleanair.models.model.Model.__init__","text":"Initialise a model with parameters and settings. Parameters model_params : dict, optional Parameters to run the model. You may wish to pass parameters for the optimizer, kernel, etc. tasks : list, optional The name of the tasks (pollutants) we are modelling. Default is ['NO2']. Other Parameters log : bool, optional Print logs. Default is True. restore : bool, optional Restore the model from a file.","title":"__init__()"},{"location":"API/cleanair/models/#cleanair.models.model.Model.check_model_params_are_valid","text":"Check the model parameters are valid for the model. Parameters model_params : dict A dictionary of model parameters. Raises KeyError If the model parameters are not sufficient.","title":"check_model_params_are_valid()"},{"location":"API/cleanair/models/#cleanair.models.model.Model.check_test_set_is_valid","text":"Check the format of x_test dictionary is correct.","title":"check_test_set_is_valid()"},{"location":"API/cleanair/models/#cleanair.models.model.Model.check_training_set_is_valid","text":"Check the format of x_train and y_train dictionaries are correct. Parameters x_train : dict Dictionary containing X training data. y_train : dict Dictionary containing Y training data. Raises KeyError If there are no keys in x_train or y_train. ValueError If the shape of x_train or y_train are incorrect.","title":"check_training_set_is_valid()"},{"location":"API/cleanair/models/#cleanair.models.model.Model.fit","text":"Fit the model to some training data. Parameters x_train : dict Keys are sources. Values are numpy arrays. y_train : dict Keys are sources. Values are a dict of pollutants and numpys. Examples x_train = { 'laqn' : x_laqn, 'satellite' : x_satellite } y_train = { 'laqn' : { 'NO2' : y_laqn_no2, 'PM10' : y_laqn_pm10 }, 'satellite' : { 'NO2' : y_satellite_no2, 'PM10' : y_satellite_pm10 } } model.fit(x_train, y_train) Notes Every value (e.g. x_laqn , y_satellite_no2 , etc) is a numpy array. The shapes are given in the table below: +-------------------+---------+ | x_laqn | (NxD) | +-------------------+---------+ | x_satellite | (MxSxD) | +-------------------+---------+ | y_laqn_* | (Nx1) | +-------------------+---------+ | y_satellite_* | (Mx1) | +-------------------+---------+ where N is the number of laqn observations, D is the number of features, S is the discretization amount, M is the number of satellite observations, and * represents a pollutant name.","title":"fit()"},{"location":"API/cleanair/models/#cleanair.models.model.Model.get_default_model_params","text":"The default model parameters if none are supplied. Returns dict Dictionary of parameters.","title":"get_default_model_params()"},{"location":"API/cleanair/models/#cleanair.models.model.Model.predict","text":"Predict using the model. Parameters x_test : dict Keys are sources. Values are numpy arrays. Returns y_pred : dict Keys are sources. Values are dicts of pollutants for keys and dict for values. Examples x_test = { 'laqn' : np.array, 'satellite' : np.array } y_pred = model.predict(x_test) json.dumps(y_pred) { 'laqn' : { 'NO2' : { 'mean' : np.array, 'var' : np.array }, 'PM10' : { 'mean' : np.array, 'var' : np.array } } }","title":"predict()"},{"location":"API/cleanair/models/#cleanair.models.model_data","text":"Vizualise available sensor data for a model fit","title":"model_data"},{"location":"API/cleanair/models/#cleanair.models.model_data.ModelData","text":"Read data from multiple database tables in order to get data for model fitting","title":"ModelData"},{"location":"API/cleanair/models/#cleanair.models.model_data.ModelData.norm_stats","text":"Get the mean and sd used for data normalisation","title":"norm_stats"},{"location":"API/cleanair/models/#cleanair.models.model_data.ModelData.x_names_norm","text":"Get the normalised x names","title":"x_names_norm"},{"location":"API/cleanair/models/#cleanair.models.model_data.ModelData.__init__","text":"Initialise the ModelData object with a config file Parameters: Name Type Description Default config A config dictionary None config_dir A directory containing config files (created by ModelData.save_config_state()) None","title":"__init__()"},{"location":"API/cleanair/models/#cleanair.models.model_data.ModelData.get_df_from_pred_dict","text":"Return a new dataframe with columns updated from pred_dict.","title":"get_df_from_pred_dict()"},{"location":"API/cleanair/models/#cleanair.models.model_data.ModelData.get_pred_data_arrays","text":"The pred data arrays. Parameters: Name Type Description Default return_y Return the sensor data if in the database for the prediction dates False dropna Drop any rows which contain NaN False Notes Satellite is never included as a key, value for prediction arrays because it is considered a training source only.","title":"get_pred_data_arrays()"},{"location":"API/cleanair/models/#cleanair.models.model_data.ModelData.get_pred_data_inputs","text":"Query the database for inputs for model prediction","title":"get_pred_data_inputs()"},{"location":"API/cleanair/models/#cleanair.models.model_data.ModelData.get_training_data_arrays","text":"The training data arrays. Notes If the include_satellite flag is set to True in config , then satellite is always returned as a source.","title":"get_training_data_arrays()"},{"location":"API/cleanair/models/#cleanair.models.model_data.ModelData.get_training_data_inputs","text":"Query the database to get inputs for model fitting.","title":"get_training_data_inputs()"},{"location":"API/cleanair/models/#cleanair.models.model_data.ModelData.get_training_satellite_inputs","text":"Get satellite inputs","title":"get_training_satellite_inputs()"},{"location":"API/cleanair/models/#cleanair.models.model_data.ModelData.restore_config_state","text":"Reload configuration state saved to disk by ModelData.save_config_state()","title":"restore_config_state()"},{"location":"API/cleanair/models/#cleanair.models.model_data.ModelData.save_config_state","text":"Save the full configuration and training/prediction data to disk: Parameters: Name Type Description Default dir_path Directory path in which to save the config files required","title":"save_config_state()"},{"location":"API/cleanair/models/#cleanair.models.model_data.ModelData.update_model_results_df","text":"Update the model results data frame with model predictions.","title":"update_model_results_df()"},{"location":"API/cleanair/models/#cleanair.models.model_data.ModelData.update_remote_tables","text":"Update the model results table with the model results","title":"update_remote_tables()"},{"location":"API/cleanair/models/#cleanair.models.model_data.ModelData.update_test_df_with_preds","text":"Update the normalised_pred_data_df with predictions for all pred sources. Parameters test_pred_dict : dict Dictionary with first level keys for pred_sources (e.g. 'laqn', 'aqe'). Second level keys are species (e.g. 'NO2', 'PM10'). Third level keys are either 'mean' or 'var'. Values are numpy arrays of predictions for a source and specie. fit_start_time : datetime Start time of the model fit.","title":"update_test_df_with_preds()"},{"location":"API/cleanair/models/#cleanair.models.model_data.ModelData.update_training_df_with_preds","text":"Updated the normalised_training_data_df with predictions on the training set.","title":"update_training_df_with_preds()"},{"location":"API/cleanair/models/#cleanair.models.svgp","text":"Sparse Variational Gaussian Process (LAQN ONLY)","title":"svgp"},{"location":"API/cleanair/models/#cleanair.models.svgp.SVGP","text":"Sparse Variational Gaussian Process for air quality.","title":"SVGP"},{"location":"API/cleanair/models/#cleanair.models.svgp.SVGP.__init__","text":"SVGP. Parameters model_params : dict, optional See get_default_model_params for more info. tasks : list, optional See super class. **kwargs : kwargs See parent class and other parameters (below). Other Parameters batch_size : int, optional Default is 100. disable_tf_warnings : bool, optional Don't print out warnings from tensorflow if True. refresh : bool, optional How often to print out the ELBO.","title":"__init__()"},{"location":"API/cleanair/models/#cleanair.models.svgp.SVGP.batch_predict","text":"Split up prediction into indepedent batchs. Parameters x_array : np.array N x D numpy array of locations to predict at. Returns y_mean : np.array N x D numpy array of means. y_var : np.array N x D numpy array of variances.","title":"batch_predict()"},{"location":"API/cleanair/models/#cleanair.models.svgp.SVGP.clean_data","text":"Remove nans and missing data for use in GPflow Parameters: Name Type Description Default x_array N x D numpy array, required y_array N x 1 numpy array required","title":"clean_data()"},{"location":"API/cleanair/models/#cleanair.models.svgp.SVGP.elbo_logger","text":"Log optimisation progress. Parameters logger_arg : unknown Argument passed as a callback from GPFlow optimiser.","title":"elbo_logger()"},{"location":"API/cleanair/models/#cleanair.models.svgp.SVGP.fit","text":"Fit the SVGP. Parameters x_train : dict See Model.fit method in the base class for further details. NxM numpy array of N observations of M covariates. Only the 'laqn' key is used in this fit method, so all observations come from this source. y_train : dict Only y_train['laqn']['NO2'] is used for fitting. The size of this array is NX1 with N sensor observations from 'laqn'. See Model.fit method in the base class for further details. Other Parameters save_model_state : bool, optional Save the model to file so that it can be restored at a later date. Default is False.","title":"fit()"},{"location":"API/cleanair/models/#cleanair.models.svgp.SVGP.get_default_model_params","text":"The default model parameters if none are supplied. Returns dict Dictionary of parameters.","title":"get_default_model_params()"},{"location":"API/cleanair/models/#cleanair.models.svgp.SVGP.predict","text":"Predict using the model at the laqn sites for NO2. Parameters x_test : dict See Model.predict for further details. Returns dict See Model.predict for further details. The shape for each pollutant will be (n, 1).","title":"predict()"},{"location":"API/cleanair/models/#cleanair.models.svgp.SVGP.setup_model","text":"Create GPFlow sparse variational Gaussian Processes Parameters x_array : np.array N x D numpy array - observations input. y_array : np.array N x 1 numpy array - observations output. inducing_locations : np.array M x D numpy array - inducing locations. num_input_dimensions : int Number of input dimensions.","title":"setup_model()"},{"location":"API/cleanair/parsers/","text":"Parsers \u00b6 \u00b6 Module for cleanair parsers. complex \u00b6 Complex per-entrypoint argument parsers DataBaseRoleParser \u00b6 Argument parser for configuring database roles DatabaseSetupParser \u00b6 Argument parsing for inserting static datafiles ModelFitParser \u00b6 A parser for the model fitting entrypoint. __init__ ( self , ** kwargs ) special \u00b6 Should be able to: - read training/test data from DB (default) - read training/test data from directory - write training/test data to directory - write result to DB (default) - turn off writing to DB (overwrite default) - write results to file parse_kwargs ( self ) \u00b6 If the -c flag is passed, then load the config.json file and overwrite any fields that are passed in kwargs. ModelValidationParser \u00b6 A parser for model validation. SatelliteArgumentParser \u00b6 Argument parsing for Satellite readings parse_args ( self , args = None , namespace = None ) \u00b6 Check whether we have the Copernicus key and try to retrieve it from a local secrets file if not ScootForecastFeatureArgumentParser \u00b6 Argument parsing for converting SCOOT forecasts into model features ScootReadingArgumentParser \u00b6 Argument parsing for SCOOT readings parse_args ( self , args = None , namespace = None ) \u00b6 Check whether we have AWS connection information and try to retrieve it from a local secrets file if not model \u00b6 A base class for cleanair parsers. BaseModelParser \u00b6 Parser for CleanAir model entrypoints. generate_data_config ( self ) \u00b6 Return a dictionary of model data configs parse_kwargs ( self ) \u00b6 If the -c flag is passed, then load the config.json file and overwrite any fields that are passed in kwargs. save_config ( self ) \u00b6 Save the key and values of the parser to a json file. simple \u00b6 Simple per-entrypoint argument parsers AQEReadingArgumentParser \u00b6 Argument parsing for AQE readings LAQNReadingArgumentParser \u00b6 Argument parsing for LAQN readings OsHighwayFeatureArgumentParser \u00b6 Argument parsing for OS highway features ScootReadingFeatureArgumentParser \u00b6 Argument parsing for converting SCOOT readings into model features ScootRoadmapArgumentParser \u00b6 Argument parsing for SCOOT road to detector mapping StaticDatasetArgumentParser \u00b6 Argument parsing for static dataset uploads StreetCanyonFeatureArgumentParser \u00b6 Argument parsing for OS highway features UKMapFeatureArgumentParser \u00b6 Argument parsing for OS highway features","title":"Parsers"},{"location":"API/cleanair/parsers/#parsers","text":"","title":"Parsers"},{"location":"API/cleanair/parsers/#cleanair.parsers","text":"Module for cleanair parsers.","title":"cleanair.parsers"},{"location":"API/cleanair/parsers/#cleanair.parsers.complex","text":"Complex per-entrypoint argument parsers","title":"complex"},{"location":"API/cleanair/parsers/#cleanair.parsers.complex.DataBaseRoleParser","text":"Argument parser for configuring database roles","title":"DataBaseRoleParser"},{"location":"API/cleanair/parsers/#cleanair.parsers.complex.DatabaseSetupParser","text":"Argument parsing for inserting static datafiles","title":"DatabaseSetupParser"},{"location":"API/cleanair/parsers/#cleanair.parsers.complex.ModelFitParser","text":"A parser for the model fitting entrypoint.","title":"ModelFitParser"},{"location":"API/cleanair/parsers/#cleanair.parsers.complex.ModelFitParser.__init__","text":"Should be able to: - read training/test data from DB (default) - read training/test data from directory - write training/test data to directory - write result to DB (default) - turn off writing to DB (overwrite default) - write results to file","title":"__init__()"},{"location":"API/cleanair/parsers/#cleanair.parsers.complex.ModelFitParser.parse_kwargs","text":"If the -c flag is passed, then load the config.json file and overwrite any fields that are passed in kwargs.","title":"parse_kwargs()"},{"location":"API/cleanair/parsers/#cleanair.parsers.complex.ModelValidationParser","text":"A parser for model validation.","title":"ModelValidationParser"},{"location":"API/cleanair/parsers/#cleanair.parsers.complex.SatelliteArgumentParser","text":"Argument parsing for Satellite readings","title":"SatelliteArgumentParser"},{"location":"API/cleanair/parsers/#cleanair.parsers.complex.SatelliteArgumentParser.parse_args","text":"Check whether we have the Copernicus key and try to retrieve it from a local secrets file if not","title":"parse_args()"},{"location":"API/cleanair/parsers/#cleanair.parsers.complex.ScootForecastFeatureArgumentParser","text":"Argument parsing for converting SCOOT forecasts into model features","title":"ScootForecastFeatureArgumentParser"},{"location":"API/cleanair/parsers/#cleanair.parsers.complex.ScootReadingArgumentParser","text":"Argument parsing for SCOOT readings","title":"ScootReadingArgumentParser"},{"location":"API/cleanair/parsers/#cleanair.parsers.complex.ScootReadingArgumentParser.parse_args","text":"Check whether we have AWS connection information and try to retrieve it from a local secrets file if not","title":"parse_args()"},{"location":"API/cleanair/parsers/#cleanair.parsers.model","text":"A base class for cleanair parsers.","title":"model"},{"location":"API/cleanair/parsers/#cleanair.parsers.model.BaseModelParser","text":"Parser for CleanAir model entrypoints.","title":"BaseModelParser"},{"location":"API/cleanair/parsers/#cleanair.parsers.model.BaseModelParser.generate_data_config","text":"Return a dictionary of model data configs","title":"generate_data_config()"},{"location":"API/cleanair/parsers/#cleanair.parsers.model.BaseModelParser.parse_kwargs","text":"If the -c flag is passed, then load the config.json file and overwrite any fields that are passed in kwargs.","title":"parse_kwargs()"},{"location":"API/cleanair/parsers/#cleanair.parsers.model.BaseModelParser.save_config","text":"Save the key and values of the parser to a json file.","title":"save_config()"},{"location":"API/cleanair/parsers/#cleanair.parsers.simple","text":"Simple per-entrypoint argument parsers","title":"simple"},{"location":"API/cleanair/parsers/#cleanair.parsers.simple.AQEReadingArgumentParser","text":"Argument parsing for AQE readings","title":"AQEReadingArgumentParser"},{"location":"API/cleanair/parsers/#cleanair.parsers.simple.LAQNReadingArgumentParser","text":"Argument parsing for LAQN readings","title":"LAQNReadingArgumentParser"},{"location":"API/cleanair/parsers/#cleanair.parsers.simple.OsHighwayFeatureArgumentParser","text":"Argument parsing for OS highway features","title":"OsHighwayFeatureArgumentParser"},{"location":"API/cleanair/parsers/#cleanair.parsers.simple.ScootReadingFeatureArgumentParser","text":"Argument parsing for converting SCOOT readings into model features","title":"ScootReadingFeatureArgumentParser"},{"location":"API/cleanair/parsers/#cleanair.parsers.simple.ScootRoadmapArgumentParser","text":"Argument parsing for SCOOT road to detector mapping","title":"ScootRoadmapArgumentParser"},{"location":"API/cleanair/parsers/#cleanair.parsers.simple.StaticDatasetArgumentParser","text":"Argument parsing for static dataset uploads","title":"StaticDatasetArgumentParser"},{"location":"API/cleanair/parsers/#cleanair.parsers.simple.StreetCanyonFeatureArgumentParser","text":"Argument parsing for OS highway features","title":"StreetCanyonFeatureArgumentParser"},{"location":"API/cleanair/parsers/#cleanair.parsers.simple.UKMapFeatureArgumentParser","text":"Argument parsing for OS highway features","title":"UKMapFeatureArgumentParser"},{"location":"API/cleanair/timestamps/","text":"Timestamps \u00b6 \u00b6 Module for timestamp conversions converters \u00b6 Timestamp conversion functions as_datetime ( maybe_dt ) \u00b6 Convert an input that might be a datetime into a datetime datetime_from_str ( naive_string , timezone , rounded = False ) \u00b6 Convert naive string to localised datetime datetime_from_unix ( timestamp ) \u00b6 Convert unix timestamp to datetime safe_strptime ( naive_string , format_str ) \u00b6 Wrapper around strptime to allow for broken time strings to_nearest_hour ( input_datetime ) \u00b6 Rounds to nearest hour by adding a timedelta of one hour if the minute is 30 or later then truncating on hour unix_from_str ( naive_string , timezone , rounded = False ) \u00b6 Convert naive string to unix timestamp utcstr_from_datetime ( input_datetime , rounded = False ) \u00b6 Convert datetime to UTC string utcstr_from_unix ( timestamp , rounded = False ) \u00b6 Convert unix timestamp to UTC string","title":"Timestamps"},{"location":"API/cleanair/timestamps/#timestamps","text":"","title":"Timestamps"},{"location":"API/cleanair/timestamps/#cleanair.timestamps","text":"Module for timestamp conversions","title":"cleanair.timestamps"},{"location":"API/cleanair/timestamps/#cleanair.timestamps.converters","text":"Timestamp conversion functions","title":"converters"},{"location":"API/cleanair/timestamps/#cleanair.timestamps.converters.as_datetime","text":"Convert an input that might be a datetime into a datetime","title":"as_datetime()"},{"location":"API/cleanair/timestamps/#cleanair.timestamps.converters.datetime_from_str","text":"Convert naive string to localised datetime","title":"datetime_from_str()"},{"location":"API/cleanair/timestamps/#cleanair.timestamps.converters.datetime_from_unix","text":"Convert unix timestamp to datetime","title":"datetime_from_unix()"},{"location":"API/cleanair/timestamps/#cleanair.timestamps.converters.safe_strptime","text":"Wrapper around strptime to allow for broken time strings","title":"safe_strptime()"},{"location":"API/cleanair/timestamps/#cleanair.timestamps.converters.to_nearest_hour","text":"Rounds to nearest hour by adding a timedelta of one hour if the minute is 30 or later then truncating on hour","title":"to_nearest_hour()"},{"location":"API/cleanair/timestamps/#cleanair.timestamps.converters.unix_from_str","text":"Convert naive string to unix timestamp","title":"unix_from_str()"},{"location":"API/cleanair/timestamps/#cleanair.timestamps.converters.utcstr_from_datetime","text":"Convert datetime to UTC string","title":"utcstr_from_datetime()"},{"location":"API/cleanair/timestamps/#cleanair.timestamps.converters.utcstr_from_unix","text":"Convert unix timestamp to UTC string","title":"utcstr_from_unix()"},{"location":"API/odysseus/databases/","text":"Databases \u00b6 \u00b6 Database classes and functions for traffic. mixins special \u00b6 Mixins for databases. traffic_data_mixin \u00b6 Mixin query for traffic data config. TrafficDataQueryMixin \u00b6 Queries for the traffic data table. get_data_config ( self , start_time = None , end_time = None , detectors = None , baseline_period = None ) \u00b6 Get the data id and config from the TrafficDataTable. traffic_metric_mixin \u00b6 Mixin query for traffic metrics. TrafficMetricQueryMixin \u00b6 Query the metrics of traffic models that have been evaluated. get_instance_metrics ( self , tag = None , data_ids = None , param_ids = None , models = None ) \u00b6 Get instances joined with the metrics. traffic_queries \u00b6 Class for querying traffic and scoot data. TrafficInstanceQuery \u00b6 Query traffic instances. TrafficQuery \u00b6 Query traffic data.","title":"Databases"},{"location":"API/odysseus/databases/#databases","text":"","title":"Databases"},{"location":"API/odysseus/databases/#odysseus.databases","text":"Database classes and functions for traffic.","title":"odysseus.databases"},{"location":"API/odysseus/databases/#odysseus.databases.mixins","text":"Mixins for databases.","title":"mixins"},{"location":"API/odysseus/databases/#odysseus.databases.mixins.traffic_data_mixin","text":"Mixin query for traffic data config.","title":"traffic_data_mixin"},{"location":"API/odysseus/databases/#odysseus.databases.mixins.traffic_data_mixin.TrafficDataQueryMixin","text":"Queries for the traffic data table.","title":"TrafficDataQueryMixin"},{"location":"API/odysseus/databases/#odysseus.databases.mixins.traffic_data_mixin.TrafficDataQueryMixin.get_data_config","text":"Get the data id and config from the TrafficDataTable.","title":"get_data_config()"},{"location":"API/odysseus/databases/#odysseus.databases.mixins.traffic_metric_mixin","text":"Mixin query for traffic metrics.","title":"traffic_metric_mixin"},{"location":"API/odysseus/databases/#odysseus.databases.mixins.traffic_metric_mixin.TrafficMetricQueryMixin","text":"Query the metrics of traffic models that have been evaluated.","title":"TrafficMetricQueryMixin"},{"location":"API/odysseus/databases/#odysseus.databases.mixins.traffic_metric_mixin.TrafficMetricQueryMixin.get_instance_metrics","text":"Get instances joined with the metrics.","title":"get_instance_metrics()"},{"location":"API/odysseus/databases/#odysseus.databases.traffic_queries","text":"Class for querying traffic and scoot data.","title":"traffic_queries"},{"location":"API/odysseus/databases/#odysseus.databases.traffic_queries.TrafficInstanceQuery","text":"Query traffic instances.","title":"TrafficInstanceQuery"},{"location":"API/odysseus/databases/#odysseus.databases.traffic_queries.TrafficQuery","text":"Query traffic data.","title":"TrafficQuery"},{"location":"API/odysseus/dates/","text":"Dates \u00b6 \u00b6 Functions and constants for dates and times. lockdown \u00b6 Functions and constants for the lockdown period. timestamp_is_lockdown ( timestamp ) \u00b6 Check if the timestamp is in the lockdown period. Parameters: Name Type Description Default timestamp str The date(time) to check. required Returns: Type Description bool True if the timestamp is greater than or equal to the lockdown start and strictly less than the lockdown end. normal \u00b6 Functions and constants for the normal baseline period. timestamp_is_normal ( timestamp ) \u00b6 Check if the timestamp is in the normal period. Parameters: Name Type Description Default timestamp str The date(time) to check. required Returns: Type Description bool True if the timestamp is greater than or equal to the normal start and strictly less than the normal end.","title":"Dates"},{"location":"API/odysseus/dates/#dates","text":"","title":"Dates"},{"location":"API/odysseus/dates/#odysseus.dates","text":"Functions and constants for dates and times.","title":"odysseus.dates"},{"location":"API/odysseus/dates/#odysseus.dates.lockdown","text":"Functions and constants for the lockdown period.","title":"lockdown"},{"location":"API/odysseus/dates/#odysseus.dates.lockdown.timestamp_is_lockdown","text":"Check if the timestamp is in the lockdown period. Parameters: Name Type Description Default timestamp str The date(time) to check. required Returns: Type Description bool True if the timestamp is greater than or equal to the lockdown start and strictly less than the lockdown end.","title":"timestamp_is_lockdown()"},{"location":"API/odysseus/dates/#odysseus.dates.normal","text":"Functions and constants for the normal baseline period.","title":"normal"},{"location":"API/odysseus/dates/#odysseus.dates.normal.timestamp_is_normal","text":"Check if the timestamp is in the normal period. Parameters: Name Type Description Default timestamp str The date(time) to check. required Returns: Type Description bool True if the timestamp is greater than or equal to the normal start and strictly less than the normal end.","title":"timestamp_is_normal()"},{"location":"API/odysseus/metric/","text":"Metric \u00b6 \u00b6 Metrics for evaluating traffic models and comparing against baselines. percent \u00b6 Functions for calculating the percentage change in traffic. percent_of_baseline ( baseline_df , comparison_df , groupby_cols = None , min_condfidence_count = 6 ) \u00b6 Percent change from the baseline for each detector in the comparison dataframe. Parameters: Name Type Description Default baseline_df DataFrame Contains observations, possibly for multiple detectors over multiple weeks. required comparison_df DataFrame A single day of observations, possibly for multiple detectors. required Kwargs Minimum number of observations to calculate the percent of baseline metric with confidence. Less observations than this value will set the low_confidence flag to true. Returns: Type Description DataFrame Percent of baseline metric for each detector with columns for e.g. flags. percent_of_baseline_counts ( baseline_count , comparison_count ) \u00b6 Calculate the percentage change of the comparison count compared to the baseline. Parameters: Name Type Description Default baseline_count int Total count in baseline period. required comparison_count int Total count in comparison period. required Returns: Type Description float If comparison_count <= baseline count then returned value will be 0 - 100%. Else returned value will be greater than 100%.","title":"Metric"},{"location":"API/odysseus/metric/#metric","text":"","title":"Metric"},{"location":"API/odysseus/metric/#odysseus.metric","text":"Metrics for evaluating traffic models and comparing against baselines.","title":"odysseus.metric"},{"location":"API/odysseus/metric/#odysseus.metric.percent","text":"Functions for calculating the percentage change in traffic.","title":"percent"},{"location":"API/odysseus/metric/#odysseus.metric.percent.percent_of_baseline","text":"Percent change from the baseline for each detector in the comparison dataframe. Parameters: Name Type Description Default baseline_df DataFrame Contains observations, possibly for multiple detectors over multiple weeks. required comparison_df DataFrame A single day of observations, possibly for multiple detectors. required Kwargs Minimum number of observations to calculate the percent of baseline metric with confidence. Less observations than this value will set the low_confidence flag to true. Returns: Type Description DataFrame Percent of baseline metric for each detector with columns for e.g. flags.","title":"percent_of_baseline()"},{"location":"API/odysseus/metric/#odysseus.metric.percent.percent_of_baseline_counts","text":"Calculate the percentage change of the comparison count compared to the baseline. Parameters: Name Type Description Default baseline_count int Total count in baseline period. required comparison_count int Total count in comparison period. required Returns: Type Description float If comparison_count <= baseline count then returned value will be 0 - 100%. Else returned value will be greater than 100%.","title":"percent_of_baseline_counts()"},{"location":"API/odysseus/parsers/","text":"Parsers \u00b6 \u00b6 Useful functions for filepath management and others. baseline_parser \u00b6 Parsers for the odysseus module and their entrypoints. BaselineParser \u00b6 Parser for querying a recent day against a baseline. validate_type ( datestr ) \u00b6 Ensure the datestr passed in valid. If yesterday is passed then return yesterdays date. parser \u00b6 Parsers for the odysseus module and their entrypoints. BaselineParser \u00b6 Parser for querying a recent day against a baseline. validate_type ( datestr ) \u00b6 Ensure the datestr passed in valid. If yesterday is passed then return yesterdays date.","title":"Parsers"},{"location":"API/odysseus/parsers/#parsers","text":"","title":"Parsers"},{"location":"API/odysseus/parsers/#odysseus.parsers","text":"Useful functions for filepath management and others.","title":"odysseus.parsers"},{"location":"API/odysseus/parsers/#odysseus.parsers.baseline_parser","text":"Parsers for the odysseus module and their entrypoints.","title":"baseline_parser"},{"location":"API/odysseus/parsers/#odysseus.parsers.baseline_parser.BaselineParser","text":"Parser for querying a recent day against a baseline.","title":"BaselineParser"},{"location":"API/odysseus/parsers/#odysseus.parsers.baseline_parser.validate_type","text":"Ensure the datestr passed in valid. If yesterday is passed then return yesterdays date.","title":"validate_type()"},{"location":"API/odysseus/parsers/#odysseus.parsers.parser","text":"Parsers for the odysseus module and their entrypoints.","title":"parser"},{"location":"API/odysseus/parsers/#odysseus.parsers.parser.BaselineParser","text":"Parser for querying a recent day against a baseline.","title":"BaselineParser"},{"location":"API/odysseus/parsers/#odysseus.parsers.parser.validate_type","text":"Ensure the datestr passed in valid. If yesterday is passed then return yesterdays date.","title":"validate_type()"},{"location":"API/odysseus/preprocess/","text":"Preprocess \u00b6 \u00b6 Functions for removing anomalies, cleaning dataframes and filtering. anomaly \u00b6 Methods for detecting and removing anomalies. get_index_of_outliers ( scoot_df , sigmas = 3 , col = 'n_vehicles_in_interval' ) \u00b6 Returns a list of indices that are outliers. remove_outliers ( scoot_df , sigmas = 3 , col = 'n_vehicles_in_interval' ) \u00b6 Remove outliers $x$ where $|x - \\mu| > k \\sigma$ for each detector where $k$ is sigmas . Parameters: Name Type Description Default scoot_df DataFrame Scoot data. required sigmas int Number of standard deviations from the mean. 3 col str Name of the column to look for anomalies. 'n_vehicles_in_interval' Returns: Type Description DataFrame Data with no anomalies. normalise \u00b6 Functions for normalising data ready for modelling. denormalise ( x , wrt_y ) \u00b6 Denormalize x given the original data it was standardized to normalise ( x ) \u00b6 Standardize all columns individually normalise_datetime ( time_df , wrt = 'hour' , col = 'measurement_start_utc' ) \u00b6 Normalise a pandas datetime series with respect to (wrt) a time attribute. Parameters: Name Type Description Default time_df DataFrame Must have a datetime col to normalise. required wrt str Normalise with respect to 'clipped_hour', 'hour' or 'epoch'. 'hour' col str Name of the datetime column. 'measurement_start_utc' Returns: Type Description DataFrame DataFrame with two new columns called 'time' and 'time_norm'. normalise_location ( space_df , x_col = 'lon' , y_col = 'lat' ) \u00b6 Normalise the location columns of a pandas dataframe. Parameters: Name Type Description Default space_df DataFrame Must have two spatial columns to normalise. required x_col The name of the column on the x spatial axis. 'lon' y_col The name of the column on the y spatial axis. 'lat' Returns: Type Description DataFrame space_df with two new columsn called lon_norm and lat_norm.","title":"Preprocess"},{"location":"API/odysseus/preprocess/#preprocess","text":"","title":"Preprocess"},{"location":"API/odysseus/preprocess/#odysseus.preprocess","text":"Functions for removing anomalies, cleaning dataframes and filtering.","title":"odysseus.preprocess"},{"location":"API/odysseus/preprocess/#odysseus.preprocess.anomaly","text":"Methods for detecting and removing anomalies.","title":"anomaly"},{"location":"API/odysseus/preprocess/#odysseus.preprocess.anomaly.get_index_of_outliers","text":"Returns a list of indices that are outliers.","title":"get_index_of_outliers()"},{"location":"API/odysseus/preprocess/#odysseus.preprocess.anomaly.remove_outliers","text":"Remove outliers $x$ where $|x - \\mu| > k \\sigma$ for each detector where $k$ is sigmas . Parameters: Name Type Description Default scoot_df DataFrame Scoot data. required sigmas int Number of standard deviations from the mean. 3 col str Name of the column to look for anomalies. 'n_vehicles_in_interval' Returns: Type Description DataFrame Data with no anomalies.","title":"remove_outliers()"},{"location":"API/odysseus/preprocess/#odysseus.preprocess.normalise","text":"Functions for normalising data ready for modelling.","title":"normalise"},{"location":"API/odysseus/preprocess/#odysseus.preprocess.normalise.denormalise","text":"Denormalize x given the original data it was standardized to","title":"denormalise()"},{"location":"API/odysseus/preprocess/#odysseus.preprocess.normalise.normalise","text":"Standardize all columns individually","title":"normalise()"},{"location":"API/odysseus/preprocess/#odysseus.preprocess.normalise.normalise_datetime","text":"Normalise a pandas datetime series with respect to (wrt) a time attribute. Parameters: Name Type Description Default time_df DataFrame Must have a datetime col to normalise. required wrt str Normalise with respect to 'clipped_hour', 'hour' or 'epoch'. 'hour' col str Name of the datetime column. 'measurement_start_utc' Returns: Type Description DataFrame DataFrame with two new columns called 'time' and 'time_norm'.","title":"normalise_datetime()"},{"location":"API/odysseus/preprocess/#odysseus.preprocess.normalise.normalise_location","text":"Normalise the location columns of a pandas dataframe. Parameters: Name Type Description Default space_df DataFrame Must have two spatial columns to normalise. required x_col The name of the column on the x spatial axis. 'lon' y_col The name of the column on the y spatial axis. 'lat' Returns: Type Description DataFrame space_df with two new columsn called lon_norm and lat_norm.","title":"normalise_location()"},{"location":"API/odysseus/scoot_processing/","text":"Scoot_processing \u00b6 \u00b6 Database classes and functions for traffic. percentage_change \u00b6 Class for querying traffic and scoot data. TrafficPercentageChange \u00b6 Queries to run on the SCOOT DB. get_percent_of_baseline ( self , baseline_period , comparison_start , comparison_end = None , detectors = None ) \u00b6 Get the values for the percent_of_baseline metric for a day and baseline. percent_of_baseline ( self , comparison_start_date ) \u00b6 Calculate the percentage of baseline update_remote_tables ( self ) \u00b6 Update all relevant tables on the remote database","title":"Scoot_processing"},{"location":"API/odysseus/scoot_processing/#scoot_processing","text":"","title":"Scoot_processing"},{"location":"API/odysseus/scoot_processing/#odysseus.scoot_processing","text":"Database classes and functions for traffic.","title":"odysseus.scoot_processing"},{"location":"API/odysseus/scoot_processing/#odysseus.scoot_processing.percentage_change","text":"Class for querying traffic and scoot data.","title":"percentage_change"},{"location":"API/odysseus/scoot_processing/#odysseus.scoot_processing.percentage_change.TrafficPercentageChange","text":"Queries to run on the SCOOT DB.","title":"TrafficPercentageChange"},{"location":"API/odysseus/scoot_processing/#odysseus.scoot_processing.percentage_change.TrafficPercentageChange.get_percent_of_baseline","text":"Get the values for the percent_of_baseline metric for a day and baseline.","title":"get_percent_of_baseline()"},{"location":"API/odysseus/scoot_processing/#odysseus.scoot_processing.percentage_change.TrafficPercentageChange.percent_of_baseline","text":"Calculate the percentage of baseline","title":"percent_of_baseline()"},{"location":"API/odysseus/scoot_processing/#odysseus.scoot_processing.percentage_change.TrafficPercentageChange.update_remote_tables","text":"Update all relevant tables on the remote database","title":"update_remote_tables()"},{"location":"API/odysseus/test_types/","text":"Test Types \u00b6 \u00b6 Traffic datasets. traffic_dataset \u00b6 A traffic dataset. TrafficDataset \u00b6 A traffic dataset that queries the database given a data config dictionary. data_config property readonly \u00b6 A dictionary of data settings. data_id property readonly \u00b6 The id of the hashed data config. dataframe property readonly \u00b6 The dataset dataframe. element_spec property readonly \u00b6 Get the element specification of the dataset. features_tensor property readonly \u00b6 The feature tensor. preprocessing property readonly \u00b6 A dictionary of preprocessing settings. target_tensor property readonly \u00b6 The target tensor. data_id_from_hash ( data_config , preprocessing ) staticmethod \u00b6 Generate an id from the hash of the two settings dictionaries. Parameters: Name Type Description Default data_config dict Settings for data. required preprocessing dict Settings for preprocessing and normalising data. required Returns: Type Description str An unique id given the settings dictionaries. from_dataframe ( traffic_df , preprocessing ) staticmethod \u00b6 Return a dataset given a traffic dataframe. Parameters: Name Type Description Default traffic_df DataFrame Preprocessed traffic data. required preprocessing dict Settings for preprocessing. required Returns: Type Description DatasetV1 A new tensorflow dataset. preprocess_dataframe ( traffic_df , preprocessing ) staticmethod \u00b6 Return a dataframe that has been normalised and preprocessed. Parameters: Name Type Description Default dataframe Raw Scoot data. required preprocessing dict Settings for preprocessing the dataframe. required Returns: Type Description DataFrame Preprocessed traffic data. update_remote_tables ( self ) \u00b6 Update the data config table for traffic. validate_data_config ( data_config ) staticmethod \u00b6 Checks if the dictionary of data settings passed is valid. Parameters: Name Type Description Default data_config dict Settings to load the data. required validate_dataframe ( traffic_df , features = None , target = None ) staticmethod \u00b6 Check the dataframe passed has the right column names. Parameters: Name Type Description Default traffic_df DataFrame Raw traffic data. required features Collection Names of features. None target Collection Names of targets. None Exceptions: Type Description AssertionError If the dataframe is not valid. validate_preprocessing ( preprocessing ) staticmethod \u00b6 Checks if the dictionary passed is valid for preprocessing a traffic dataset. Parameters: Name Type Description Default preprocessing dict Settings for preprocessing and normalising the traffic data. required Exceptions: Type Description AssertionError If the dictionary is not valid. validate_dictionary ( dict_to_check , min_keys , value_types ) \u00b6 Check the dictionary contains a minimum set of keys and the value types are as expected. Parameters: Name Type Description Default dict_to_check dict A dictionary which will be validated. required min_keys list The minimum keys that must be in the dict. required value_types list The types of the dictionary values. required Exceptions: Type Description KeyError If one of the dictionary keys are missing. TypeError If the type of the values are invalid. Notes In python 3.8 type hinting for dictionaries is supported. When upgrading to python 3.8 this function should use type hinting. Only supports dictionaries with one depth level.","title":"Test Types"},{"location":"API/odysseus/test_types/#test-types","text":"","title":"Test Types"},{"location":"API/odysseus/test_types/#odysseus.dataset","text":"Traffic datasets.","title":"odysseus.dataset"},{"location":"API/odysseus/test_types/#odysseus.dataset.traffic_dataset","text":"A traffic dataset.","title":"traffic_dataset"},{"location":"API/odysseus/test_types/#odysseus.dataset.traffic_dataset.TrafficDataset","text":"A traffic dataset that queries the database given a data config dictionary.","title":"TrafficDataset"},{"location":"API/odysseus/test_types/#odysseus.dataset.traffic_dataset.TrafficDataset.data_config","text":"A dictionary of data settings.","title":"data_config"},{"location":"API/odysseus/test_types/#odysseus.dataset.traffic_dataset.TrafficDataset.data_id","text":"The id of the hashed data config.","title":"data_id"},{"location":"API/odysseus/test_types/#odysseus.dataset.traffic_dataset.TrafficDataset.dataframe","text":"The dataset dataframe.","title":"dataframe"},{"location":"API/odysseus/test_types/#odysseus.dataset.traffic_dataset.TrafficDataset.element_spec","text":"Get the element specification of the dataset.","title":"element_spec"},{"location":"API/odysseus/test_types/#odysseus.dataset.traffic_dataset.TrafficDataset.features_tensor","text":"The feature tensor.","title":"features_tensor"},{"location":"API/odysseus/test_types/#odysseus.dataset.traffic_dataset.TrafficDataset.preprocessing","text":"A dictionary of preprocessing settings.","title":"preprocessing"},{"location":"API/odysseus/test_types/#odysseus.dataset.traffic_dataset.TrafficDataset.target_tensor","text":"The target tensor.","title":"target_tensor"},{"location":"API/odysseus/test_types/#odysseus.dataset.traffic_dataset.TrafficDataset.data_id_from_hash","text":"Generate an id from the hash of the two settings dictionaries. Parameters: Name Type Description Default data_config dict Settings for data. required preprocessing dict Settings for preprocessing and normalising data. required Returns: Type Description str An unique id given the settings dictionaries.","title":"data_id_from_hash()"},{"location":"API/odysseus/test_types/#odysseus.dataset.traffic_dataset.TrafficDataset.from_dataframe","text":"Return a dataset given a traffic dataframe. Parameters: Name Type Description Default traffic_df DataFrame Preprocessed traffic data. required preprocessing dict Settings for preprocessing. required Returns: Type Description DatasetV1 A new tensorflow dataset.","title":"from_dataframe()"},{"location":"API/odysseus/test_types/#odysseus.dataset.traffic_dataset.TrafficDataset.preprocess_dataframe","text":"Return a dataframe that has been normalised and preprocessed. Parameters: Name Type Description Default dataframe Raw Scoot data. required preprocessing dict Settings for preprocessing the dataframe. required Returns: Type Description DataFrame Preprocessed traffic data.","title":"preprocess_dataframe()"},{"location":"API/odysseus/test_types/#odysseus.dataset.traffic_dataset.TrafficDataset.update_remote_tables","text":"Update the data config table for traffic.","title":"update_remote_tables()"},{"location":"API/odysseus/test_types/#odysseus.dataset.traffic_dataset.TrafficDataset.validate_data_config","text":"Checks if the dictionary of data settings passed is valid. Parameters: Name Type Description Default data_config dict Settings to load the data. required","title":"validate_data_config()"},{"location":"API/odysseus/test_types/#odysseus.dataset.traffic_dataset.TrafficDataset.validate_dataframe","text":"Check the dataframe passed has the right column names. Parameters: Name Type Description Default traffic_df DataFrame Raw traffic data. required features Collection Names of features. None target Collection Names of targets. None Exceptions: Type Description AssertionError If the dataframe is not valid.","title":"validate_dataframe()"},{"location":"API/odysseus/test_types/#odysseus.dataset.traffic_dataset.TrafficDataset.validate_preprocessing","text":"Checks if the dictionary passed is valid for preprocessing a traffic dataset. Parameters: Name Type Description Default preprocessing dict Settings for preprocessing and normalising the traffic data. required Exceptions: Type Description AssertionError If the dictionary is not valid.","title":"validate_preprocessing()"},{"location":"API/odysseus/test_types/#odysseus.dataset.traffic_dataset.validate_dictionary","text":"Check the dictionary contains a minimum set of keys and the value types are as expected. Parameters: Name Type Description Default dict_to_check dict A dictionary which will be validated. required min_keys list The minimum keys that must be in the dict. required value_types list The types of the dictionary values. required Exceptions: Type Description KeyError If one of the dictionary keys are missing. TypeError If the type of the values are invalid. Notes In python 3.8 type hinting for dictionaries is supported. When upgrading to python 3.8 this function should use type hinting. Only supports dictionaries with one depth level.","title":"validate_dictionary()"},{"location":"setup/azure-account/","text":"Azure account \u00b6 To contribute to the Turing deployment of this project you will need to be on the Turing Institute's Azure active directory. In other words you will need a turing email address <someone>@turing.ac.uk . If you do not have one already contact an infrastructure administrator . If you are deploying the CleanAir infrastrucure elsewhere you should have access to an Azure account (the cloud-computing platform where the infrastructure is deployed).","title":"Azure account"},{"location":"setup/azure-account/#azure-account","text":"To contribute to the Turing deployment of this project you will need to be on the Turing Institute's Azure active directory. In other words you will need a turing email address <someone>@turing.ac.uk . If you do not have one already contact an infrastructure administrator . If you are deploying the CleanAir infrastrucure elsewhere you should have access to an Azure account (the cloud-computing platform where the infrastructure is deployed).","title":"Azure account"},{"location":"setup/configure-local-database/","text":"Configure a local database \u00b6 In production we use a managed PostgreSQL database . However, it is useful to have a local copy to run tests and for development. To set up a local version start a local postgres server: brew services start postgresql If you installed the database using conda Set it up the server and users first with: initdb -D mylocal_db pg_ctl -D mylocal_db -l logfile start createdb --owner = ${ USER } myinner_db When you want to work in this environment again you'll need to run: pg_ctl -D mylocal_db -l logfile start You can stop it with: pg_ctl -D mylocal_db stop Create a local secrets file \u00b6 We store database credentials in json files. For production databases you should never store database passwords in these files - for more information see the production database section . mkdir -p .secrets echo '{ \"username\": \"postgres\", \"password\": \"''\", \"host\": \"localhost\", \"port\": 5432, \"db_name\": \"cleanair_test_db\", \"ssl_mode\": \"prefer\" }' >> .secrets/.db_secrets_offline.json N.B In some cases your default username may be your OS user. Change the username in the file above if this is the case. createdb cleanair_test_db Create Schema and roles \u00b6 We must now setup the database schema. This also creates a number of roles on the database. Create a variable with the location of your secrets file SECRETS = $( pwd ) /.secrets/.db_secrets_offline.json python containers/entrypoints/setup/configure_db_roles.py -s $SECRETS -c configuration/database_role_config/local_database_config.yaml Static data insert \u00b6 The database requires a number of static datasets. We can now insert static data into our local database. You will need a SAS token to access static data files stored on Azure. If you have access Azure you can log in to Azure from the command line and run the following to obtain a SAS token: SAS_TOKEN = $( python containers/entrypoints/setup/insert_static_datasets.py generate ) By default the SAS token will last for 1 hour. If you need a longer expiry time pass --days and --hours arguments to the program above. N.B. It's better to use short expiry dates where possible. Otherwise you must request a SAS token from an infrastructure developer and set it as a variable: SAS_TOKEN = <SAS_TOKEN> You can then download and insert all static data into the database by running the following: python containers/entrypoints/setup/insert_static_datasets.py insert -t $SAS_TOKEN -s $SECRETS -d rectgrid_100 street_canyon hexgrid london_boundary oshighway_roadlink scoot_detector urban_village If you would also like to add UKMAP to the database run: python containers/entrypoints/setup/insert_static_datasets.py insert -t $SAS_TOKEN -s $SECRETS -d ukmap UKMAP is extremly large and will take ~1h to download and insert. We therefore do not run tests against UKMAP at the moment. N.B SAS tokens will expire after a short length of time, after which you will need to request a new one. Check the database configuration \u00b6 You can check everything configured correctly by running: pytest containers/tests/test_database_init --secretfile $SECRETS","title":"Configure a local database"},{"location":"setup/configure-local-database/#configure-a-local-database","text":"In production we use a managed PostgreSQL database . However, it is useful to have a local copy to run tests and for development. To set up a local version start a local postgres server: brew services start postgresql If you installed the database using conda Set it up the server and users first with: initdb -D mylocal_db pg_ctl -D mylocal_db -l logfile start createdb --owner = ${ USER } myinner_db When you want to work in this environment again you'll need to run: pg_ctl -D mylocal_db -l logfile start You can stop it with: pg_ctl -D mylocal_db stop","title":"Configure a local database"},{"location":"setup/configure-local-database/#create-a-local-secrets-file","text":"We store database credentials in json files. For production databases you should never store database passwords in these files - for more information see the production database section . mkdir -p .secrets echo '{ \"username\": \"postgres\", \"password\": \"''\", \"host\": \"localhost\", \"port\": 5432, \"db_name\": \"cleanair_test_db\", \"ssl_mode\": \"prefer\" }' >> .secrets/.db_secrets_offline.json N.B In some cases your default username may be your OS user. Change the username in the file above if this is the case. createdb cleanair_test_db","title":"Create a local secrets file"},{"location":"setup/configure-local-database/#create-schema-and-roles","text":"We must now setup the database schema. This also creates a number of roles on the database. Create a variable with the location of your secrets file SECRETS = $( pwd ) /.secrets/.db_secrets_offline.json python containers/entrypoints/setup/configure_db_roles.py -s $SECRETS -c configuration/database_role_config/local_database_config.yaml","title":"Create Schema and roles"},{"location":"setup/configure-local-database/#static-data-insert","text":"The database requires a number of static datasets. We can now insert static data into our local database. You will need a SAS token to access static data files stored on Azure. If you have access Azure you can log in to Azure from the command line and run the following to obtain a SAS token: SAS_TOKEN = $( python containers/entrypoints/setup/insert_static_datasets.py generate ) By default the SAS token will last for 1 hour. If you need a longer expiry time pass --days and --hours arguments to the program above. N.B. It's better to use short expiry dates where possible. Otherwise you must request a SAS token from an infrastructure developer and set it as a variable: SAS_TOKEN = <SAS_TOKEN> You can then download and insert all static data into the database by running the following: python containers/entrypoints/setup/insert_static_datasets.py insert -t $SAS_TOKEN -s $SECRETS -d rectgrid_100 street_canyon hexgrid london_boundary oshighway_roadlink scoot_detector urban_village If you would also like to add UKMAP to the database run: python containers/entrypoints/setup/insert_static_datasets.py insert -t $SAS_TOKEN -s $SECRETS -d ukmap UKMAP is extremly large and will take ~1h to download and insert. We therefore do not run tests against UKMAP at the moment. N.B SAS tokens will expire after a short length of time, after which you will need to request a new one.","title":"Static data insert"},{"location":"setup/configure-local-database/#check-the-database-configuration","text":"You can check everything configured correctly by running: pytest containers/tests/test_database_init --secretfile $SECRETS","title":"Check the database configuration"},{"location":"setup/infrastructure-dependencies/","text":"Infrastructure dependencies \u00b6 Cloud infrastructure developers will require the following in addition to the non-infrastructure dependencies . Infrastructure development \u00b6 Access to the deployment Azure subscription Terraform (for configuring the Azure infrastructure) Travis Continuous Integration (CI) CLI (for setting up automatic deployments) Azure subscription \u00b6 You need to have access to the CleanAir Azure subscription to deploy infrastructure. If you need access contact an infrastructure administrator Terraform \u00b6 The Azure infrastructure is managed with Terraform . To get started download Terraform from their website . If using Mac OS, you can instead use homebrew : brew install terraform Travis CI CLI \u00b6 Ensure you have Ruby 1.9.3 or above installed: brew install ruby gem update --system Then install the Travis CI CLI with: gem install travis -no-rdoc -no-ri On some versions of OSX, this fails, so you may need the following alternative: ARCHFLAGS=-Wno-error=unused-command-line-argument-hard-error-in-future gem install --user-install travis -v 1.8.13 --no-document Verify with travis version If this fails ensure Gems user_dir is on the path: cat << EOF >> ~/.bash_profile export PATH=\"\\$PATH:$(ruby -e 'puts Gem.user_dir')/bin\" EOF With a Conda environment \u00b6 It's possible to set it up all with a conda environment, this way you can keep different versions of software around in your machine. All the steps above can be done with: # Non-infrastructure dependencies conda create -n busyness python = 3 .7 conda activate busyness conda install -c anaconda postgresql conda install -c conda-forge gdal postgis uwsgi pip install azure-cli pip install azure-nspkg azure-mgmt-nspkg # The following fails with: ERROR: azure-cli 2.6.0 has requirement azure-storage-blob<2.0.0,>=1.3.1, but you'll have azure-storage-blob 12.3.0 which is incompatible. # but they install fine. pip install -r containers/requirements.txt pip install -e 'containers/cleanair[models, dashboard]' pip install -e 'containers/odysseus' pip install -e 'containers/urbanair' ## Infrastructure dependencies # if you don't get rb-ffi and rb-json you'll need to install gcc_linux-64 and libgcc to build these in order to install travis. conda install -c conda-forge terraform ruby rb-ffi rb-json # At least on Linux you'll need to dissable IPV6 to make this version of gem to work. gem install travis -no-rdoc -no-ri # Create a soft link of the executables installed by gem into a place seen within the conda env. conda_env = $( conda info --json | grep -w \"active_prefix\" | awk '{print $2}' | sed -e 's/,//' -e 's/\"//g' ) ln -s $( find $conda_env -iname 'travis' | grep bin ) $conda_env /bin/ Login to Azure \u00b6 To start working with Azure , you must first login to your account from the terminal: az login Infrastructure developers \u00b6 Infrastructure developers should additionally check which Azure subscriptions you have access to by running az account list --output table --refresh Then set your default subscription to the Clean Air project (if you cannot see it in the output generated from the last line you do not have access): az account set --subscription \"CleanAir\" If you don't have access this is ok. You only need it to deploy and manage infrastructure.","title":"Infrastructure dependencies"},{"location":"setup/infrastructure-dependencies/#infrastructure-dependencies","text":"Cloud infrastructure developers will require the following in addition to the non-infrastructure dependencies .","title":"Infrastructure dependencies"},{"location":"setup/infrastructure-dependencies/#infrastructure-development","text":"Access to the deployment Azure subscription Terraform (for configuring the Azure infrastructure) Travis Continuous Integration (CI) CLI (for setting up automatic deployments)","title":"Infrastructure development"},{"location":"setup/infrastructure-dependencies/#azure-subscription","text":"You need to have access to the CleanAir Azure subscription to deploy infrastructure. If you need access contact an infrastructure administrator","title":"Azure subscription"},{"location":"setup/infrastructure-dependencies/#terraform","text":"The Azure infrastructure is managed with Terraform . To get started download Terraform from their website . If using Mac OS, you can instead use homebrew : brew install terraform","title":"Terraform"},{"location":"setup/infrastructure-dependencies/#travis-ci-cli","text":"Ensure you have Ruby 1.9.3 or above installed: brew install ruby gem update --system Then install the Travis CI CLI with: gem install travis -no-rdoc -no-ri On some versions of OSX, this fails, so you may need the following alternative: ARCHFLAGS=-Wno-error=unused-command-line-argument-hard-error-in-future gem install --user-install travis -v 1.8.13 --no-document Verify with travis version If this fails ensure Gems user_dir is on the path: cat << EOF >> ~/.bash_profile export PATH=\"\\$PATH:$(ruby -e 'puts Gem.user_dir')/bin\" EOF","title":"Travis CI CLI"},{"location":"setup/infrastructure-dependencies/#with-a-conda-environment","text":"It's possible to set it up all with a conda environment, this way you can keep different versions of software around in your machine. All the steps above can be done with: # Non-infrastructure dependencies conda create -n busyness python = 3 .7 conda activate busyness conda install -c anaconda postgresql conda install -c conda-forge gdal postgis uwsgi pip install azure-cli pip install azure-nspkg azure-mgmt-nspkg # The following fails with: ERROR: azure-cli 2.6.0 has requirement azure-storage-blob<2.0.0,>=1.3.1, but you'll have azure-storage-blob 12.3.0 which is incompatible. # but they install fine. pip install -r containers/requirements.txt pip install -e 'containers/cleanair[models, dashboard]' pip install -e 'containers/odysseus' pip install -e 'containers/urbanair' ## Infrastructure dependencies # if you don't get rb-ffi and rb-json you'll need to install gcc_linux-64 and libgcc to build these in order to install travis. conda install -c conda-forge terraform ruby rb-ffi rb-json # At least on Linux you'll need to dissable IPV6 to make this version of gem to work. gem install travis -no-rdoc -no-ri # Create a soft link of the executables installed by gem into a place seen within the conda env. conda_env = $( conda info --json | grep -w \"active_prefix\" | awk '{print $2}' | sed -e 's/,//' -e 's/\"//g' ) ln -s $( find $conda_env -iname 'travis' | grep bin ) $conda_env /bin/","title":"With a Conda environment"},{"location":"setup/infrastructure-dependencies/#login-to-azure","text":"To start working with Azure , you must first login to your account from the terminal: az login","title":"Login to Azure"},{"location":"setup/infrastructure-dependencies/#infrastructure-developers","text":"Infrastructure developers should additionally check which Azure subscriptions you have access to by running az account list --output table --refresh Then set your default subscription to the Clean Air project (if you cannot see it in the output generated from the last line you do not have access): az account set --subscription \"CleanAir\" If you don't have access this is ok. You only need it to deploy and manage infrastructure.","title":"Infrastructure developers"},{"location":"setup/non-infrastructure-dependencies/","text":"Non-infrastructure dependencies \u00b6 To contribute as a non-infrastructure developer you will need the following: Azure command line interface (CLI) (for managing your Azure subscriptions) Docker (For building and testing images locally) postgreSQL (command-line tool for interacting with db) CleanAir python packages (install python packages) GDAL (For inserting static datasets) The instructions below are to install the dependencies system-wide, however you can follow the instructions at the end if you wish to use an anaconda environment if you want to keep it all separated from your system. Windows is not supported. However, you may use Windows Subsystem for Linux 2 and then install dependencies with conda . Azure CLI \u00b6 If you have not already installed the command line interface for Azure , please follow the procedure here to get started Or follow a simpler option Install it using on your own preferred environment with `pip install azure-cli` Docker \u00b6 Download and install Docker Desktop PostgreSQL \u00b6 PostgreSQL and PostGIS . brew install postgresql postgis GDAL \u00b6 GDAl can be installed using brew on a mac brew install gdal or any of the binaries provided for different platforms. Development tools \u00b6 The following are optional as we can run everything on docker images. However, they are recommended for development/testing and required for setting up a local copy of the database. pip install -r containers/requirements.txt CleanAir Python packages \u00b6 To run the CleanAir functionality locally (without a docker image) you can install the package with pip . For a basic install which will allow you to set up a local database run: pip install -e 'containers/cleanair[<optional-dependencies>]' Certain functionality requires optional dependencies. These can be installed by adding the following: Option keyword Functionality models CleanAir GPFlow models traffic FBProphet Trafic Models dashboards Model fitting Dashboards For getting started we recommend: pip install -e 'containers/cleanair[models, traffic, dashboard]' UATraffic (London Busyness only) \u00b6 All additional functionality related to the London Busyness project requires: pip install -e 'containers/odysseus' UrbanAir Flask API package \u00b6 pip install -e 'containers/urbanair'","title":"Non-infrastructure dependencies"},{"location":"setup/non-infrastructure-dependencies/#non-infrastructure-dependencies","text":"To contribute as a non-infrastructure developer you will need the following: Azure command line interface (CLI) (for managing your Azure subscriptions) Docker (For building and testing images locally) postgreSQL (command-line tool for interacting with db) CleanAir python packages (install python packages) GDAL (For inserting static datasets) The instructions below are to install the dependencies system-wide, however you can follow the instructions at the end if you wish to use an anaconda environment if you want to keep it all separated from your system. Windows is not supported. However, you may use Windows Subsystem for Linux 2 and then install dependencies with conda .","title":"Non-infrastructure dependencies"},{"location":"setup/non-infrastructure-dependencies/#azure-cli","text":"If you have not already installed the command line interface for Azure , please follow the procedure here to get started Or follow a simpler option Install it using on your own preferred environment with `pip install azure-cli`","title":"Azure CLI"},{"location":"setup/non-infrastructure-dependencies/#docker","text":"Download and install Docker Desktop","title":"Docker"},{"location":"setup/non-infrastructure-dependencies/#postgresql","text":"PostgreSQL and PostGIS . brew install postgresql postgis","title":"PostgreSQL"},{"location":"setup/non-infrastructure-dependencies/#gdal","text":"GDAl can be installed using brew on a mac brew install gdal or any of the binaries provided for different platforms.","title":"GDAL"},{"location":"setup/non-infrastructure-dependencies/#development-tools","text":"The following are optional as we can run everything on docker images. However, they are recommended for development/testing and required for setting up a local copy of the database. pip install -r containers/requirements.txt","title":"Development tools"},{"location":"setup/non-infrastructure-dependencies/#cleanair-python-packages","text":"To run the CleanAir functionality locally (without a docker image) you can install the package with pip . For a basic install which will allow you to set up a local database run: pip install -e 'containers/cleanair[<optional-dependencies>]' Certain functionality requires optional dependencies. These can be installed by adding the following: Option keyword Functionality models CleanAir GPFlow models traffic FBProphet Trafic Models dashboards Model fitting Dashboards For getting started we recommend: pip install -e 'containers/cleanair[models, traffic, dashboard]'","title":"CleanAir Python packages"},{"location":"setup/non-infrastructure-dependencies/#uatraffic-london-busyness-only","text":"All additional functionality related to the London Busyness project requires: pip install -e 'containers/odysseus'","title":"UATraffic (London Busyness only)"},{"location":"setup/non-infrastructure-dependencies/#urbanair-flask-api-package","text":"pip install -e 'containers/urbanair'","title":"UrbanAir Flask API package"}]}