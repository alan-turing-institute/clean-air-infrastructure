{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "import gpflow\n",
    "import pickle\n",
    "import calendar\n",
    "import tensorflow as tf\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from gpflow.utilities import print_summary\n",
    "\n",
    "gpflow.config.set_default_summary_fmt(\"notebook\")\n",
    "\n",
    "# cleanair modules for scoot\n",
    "from cleanair.scoot import (\n",
    "    sample_n,\n",
    "    ScootQuery,\n",
    "    sample_intensity,\n",
    "    plotly_results,\n",
    "    choose_kernel,\n",
    "    save_model_and_metadata\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup filepaths\n",
    "\n",
    "All data, results, figures and models are held in the `experiments/NAME` directories where `NAME` is the name of your experiment.\n",
    "\n",
    "```\n",
    "experiments/\n",
    "    NAME/\n",
    "        data/\n",
    "            normal_scoot.csv\n",
    "            lockdown_scoot.csv\n",
    "        results/\n",
    "            lockdown_SCOOT_ID.npy\n",
    "            normal_SCOOT_ID.npy\n",
    "            ...\n",
    "        models/\n",
    "            lockdown_SCOOT_ID.m5\n",
    "            normal_SCOOT_ID.m5\n",
    "            ...\n",
    "        figures/\n",
    "            lockdown_SCOOT_ID.png\n",
    "            normal_SCOOT_ID.png\n",
    "            ...\n",
    "        settings/\n",
    "            kernel_settings.json\n",
    "            scoot_settings.json\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give your experiment a useful name\n",
    "name = \"test\"\n",
    "\n",
    "# setup filepaths\n",
    "secretfile = \"../../terraform/.secrets/db_traffic.json\"\n",
    "xpfp = \"./experiments\"  # root to experiments filepaths directory\n",
    "data_dir = os.path.join(xpfp, name, \"data\")\n",
    "results_dir = os.path.join(xpfp, name, \"results\")\n",
    "models_dir = os.path.join(xpfp, name, \"models\")\n",
    "settings_dir = os.path.join(xpfp, name, \"settings\")\n",
    "\n",
    "# make directories\n",
    "Path(os.path.join(xpfp, name)).mkdir(exist_ok=True, parents=True)\n",
    "Path(data_dir).mkdir(exist_ok=True)         # input data and processed training data\n",
    "Path(results_dir).mkdir(exist_ok=True)      # predictions from model\n",
    "Path(models_dir).mkdir(exist_ok=True)       # saving model status\n",
    "Path(settings_dir).mkdir(exist_ok=True)     # for storing parameters\n",
    "\n",
    "# get the settings for kernels and scoot data\n",
    "with open(os.path.join(settings_dir, \"kernel_settings.json\")) as kernel_file:\n",
    "    kernel_settings = json.load(kernel_file)\n",
    "with open(os.path.join(settings_dir, \"scoot_settings.json\")) as scoot_file:\n",
    "    scoot_settings = json.load(scoot_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if true, all input data will be read from a local file\n",
    "read_data_from_file = True\n",
    "save_data_to_file = False\n",
    "\n",
    "# choose a start and end date for querying \"normal traffic\" period\n",
    "normal_start = \"2020-02-10 00:00:00\"\n",
    "normal_end = \"2020-02-24 00:00:00\"\n",
    "\n",
    "# choose a start and end date for querying \"lockdown traffic\" period\n",
    "lockdown_start = \"2020-03-16 00:00:00\"\n",
    "lockdown_end = \"2020-03-30 00:00:00\"\n",
    "\n",
    "# columns to analyse\n",
    "columns = [\"n_vehicles_in_interval\", \"occupancy_percentage\", \"congestion_percentage\", \"saturation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detector readings\n",
    "\n",
    "You can read scoot either from the DB or from a local file. Make sure you have set `read_data_from_file` and `save_data_to_file` correctly before running the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-04-01 12:04:48     INFO: Database connection information loaded from<_io.TextIOWrapper name='../../terraform/.secrets/db_traffic.json' mode='r' encoding='UTF-8'>\n"
    }
   ],
   "source": [
    "normal_fp = os.path.join(data_dir, \"normal_scoot.csv\")\n",
    "lockdown_fp = os.path.join(data_dir, \"lockdown_scoot.csv\")\n",
    "\n",
    "if read_data_from_file:\n",
    "    # read data from csv\n",
    "    normal_df = pd.read_csv(normal_fp)\n",
    "    lockdown_df = pd.read_csv(lockdown_fp)\n",
    "else:\n",
    "    # create an object for querying from DB\n",
    "    SQ = ScootQuery(secretfile=secretfile)\n",
    "    # read the data from DB\n",
    "    normal_df = SQ.get_all_readings(\n",
    "        start_datetime=normal_start,\n",
    "        end_datetime=normal_end\n",
    "    )\n",
    "    lockdown_df = SQ.get_all_readings(\n",
    "        start_datetime=lockdown_start,\n",
    "        end_datetime=lockdown_end\n",
    "    )\n",
    "    # save the data to csv if required\n",
    "    if save_data_to_file:\n",
    "        normal_df.to_csv(normal_fp)\n",
    "        lockdown_df.to_csv(lockdown_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "    - Convert Datetime to epoch\n",
    "    - Add normalised/standardised columns\n",
    "    - Get a dataframe for only a subset of sensors and for given time range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(x):\n",
    "    \"\"\"Standardize all columns individually\"\"\"\n",
    "    return (x - np.mean(x, axis=0)) / np.std(x, axis=0)\n",
    "\n",
    "def denormalise(x, wrt_y):\n",
    "    \"\"\"Denormalize x given the original data it was standardized to\"\"\"\n",
    "    return ( x * np.std(wrt_y, axis=0) ) + np.mean(wrt_y, axis=0)\n",
    "\n",
    "def clean_and_normalise_df(df: pd.DataFrame):\n",
    "    \"\"\"Normalise lat, lon, epoch.\"\"\"\n",
    "    df['measurement_start_utc'] = pd.to_datetime(df['measurement_start_utc'])\n",
    "    df['epoch'] = df['measurement_start_utc'].astype('int64')//1e9 #convert to epoch\n",
    "    df['epoch_norm'] = normalise(df['epoch'])\n",
    "    df['lat_norm'] = normalise(df['lat'])\n",
    "    df['lon_norm'] = normalise(df['lon'])\n",
    "    return df\n",
    "\n",
    "def filter_df(df: pd.DataFrame, detector_list: list, start: str, end: str):\n",
    "    \"\"\"\n",
    "    Return a dataframe that only contains sensors in the list\n",
    "    and only contains observations between the start and end datetime.\n",
    "    \"\"\"\n",
    "    return df.loc[\n",
    "        (df['detector_id'].isin(detector_list)) &\n",
    "        (df[\"measurement_start_utc\"] >= start) &\n",
    "        (df[\"measurement_start_utc\"] < end)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        detector_id       lon        lat measurement_start_utc  \\\n67954     N00/002g1 -0.102058  51.513892   2020-02-10 09:00:00   \n1055111   N00/002e1 -0.107637  51.514252   2020-02-14 15:00:00   \n98340     N00/002e1 -0.107637  51.514252   2020-02-10 13:00:00   \n\n        measurement_end_utc  n_vehicles_in_interval  occupancy_percentage  \\\n67954   2020-02-10 10:00:00                     238             41.052026   \n1055111 2020-02-14 16:00:00                     248              7.938712   \n98340   2020-02-10 14:00:00                     252              8.434961   \n\n         congestion_percentage  saturation         epoch  epoch_norm  \\\n67954                25.050847  197.338983  1.581325e+09   -1.676775   \n1055111               0.000000   76.637931  1.581692e+09   -0.686253   \n98340                 0.000000  102.900000  1.581340e+09   -1.637931   \n\n         lat_norm  lon_norm  \n67954    0.136659  0.156069  \n1055111  0.142769  0.114531  \n98340    0.142769  0.114531  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>detector_id</th>\n      <th>lon</th>\n      <th>lat</th>\n      <th>measurement_start_utc</th>\n      <th>measurement_end_utc</th>\n      <th>n_vehicles_in_interval</th>\n      <th>occupancy_percentage</th>\n      <th>congestion_percentage</th>\n      <th>saturation</th>\n      <th>epoch</th>\n      <th>epoch_norm</th>\n      <th>lat_norm</th>\n      <th>lon_norm</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>67954</th>\n      <td>N00/002g1</td>\n      <td>-0.102058</td>\n      <td>51.513892</td>\n      <td>2020-02-10 09:00:00</td>\n      <td>2020-02-10 10:00:00</td>\n      <td>238</td>\n      <td>41.052026</td>\n      <td>25.050847</td>\n      <td>197.338983</td>\n      <td>1.581325e+09</td>\n      <td>-1.676775</td>\n      <td>0.136659</td>\n      <td>0.156069</td>\n    </tr>\n    <tr>\n      <th>1055111</th>\n      <td>N00/002e1</td>\n      <td>-0.107637</td>\n      <td>51.514252</td>\n      <td>2020-02-14 15:00:00</td>\n      <td>2020-02-14 16:00:00</td>\n      <td>248</td>\n      <td>7.938712</td>\n      <td>0.000000</td>\n      <td>76.637931</td>\n      <td>1.581692e+09</td>\n      <td>-0.686253</td>\n      <td>0.142769</td>\n      <td>0.114531</td>\n    </tr>\n    <tr>\n      <th>98340</th>\n      <td>N00/002e1</td>\n      <td>-0.107637</td>\n      <td>51.514252</td>\n      <td>2020-02-10 13:00:00</td>\n      <td>2020-02-10 14:00:00</td>\n      <td>252</td>\n      <td>8.434961</td>\n      <td>0.000000</td>\n      <td>102.900000</td>\n      <td>1.581340e+09</td>\n      <td>-1.637931</td>\n      <td>0.142769</td>\n      <td>0.114531</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "# choose a start and end datetime to filter by\n",
    "start_normal_interval = \"2020-02-10 00:00:00\"\n",
    "end_normal_interval = \"2020-02-17 00:00:00\"\n",
    "start_lockdown_interval = \"2020-03-16 00:00:00\"\n",
    "end_lockdown_interval = \"2020-03-23 00:00:00\"\n",
    "\n",
    "# get list of detectors from json file to filter by\n",
    "detector_list = scoot_settings[\"scoot_ids\"]\n",
    "# detector_list = list(np.unique(normal_df['detector_id']))   # all scoot detectors\n",
    "\n",
    "# clean data and normalise\n",
    "# TODO: IMPORTANT - normalisation should be same for normal and lockdown periods.\n",
    "normal_df = clean_and_normalise_df(normal_df)\n",
    "lockdown_df = clean_and_normalise_df(lockdown_df)\n",
    "\n",
    "# filter normal and lockdown dataframes by interval dates and same detectors\n",
    "normal_interval_df = filter_df(normal_df, detector_list, start_normal_interval, end_normal_interval)\n",
    "lockdown_interval_df = filter_df(lockdown_df, detector_list, start_lockdown_interval, end_lockdown_interval)\n",
    "\n",
    "normal_interval_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split up into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X(df):\n",
    "    return np.array(df[['epoch_norm', 'lon_norm', 'lat_norm']])\n",
    "\n",
    "def get_Y(df):\n",
    "    return np.array(df[['n_vehicles_in_interval']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_group = normal_interval_df.groupby(\"detector_id\")\n",
    "lockdown_group = lockdown_interval_df.groupby(\"detector_id\")\n",
    "\n",
    "# list of dfs for all sensors\n",
    "normal_df_list = [normal_group.get_group(id) for id in detector_list] \n",
    "lockdown_df_list = [lockdown_group.get_group(id) for id in detector_list]\n",
    "\n",
    "# get list of numpy arrays for each dataframe\n",
    "X_arr = [get_X(df) for df in normal_df_list] # |Number of scoot sensors| x N_i x D\n",
    "Y_arr = [get_Y(df) for df in normal_df_list] # |Number of scoot sensors| x N_i x 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit LGCP model for each sensor\n",
    "\n",
    "The input $X$ is time epoch, lat, lon and output $Y$ is the integer `n_vehicles_in_interval`.\n",
    "\n",
    "NOTE for 2 days of scoot data there are approx 400000 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set random seed\n",
    "gpflow.config.set_default_float(np.float64)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimization functions - train the model for the given epochs\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "def optimization_step(model: gpflow.models.SVGP, X, Y):\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        obj = -model.elbo(X, Y)\n",
    "        grads = tape.gradient(obj, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "def simple_training_loop(X, Y, model: gpflow.models.SVGP, epochs: int = 1, logging_epoch_freq: int = 10, num_batches_per_epoch: int = 10):\n",
    "    tf_optimization_step = tf.function(optimization_step)\n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(num_batches_per_epoch):\n",
    "            tf_optimization_step(model, X, Y)\n",
    "\n",
    "        epoch_id = epoch + 1\n",
    "        if epoch_id % logging_epoch_freq == 0:\n",
    "            tf.print(f\"Epoch {epoch_id}: ELBO (train) {model.elbo(X,Y)}\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Given the data and the specific sensor this function optimise the ELBO and plot the results \n",
    "def train_sensor_model(scoot_id, X_arr, Y_arr, kernelsettings, epochs = 100, logging_epoch_freq = 10, plot=True):\n",
    "    \n",
    "    ## To remove newaxis when more features\n",
    "    num_features = X_arr[scoot_id][:,0][:,np.newaxis].shape[0]\n",
    "    \n",
    "    X = tf.convert_to_tensor(X_arr[scoot_id][:,0][:,np.newaxis])\n",
    "    Y = tf.convert_to_tensor(Y_arr[scoot_id].astype(np.float64))\n",
    "    \n",
    "    ## To pass it as a function arg\n",
    "    k = choose_kernel(kernelsettings)\n",
    "#     k = gpflow.kernels.RBF() * gpflow.kernels.Periodic(0.1)\n",
    "    \n",
    "    lik = gpflow.likelihoods.Poisson()\n",
    "    \n",
    "    ## Add code for inducing inputs - Needed when we run on the full data\n",
    "    model = gpflow.models.SVGP(kernel = k, likelihood=lik, inducing_variable=X)\n",
    "    \n",
    "    ## Uncomment to see which variables are training and those that are not\n",
    "    #print_summary(model)\n",
    "    \n",
    "    simple_training_loop(X, Y, model, epochs = epochs, \n",
    "                         logging_epoch_freq = logging_epoch_freq)\n",
    "\n",
    "    return model,X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS BUGGY\n",
    "## Computes percentage cover (see Virginia's pdf for details)\n",
    "def percentage_coverage(model,test_inputs,Ytest,quantile:int = 0.95, num_samples:int = 10,num_pertubations: int = 100):\n",
    "    # Number of times total counts were within 90th percentile\n",
    "    coverage_events = 0\n",
    "    \n",
    "    # Loop over pertubations\n",
    "    for i in range(num_pertubations):\n",
    "\n",
    "        # Change seed\n",
    "        np.random.seed(i)\n",
    "        \n",
    "        # Sample from latent function (intensity)\n",
    "        intensity_sample = np.exp(model.predict_f_samples(test_inputs,num_samples))\n",
    "        # Compute emprical distribution of counts\n",
    "        empirical_count_distribution = np.random.poisson(intensity_sample)\n",
    "        \n",
    "        # Total number of actual counts\n",
    "        total_counts = np.sum(Ytest)\n",
    "       \n",
    "        # Compute upper and lower quantiles from the empirical distribution of counts\n",
    "        upper_q = np.quantile(np.sum(samples[:,:,0],axis=1),quantile)\n",
    "        lower_q = np.quantile(np.sum(samples[:,:,0],axis=1),1-quantile)\n",
    "    \n",
    "        # Add 1 - if total counts are within quantile, 0 - otherwise\n",
    "        coverage_events += int((total_counts < upper_q) & (total_counts > lower_q))\n",
    "        binary = int((total_counts < upper_q) & (total_counts > lower_q)) # this is kept for debugging (remove afterwards)\n",
    "\n",
    "    return empirical_count_distribution, binary, total_counts, upper_q, lower_q # this is kept for debugging (remove afterwards)\n",
    "    return coverage_events/num_pertubations # this should be the output after debugging\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run entire training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "logging_epoch_freq = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Using product of periodic and rbf kernels\nHyperparameters of periodic\n{'period': 0.1}\nHyperparameters of rbf\n{}\nWARNING:tensorflow:Entity <bound method Dispatcher.dispatch_iter of <dispatched prior_kl>> appears to be a generator function. It will not be converted by AutoGraph.\n2020-04-01 13:14:18 WARNING: Entity <bound method Dispatcher.dispatch_iter of <dispatched prior_kl>> appears to be a generator function. It will not be converted by AutoGraph.\nWARNING: Entity <bound method Dispatcher.dispatch_iter of <dispatched prior_kl>> appears to be a generator function. It will not be converted by AutoGraph.\nWARNING:tensorflow:Entity <bound method Dispatcher.dispatch_iter of <dispatched conditional>> appears to be a generator function. It will not be converted by AutoGraph.\n2020-04-01 13:14:19 WARNING: Entity <bound method Dispatcher.dispatch_iter of <dispatched conditional>> appears to be a generator function. It will not be converted by AutoGraph.\nWARNING: Entity <bound method Dispatcher.dispatch_iter of <dispatched conditional>> appears to be a generator function. It will not be converted by AutoGraph.\nWARNING:tensorflow:Entity <bound method Dispatcher.dispatch_iter of <dispatched Kuu>> appears to be a generator function. It will not be converted by AutoGraph.\n2020-04-01 13:14:19 WARNING: Entity <bound method Dispatcher.dispatch_iter of <dispatched Kuu>> appears to be a generator function. It will not be converted by AutoGraph.\nWARNING: Entity <bound method Dispatcher.dispatch_iter of <dispatched Kuu>> appears to be a generator function. It will not be converted by AutoGraph.\nWARNING:tensorflow:Entity <bound method Dispatcher.dispatch_iter of <dispatched Kuf>> appears to be a generator function. It will not be converted by AutoGraph.\n2020-04-01 13:14:19 WARNING: Entity <bound method Dispatcher.dispatch_iter of <dispatched Kuf>> appears to be a generator function. It will not be converted by AutoGraph.\nWARNING: Entity <bound method Dispatcher.dispatch_iter of <dispatched Kuf>> appears to be a generator function. It will not be converted by AutoGraph.\nWARNING:tensorflow:AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fd71be07550>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fd6fe41e810>, <gast.gast.Return object at 0x7fd6fe41e150>]\n2020-04-01 13:14:21 WARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fd71be07550>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fd6fe41e810>, <gast.gast.Return object at 0x7fd6fe41e150>]\nWARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fd71be07550>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fd6fe41e810>, <gast.gast.Return object at 0x7fd6fe41e150>]\nWARNING:tensorflow:AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fd71be07550>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fd6fe51fa10>, <gast.gast.Return object at 0x7fd6fe51fa90>]\n2020-04-01 13:14:21 WARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fd71be07550>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fd6fe51fa10>, <gast.gast.Return object at 0x7fd6fe51fa90>]\nWARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fd71be07550>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fd6fe51fa10>, <gast.gast.Return object at 0x7fd6fe51fa90>]\n"
    }
   ],
   "source": [
    "model0,Xtest0 = train_sensor_model(0, X_arr, Y_arr, kernel_settings, epochs, logging_epoch_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1,Xtest1 = train_sensor_model(1, X_arr, Y_arr, kernel_settings, epochs, logging_epoch_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Saving data todata/models/N00_002e1/10Feb_23Mar\ndata/models/N00_002e1/10Feb_23Mar/Y.npy\n"
    }
   ],
   "source": [
    "save_model_to_file(model0, name, detector_id, models_dir)\n",
    "save_model_and_metadata(detector_list[index], model0, X_arr[index], Y_arr[index], start_normal_interval, end_lockdown_interval, kernel_settings, scoot_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cleanair",
   "language": "python",
   "name": "cleanair"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}