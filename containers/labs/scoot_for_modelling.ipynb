{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling SCOOT\n",
    "\n",
    "Load the data (clean + normalise) then run an SVGP on each sensor individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "import gpflow\n",
    "import pickle\n",
    "import calendar\n",
    "import tensorflow as tf\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from gpflow.utilities import print_summary\n",
    "\n",
    "gpflow.config.set_default_summary_fmt(\"notebook\")\n",
    "\n",
    "# cleanair modules for scoot\n",
    "from cleanair.scoot import (\n",
    "    ScootQuery,\n",
    "    parse_kernel,\n",
    "    save_model_to_file,\n",
    "    save_processed_data_to_file\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup filepaths\n",
    "\n",
    "All data, results, figures and models are held in the `EXPERIMENT_DIR/NAME` directories where `NAME` is the name of your experiment.\n",
    "\n",
    "```\n",
    "EXPERIMENT_DIR/\n",
    "    NAME/\n",
    "        data/\n",
    "            normal_scoot.csv\n",
    "            lockdown_scoot.csv\n",
    "            SCOOT_ID.npy\n",
    "        results/\n",
    "            KERNEL_ID\n",
    "                lockdown_SCOOT_ID.npy\n",
    "                normal_SCOOT_ID.npy\n",
    "            ...\n",
    "        models/\n",
    "            KERNEL_ID\n",
    "                lockdown_SCOOT_ID.m5\n",
    "                normal_SCOOT_ID.m5\n",
    "                ...\n",
    "        figures/\n",
    "            KERNEL_ID\n",
    "                lockdown_SCOOT_ID.png\n",
    "                normal_SCOOT_ID.png\n",
    "                ...\n",
    "        settings/\n",
    "            kernel_settings.json\n",
    "            scoot_settings.json\n",
    "```\n",
    " Here I'm assuming each scoot detector is trained independently. If this changes we may need to change file structure (should be ok through use of `cleanair.scoot.util` helper functions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give your experiment a useful name\n",
    "name = \"test\"\n",
    "\n",
    "# setup filepaths\n",
    "user_settings_fp = os.path.join(\"..\", \"..\", \"terraform\", \".secrets\", \"user_settings.json\")\n",
    "with open(user_settings_fp) as json_file:\n",
    "    user_settings = json.load(json_file)\n",
    "secretfile = user_settings[\"secretfile\"]\n",
    "xpfp = user_settings[\"experiment_dir\"]  # root to experiments filepaths directory\n",
    "data_dir = os.path.join(xpfp, name, \"data\")\n",
    "results_dir = os.path.join(xpfp, name, \"results\")\n",
    "models_dir = os.path.join(xpfp, name, \"models\")\n",
    "settings_dir = os.path.join(xpfp, name, \"settings\")\n",
    "\n",
    "# make directories\n",
    "Path(os.path.join(xpfp, name)).mkdir(exist_ok=True, parents=True)\n",
    "Path(data_dir).mkdir(exist_ok=True)         # input data and processed training data\n",
    "Path(results_dir).mkdir(exist_ok=True)      # predictions from model\n",
    "Path(models_dir).mkdir(exist_ok=True)       # saving model status\n",
    "Path(settings_dir).mkdir(exist_ok=True)     # for storing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if true, all input data will be read from a local file\n",
    "read_data_from_file = True\n",
    "save_data_to_file = False\n",
    "\n",
    "# choose a start and end date for querying \"normal traffic\" period\n",
    "normal_start = \"2020-02-10 00:00:00\"\n",
    "normal_end = \"2020-02-24 00:00:00\"\n",
    "\n",
    "# choose a start and end date for querying \"lockdown traffic\" period\n",
    "lockdown_start = \"2020-03-16 00:00:00\"\n",
    "lockdown_end = \"2020-03-30 00:00:00\"\n",
    "\n",
    "# columns to analyse\n",
    "columns = [\"n_vehicles_in_interval\"]\n",
    "\n",
    "# seeds\n",
    "gpflow.config.set_default_float(np.float64)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detector readings\n",
    "\n",
    "You can read scoot either from the DB or from a local file. Make sure you have set `read_data_from_file` and `save_data_to_file` correctly before running the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_fp = os.path.join(data_dir, \"normal_scoot.csv\")\n",
    "lockdown_fp = os.path.join(data_dir, \"lockdown_scoot.csv\")\n",
    "\n",
    "if read_data_from_file:\n",
    "    # read data from csv\n",
    "    normal_df = pd.read_csv(normal_fp)\n",
    "    lockdown_df = pd.read_csv(lockdown_fp)\n",
    "else:\n",
    "    # create an object for querying from DB\n",
    "    SQ = ScootQuery(secretfile=secretfile)\n",
    "    # read the data from DB\n",
    "    normal_df = SQ.get_all_readings(\n",
    "        start_datetime=normal_start,\n",
    "        end_datetime=normal_end\n",
    "    )\n",
    "    lockdown_df = SQ.get_all_readings(\n",
    "        start_datetime=lockdown_start,\n",
    "        end_datetime=lockdown_end\n",
    "    )\n",
    "    # save the data to csv if required\n",
    "    if save_data_to_file:\n",
    "        normal_df.to_csv(normal_fp)\n",
    "        lockdown_df.to_csv(lockdown_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "    - Convert Datetime to epoch\n",
    "    - Add normalised/standardised columns\n",
    "    - Get a dataframe for only a subset of sensors and for given time range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(x):\n",
    "    \"\"\"Standardize all columns individually\"\"\"\n",
    "    return (x - np.mean(x, axis=0)) / np.std(x, axis=0)\n",
    "\n",
    "def denormalise(x, wrt_y):\n",
    "    \"\"\"Denormalize x given the original data it was standardized to\"\"\"\n",
    "    return ( x * np.std(wrt_y, axis=0) ) + np.mean(wrt_y, axis=0)\n",
    "\n",
    "def clean_and_normalise_df(df: pd.DataFrame):\n",
    "    \"\"\"Normalise lat, lon, epoch.\"\"\"\n",
    "    df['measurement_start_utc'] = pd.to_datetime(df['measurement_start_utc'])\n",
    "    df['epoch'] = df['measurement_start_utc'].astype('int64')//1e9 #convert to epoch\n",
    "    df['epoch_norm'] = normalise(df['epoch'])\n",
    "    df['lat_norm'] = normalise(df['lat'])\n",
    "    df['lon_norm'] = normalise(df['lon'])\n",
    "    return df\n",
    "\n",
    "def filter_df(df: pd.DataFrame, detector_list: list, start: str, end: str):\n",
    "    \"\"\"\n",
    "    Return a dataframe that only contains sensors in the list\n",
    "    and only contains observations between the start and end datetime.\n",
    "    \"\"\"\n",
    "    return df.loc[\n",
    "        (df['detector_id'].isin(detector_list)) &\n",
    "        (df[\"measurement_start_utc\"] >= start) &\n",
    "        (df[\"measurement_start_utc\"] < end)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "         Unnamed: 0 detector_id       lon        lat measurement_start_utc  \\\n218885       218885   N00/002e1 -0.107637  51.514252   2020-02-11 03:00:00   \n208930       208930   N00/002e1 -0.107637  51.514252   2020-02-11 02:00:00   \n1313309     1313309   N00/002g1 -0.102058  51.513892   2020-02-15 17:00:00   \n\n         measurement_end_utc  n_vehicles_in_interval  occupancy_percentage  \\\n218885   2020-02-11 04:00:00                      88              5.343254   \n208930   2020-02-11 03:00:00                      92              2.311012   \n1313309  2020-02-15 18:00:00                     260             15.098009   \n\n         congestion_percentage  saturation         epoch  epoch_norm  \\\n218885                2.133333   22.083333  1.581390e+09   -1.484684   \n208930                0.000000   20.850000  1.581386e+09   -1.495099   \n1313309               6.103448   95.310345  1.581786e+09   -0.339040   \n\n         lat_norm  lon_norm  \n218885   0.143802  0.113706  \n208930   0.143802  0.113706  \n1313309  0.137691  0.155247  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>detector_id</th>\n      <th>lon</th>\n      <th>lat</th>\n      <th>measurement_start_utc</th>\n      <th>measurement_end_utc</th>\n      <th>n_vehicles_in_interval</th>\n      <th>occupancy_percentage</th>\n      <th>congestion_percentage</th>\n      <th>saturation</th>\n      <th>epoch</th>\n      <th>epoch_norm</th>\n      <th>lat_norm</th>\n      <th>lon_norm</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>218885</th>\n      <td>218885</td>\n      <td>N00/002e1</td>\n      <td>-0.107637</td>\n      <td>51.514252</td>\n      <td>2020-02-11 03:00:00</td>\n      <td>2020-02-11 04:00:00</td>\n      <td>88</td>\n      <td>5.343254</td>\n      <td>2.133333</td>\n      <td>22.083333</td>\n      <td>1.581390e+09</td>\n      <td>-1.484684</td>\n      <td>0.143802</td>\n      <td>0.113706</td>\n    </tr>\n    <tr>\n      <th>208930</th>\n      <td>208930</td>\n      <td>N00/002e1</td>\n      <td>-0.107637</td>\n      <td>51.514252</td>\n      <td>2020-02-11 02:00:00</td>\n      <td>2020-02-11 03:00:00</td>\n      <td>92</td>\n      <td>2.311012</td>\n      <td>0.000000</td>\n      <td>20.850000</td>\n      <td>1.581386e+09</td>\n      <td>-1.495099</td>\n      <td>0.143802</td>\n      <td>0.113706</td>\n    </tr>\n    <tr>\n      <th>1313309</th>\n      <td>1313309</td>\n      <td>N00/002g1</td>\n      <td>-0.102058</td>\n      <td>51.513892</td>\n      <td>2020-02-15 17:00:00</td>\n      <td>2020-02-15 18:00:00</td>\n      <td>260</td>\n      <td>15.098009</td>\n      <td>6.103448</td>\n      <td>95.310345</td>\n      <td>1.581786e+09</td>\n      <td>-0.339040</td>\n      <td>0.137691</td>\n      <td>0.155247</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# choose a start and end datetime to filter by\n",
    "start_normal_interval = \"2020-02-10 00:00:00\"\n",
    "end_normal_interval = \"2020-02-17 00:00:00\"\n",
    "start_lockdown_interval = \"2020-03-16 00:00:00\"\n",
    "end_lockdown_interval = \"2020-03-23 00:00:00\"\n",
    "\n",
    "# get list of detectors from json file to filter by\n",
    "detector_list = [\"N00/002e1\",\"N00/002g1\",\"N13/016a1\"]\n",
    "# detector_list = list(np.unique(normal_df['detector_id']))   # all scoot detectors\n",
    "\n",
    "# clean data and normalise\n",
    "# TODO: IMPORTANT - normalisation should be same for normal and lockdown periods.\n",
    "normal_df = clean_and_normalise_df(normal_df)\n",
    "lockdown_df = clean_and_normalise_df(lockdown_df)\n",
    "\n",
    "# filter normal and lockdown dataframes by interval dates and same detectors\n",
    "normal_interval_df = filter_df(normal_df, detector_list, start_normal_interval, end_normal_interval)\n",
    "lockdown_interval_df = filter_df(lockdown_df, detector_list, start_lockdown_interval, end_lockdown_interval)\n",
    "\n",
    "normal_interval_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save kernel and data settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split up into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X(df):\n",
    "    return np.array(df[['epoch_norm', 'lon_norm', 'lat_norm']])\n",
    "\n",
    "def get_Y(df):\n",
    "    return np.array(df[['n_vehicles_in_interval']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_group = normal_interval_df.groupby(\"detector_id\")\n",
    "lockdown_group = lockdown_interval_df.groupby(\"detector_id\")\n",
    "\n",
    "# list of dfs for all sensors\n",
    "normal_df_list = [normal_group.get_group(id) for id in detector_list] \n",
    "lockdown_df_list = [lockdown_group.get_group(id) for id in detector_list]\n",
    "\n",
    "# get list of numpy arrays for each dataframe\n",
    "X_arr = [get_X(df) for df in normal_df_list] # |Number of scoot sensors| x N_i x D\n",
    "Y_arr = [get_Y(df) for df in normal_df_list] # |Number of scoot sensors| x N_i x 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit LGCP model for each sensor\n",
    "\n",
    "The input $X$ is time epoch, lat, lon and output $Y$ is the integer `n_vehicles_in_interval`.\n",
    "\n",
    "NOTE for 2 days of scoot data there are approx 400000 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimization functions - train the model for the given epochs\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "def optimization_step(model: gpflow.models.SVGP, X, Y):\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        obj = -model.elbo(X, Y)\n",
    "        grads = tape.gradient(obj, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "def simple_training_loop(X, Y, model: gpflow.models.SVGP, epochs: int = 1, logging_epoch_freq: int = 10, num_batches_per_epoch: int = 10):\n",
    "    tf_optimization_step = tf.function(optimization_step)\n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(num_batches_per_epoch):\n",
    "            tf_optimization_step(model, X, Y)\n",
    "\n",
    "        epoch_id = epoch + 1\n",
    "        if epoch_id % logging_epoch_freq == 0:\n",
    "            tf.print(f\"Epoch {epoch_id}: ELBO (train) {model.elbo(X,Y)}\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Given the data and the specific sensor this function optimise the ELBO and plot the results \n",
    "def train_sensor_model(X, Y, kernel, epochs = 100, logging_epoch_freq = 10, M=10, inducing_point_method=\"random\"):\n",
    "    \n",
    "    ## To remove newaxis when more features\n",
    "    num_features = X[:,0][:,np.newaxis].shape[0]\n",
    "    \n",
    "    X = tf.convert_to_tensor(X[:,0][:,np.newaxis])\n",
    "    Y = tf.convert_to_tensor(Y.astype(np.float64))\n",
    "\n",
    "    # ToDo : number of rows\n",
    "    if M == X.shape[0]:\n",
    "        ind_points = X\n",
    "    elif inducing_point_method == \"random\":\n",
    "        # randomly select \n",
    "        ind_points = tf.random.shuffle(X)[:M]\n",
    "    else:\n",
    "        # select of regular grid\n",
    "        ind_points = tf.expand_dims(\n",
    "            tf.linspace(np.min(X_arr[0][:,0]), np.max(X_arr[0][:,0]), M),1\n",
    "        )\n",
    "    \n",
    "    lik = gpflow.likelihoods.Poisson()\n",
    "    \n",
    "    ## Add code for inducing inputs - Needed when we run on the full data\n",
    "    model = gpflow.models.SVGP(kernel=kernel, likelihood=lik, inducing_variable=ind_points)\n",
    "    \n",
    "    ## Uncomment to see which variables are training and those that are not\n",
    "    #print_summary(model)\n",
    "    \n",
    "    simple_training_loop(X, Y, model, epochs = epochs, \n",
    "                         logging_epoch_freq = logging_epoch_freq)\n",
    "\n",
    "    return model,X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoot_settings = dict(\n",
    "  scoot_ids=detector_list,\n",
    "  lockdown_start=start_lockdown_interval,\n",
    "  lockdown_end=end_lockdown_interval,\n",
    "  normal_start=start_normal_interval,\n",
    "  normal_end=end_normal_interval,\n",
    "  columns=columns,\n",
    ")\n",
    "\n",
    "# periodic with 0.5, lengthscale ...\n",
    "# periodic with rbf with params from virgi\n",
    "# periodic x matern32/12 shouldn't work\n",
    "# periodic with matern52 play\n",
    "# periodic with 0.5 + rbf\n",
    "kernel_settings = {\n",
    "    \"periodicXrbf\": [           # periodic * rbf\n",
    "        {\n",
    "            \"name\":\"periodic\",\n",
    "            \"hyperparameters\": {\n",
    "                \"period\":0.5\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\":\"rbf\",\n",
    "            \"hyperparameters\":{}\n",
    "        }\n",
    "    ],\n",
    "    \"periodicXmatern52\": [      # period * matern52\n",
    "        {\n",
    "            \"name\":\"periodic\",\n",
    "            \"hyperparameters\": {\n",
    "                \"period\":0.5\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"matern52\",\n",
    "            \"hyperparameters\": {}\n",
    "        }\n",
    "    ],\n",
    "    \"periodic\": {               # periodic with hand tuned params\n",
    "        \"name\": \"periodic\",\n",
    "        \"hyperparameters\": {\n",
    "            \"period\": 0.5,\n",
    "            \"lengthscale\": 0.7,\n",
    "            \"variance\": 4.5\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# get the settings for kernels and scoot data\n",
    "with open(os.path.join(settings_dir, \"kernel_settings.json\"), \"w\") as kernel_file:\n",
    "    json.dump(kernel_settings, kernel_file)\n",
    "with open(os.path.join(settings_dir, \"scoot_settings.json\"), \"w\") as scoot_file:\n",
    "    json.dump(scoot_settings, scoot_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run entire training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-1114.222524808013\nEpoch 700: ELBO (train) -1096.6244560889907\nEpoch 800: ELBO (train) -1080.5037720582284\nEpoch 900: ELBO (train) -1074.7057133982923\nEpoch 1000: ELBO (train) -1060.8537782037456\nEpoch 1100: ELBO (train) -1055.906339869413\nEpoch 1200: ELBO (train) -1052.235789714821\nEpoch 1300: ELBO (train) -1049.0308810154027\nEpoch 1400: ELBO (train) -1047.952111351854\nEpoch 1500: ELBO (train) -1045.7961583696147\nEpoch 1600: ELBO (train) -1045.7958456054864\nEpoch 1700: ELBO (train) -1045.2545362489527\nEpoch 1800: ELBO (train) -1043.9267746626954\nEpoch 1900: ELBO (train) -1044.046053161237\nEpoch 2000: ELBO (train) -1042.6598736867759\n*\nperiodic\n{'period': 0.5}\nmatern52\n{}\n\nWARNING:tensorflow:AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea7569dbd0>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea4376b890>, <gast.gast.Return object at 0x7fea4376b190>]\n2020-04-02 14:12:45 WARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea7569dbd0>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea4376b890>, <gast.gast.Return object at 0x7fea4376b190>]\nWARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea7569dbd0>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea4376b890>, <gast.gast.Return object at 0x7fea4376b190>]\nWARNING:tensorflow:AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea7569dbd0>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea75777550>, <gast.gast.Return object at 0x7fea75777bd0>]\n2020-04-02 14:12:45 WARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea7569dbd0>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea75777550>, <gast.gast.Return object at 0x7fea75777bd0>]\nWARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea7569dbd0>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea75777550>, <gast.gast.Return object at 0x7fea75777bd0>]\nEpoch 100: ELBO (train) -2478.894456273906\nEpoch 200: ELBO (train) -1547.1682377170875\nEpoch 300: ELBO (train) -1440.4375537232067\nEpoch 400: ELBO (train) -1394.0921422345245\nEpoch 500: ELBO (train) -1365.3179702192024\nEpoch 600: ELBO (train) -1348.2473323668\nEpoch 700: ELBO (train) -1337.023311685791\nEpoch 800: ELBO (train) -1328.8576776910502\nEpoch 900: ELBO (train) -1323.5652662635891\nEpoch 1000: ELBO (train) -1319.516496631906\nEpoch 1100: ELBO (train) -1316.6225101954062\nEpoch 1200: ELBO (train) -1314.2540752986256\nEpoch 1300: ELBO (train) -1311.6371695995326\nEpoch 1400: ELBO (train) -1305.2323822077685\nEpoch 1500: ELBO (train) -1302.9121331410126\nEpoch 1600: ELBO (train) -1301.8772796104304\nEpoch 1700: ELBO (train) -1301.2826137475115\nEpoch 1800: ELBO (train) -1300.833975623846\nEpoch 1900: ELBO (train) -1300.4394685829795\nEpoch 2000: ELBO (train) -1300.073797331314\nperiodic\n{'period': 0.5, 'lengthscale': 0.7, 'variance': 4.5}\nWARNING:tensorflow:AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea53baea50>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea532f2a90>, <gast.gast.Return object at 0x7fea532f2850>]\n2020-04-02 14:13:01 WARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea53baea50>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea532f2a90>, <gast.gast.Return object at 0x7fea532f2850>]\nWARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea53baea50>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea532f2a90>, <gast.gast.Return object at 0x7fea532f2850>]\nWARNING:tensorflow:AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea53baea50>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea764f0950>, <gast.gast.Return object at 0x7fea764f06d0>]\n2020-04-02 14:13:01 WARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea53baea50>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea764f0950>, <gast.gast.Return object at 0x7fea764f06d0>]\nWARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea53baea50>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea764f0950>, <gast.gast.Return object at 0x7fea764f06d0>]\nEpoch 100: ELBO (train) -3554.5026661182674\nEpoch 200: ELBO (train) -1496.6118972166023\nEpoch 300: ELBO (train) -1350.4308802895023\nEpoch 400: ELBO (train) -1333.4516551434194\nEpoch 500: ELBO (train) -1287.7351651927036\nEpoch 600: ELBO (train) -1267.133979506652\nEpoch 700: ELBO (train) -1257.3617767293101\nEpoch 800: ELBO (train) -1207.0694045729147\nEpoch 900: ELBO (train) -1183.5891860721215\nEpoch 1000: ELBO (train) -1168.082772378268\nEpoch 1100: ELBO (train) -1162.944996692143\nEpoch 1200: ELBO (train) -1159.1596593260788\nEpoch 1300: ELBO (train) -1156.4306337615874\nEpoch 1400: ELBO (train) -1154.8561544191953\nEpoch 1500: ELBO (train) -1153.6433300741155\nEpoch 1600: ELBO (train) -1151.559733546395\nEpoch 1700: ELBO (train) -1153.7419730824251\nEpoch 1800: ELBO (train) -1146.0765088412722\nEpoch 1900: ELBO (train) -1144.4613046473532\nEpoch 2000: ELBO (train) -1143.6888797329516\n*\nperiodic\n{'period': 0.5}\nrbf\n{}\n\nWARNING:tensorflow:AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea532dda50>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea53c09390>, <gast.gast.Return object at 0x7fea53c09410>]\n2020-04-02 14:13:14 WARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea532dda50>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea53c09390>, <gast.gast.Return object at 0x7fea53c09410>]\nWARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea532dda50>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea53c09390>, <gast.gast.Return object at 0x7fea53c09410>]\nWARNING:tensorflow:AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea532dda50>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea5333add0>, <gast.gast.Return object at 0x7fea5333ae50>]\n2020-04-02 14:13:14 WARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea532dda50>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea5333add0>, <gast.gast.Return object at 0x7fea5333ae50>]\nWARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea532dda50>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea5333add0>, <gast.gast.Return object at 0x7fea5333ae50>]\nEpoch 100: ELBO (train) -5413.88607555642\nEpoch 200: ELBO (train) -2983.3164815214705\nEpoch 300: ELBO (train) -2521.7872328979943\nEpoch 400: ELBO (train) -2426.170874318621\nEpoch 500: ELBO (train) -2374.881778386229\nEpoch 600: ELBO (train) -2303.33677825347\nEpoch 700: ELBO (train) -2283.6068658968743\nEpoch 800: ELBO (train) -2253.841649288285\nEpoch 900: ELBO (train) -2220.713947042212\nEpoch 1000: ELBO (train) -2205.9990539019473\nEpoch 1100: ELBO (train) -2193.5205072564117\nEpoch 1200: ELBO (train) -2182.386384394041\nEpoch 1300: ELBO (train) -2170.7290246451244\nEpoch 1400: ELBO (train) -2168.8071585796506\nEpoch 1500: ELBO (train) -2166.501338416617\nEpoch 1600: ELBO (train) -2086.7439464720615\nEpoch 1700: ELBO (train) -1925.7203989917828\nEpoch 1800: ELBO (train) -1770.8817162847786\nEpoch 1900: ELBO (train) -1715.1430828080308\nEpoch 2000: ELBO (train) -1667.457872961903\n*\nperiodic\n{'period': 0.5}\nmatern52\n{}\n\nWARNING:tensorflow:AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea32114b10>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea53b81650>, <gast.gast.Return object at 0x7fea53b81ed0>]\n2020-04-02 14:13:30 WARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea32114b10>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea53b81650>, <gast.gast.Return object at 0x7fea53b81ed0>]\nWARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea32114b10>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea53b81650>, <gast.gast.Return object at 0x7fea53b81ed0>]\nWARNING:tensorflow:AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea32114b10>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea61b5efd0>, <gast.gast.Return object at 0x7fea61b5e490>]\n2020-04-02 14:13:30 WARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea32114b10>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea61b5efd0>, <gast.gast.Return object at 0x7fea61b5e490>]\nWARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea32114b10>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea61b5efd0>, <gast.gast.Return object at 0x7fea61b5e490>]\nEpoch 100: ELBO (train) -4006.392477466465\nEpoch 200: ELBO (train) -2707.4102921770586\nEpoch 300: ELBO (train) -2368.287195081716\nEpoch 400: ELBO (train) -1879.9099639021633\nEpoch 500: ELBO (train) -1740.3226350686732\nEpoch 600: ELBO (train) -1766.7387498448286\nEpoch 700: ELBO (train) -1898.3728363182406\nEpoch 800: ELBO (train) -1650.9833344230192\nEpoch 900: ELBO (train) -1630.7925068378763\nEpoch 1000: ELBO (train) -1611.1205504103689\nEpoch 1100: ELBO (train) -1595.482308825487\nEpoch 1200: ELBO (train) -1586.215077879295\nEpoch 1300: ELBO (train) -1580.8678651868884\nEpoch 1400: ELBO (train) -1577.5032934271387\nEpoch 1500: ELBO (train) -1575.753639274707\nEpoch 1600: ELBO (train) -1574.57200259282\nEpoch 1700: ELBO (train) -1573.8091889633279\nEpoch 1800: ELBO (train) -1571.5339616881674\nEpoch 1900: ELBO (train) -1570.242688401953\nEpoch 2000: ELBO (train) -1566.6046080388073\nperiodic\n{'period': 0.5, 'lengthscale': 0.7, 'variance': 4.5}\nWARNING:tensorflow:AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea53733550>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea53751690>, <gast.gast.Return object at 0x7fea53751cd0>]\n2020-04-02 14:13:47 WARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea53733550>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea53751690>, <gast.gast.Return object at 0x7fea53751cd0>]\nWARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea53733550>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea53751690>, <gast.gast.Return object at 0x7fea53751cd0>]\nWARNING:tensorflow:AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea53733550>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea4384d5d0>, <gast.gast.Return object at 0x7fea4384ddd0>]\n2020-04-02 14:13:47 WARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea53733550>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea4384d5d0>, <gast.gast.Return object at 0x7fea4384ddd0>]\nWARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fea53733550>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fea4384d5d0>, <gast.gast.Return object at 0x7fea4384ddd0>]\nEpoch 100: ELBO (train) -4582.736569827306\nEpoch 200: ELBO (train) -2177.51902064576\nEpoch 300: ELBO (train) -2027.1796266617414\nEpoch 400: ELBO (train) -1997.329072139025\nEpoch 500: ELBO (train) -2000.2704199189272\nEpoch 600: ELBO (train) -1986.6618482834724\nEpoch 700: ELBO (train) -1980.8765964283878\nEpoch 800: ELBO (train) -1976.7140506403573\nEpoch 900: ELBO (train) -1973.4881953737759\nEpoch 1000: ELBO (train) -1970.8467407220667\nEpoch 1100: ELBO (train) -1968.3610087047477\nEpoch 1200: ELBO (train) -1963.9061180976698\nEpoch 1300: ELBO (train) -1961.1274481941482\nEpoch 1400: ELBO (train) -1959.9087395771471\nEpoch 1500: ELBO (train) -1959.629299733035\nEpoch 1600: ELBO (train) -1959.0255992919633\nEpoch 1700: ELBO (train) -1958.8430257160467\nEpoch 1800: ELBO (train) -1958.5914869499059\nEpoch 1900: ELBO (train) -1958.5034240104803\nEpoch 2000: ELBO (train) -1958.4333009117836\n"
    }
   ],
   "source": [
    "# setup parameters\n",
    "epochs = 2000\n",
    "logging_epoch_freq = 100\n",
    "M = 20      # number of inducing points\n",
    "\n",
    "# loop through list of sensor. train model for each sensor\n",
    "for i in range(len(detector_list)):\n",
    "    for kernel_id in kernel_settings:\n",
    "        detector_id = detector_list[i]\n",
    "        # get a kernel from json/dict/list\n",
    "        kernel = parse_kernel(kernel_settings[kernel_id])\n",
    "\n",
    "        # train model\n",
    "        model, Xtest = train_sensor_model(\n",
    "            X_arr[i], Y_arr[i], kernel, epochs, logging_epoch_freq, M=M\n",
    "        )\n",
    "\n",
    "        # save model and processed data to file\n",
    "        save_model_to_file(model, name, kernel_id, detector_id, xp_root=xpfp)\n",
    "        save_processed_data_to_file(X_arr[i], Y_arr[i], name, detector_id, xp_root=xpfp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cleanair",
   "language": "python",
   "name": "cleanair"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}