{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling SCOOT\n",
    "\n",
    "Load the data (clean + normalise) then run an SVGP on each sensor individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "import gpflow\n",
    "import pickle\n",
    "import calendar\n",
    "import tensorflow as tf\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from gpflow.utilities import print_summary\n",
    "\n",
    "gpflow.config.set_default_summary_fmt(\"notebook\")\n",
    "\n",
    "# cleanair modules for scoot\n",
    "from cleanair.scoot import (\n",
    "    ScootQuery,\n",
    "    parse_kernel,\n",
    "    save_model_to_file,\n",
    "    save_processed_data_to_file\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup filepaths\n",
    "\n",
    "All data, results, figures and models are held in the `EXPERIMENT_DIR/NAME` directories where `NAME` is the name of your experiment.\n",
    "\n",
    "```\n",
    "EXPERIMENT_DIR/\n",
    "    NAME/\n",
    "        data/\n",
    "            normal_scoot.csv\n",
    "            lockdown_scoot.csv\n",
    "            SCOOT_ID.npy\n",
    "        results/\n",
    "            KERNEL_ID\n",
    "                lockdown_SCOOT_ID.npy\n",
    "                normal_SCOOT_ID.npy\n",
    "            ...\n",
    "        models/\n",
    "            KERNEL_ID\n",
    "                lockdown_SCOOT_ID.m5\n",
    "                normal_SCOOT_ID.m5\n",
    "                ...\n",
    "        figures/\n",
    "            KERNEL_ID\n",
    "                lockdown_SCOOT_ID.png\n",
    "                normal_SCOOT_ID.png\n",
    "                ...\n",
    "        settings/\n",
    "            kernel_settings.json\n",
    "            scoot_settings.json\n",
    "```\n",
    " Here I'm assuming each scoot detector is trained independently. If this changes we may need to change file structure (should be ok through use of `cleanair.scoot.util` helper functions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give your experiment a useful name\n",
    "name = \"monday\"\n",
    "\n",
    "# setup filepaths\n",
    "user_settings_fp = os.path.join(\"..\", \"..\", \"terraform\", \".secrets\", \"user_settings.json\")\n",
    "with open(user_settings_fp) as json_file:\n",
    "    user_settings = json.load(json_file)\n",
    "secretfile = user_settings[\"secretfile\"]\n",
    "xpfp = user_settings[\"experiment_dir\"]  # root to experiments filepaths directory\n",
    "data_dir = os.path.join(xpfp, name, \"data\")\n",
    "results_dir = os.path.join(xpfp, name, \"results\")\n",
    "models_dir = os.path.join(xpfp, name, \"models\")\n",
    "settings_dir = os.path.join(xpfp, name, \"settings\")\n",
    "\n",
    "# make directories\n",
    "Path(os.path.join(xpfp, name)).mkdir(exist_ok=True, parents=True)\n",
    "Path(data_dir).mkdir(exist_ok=True)         # input data and processed training data\n",
    "Path(results_dir).mkdir(exist_ok=True)      # predictions from model\n",
    "Path(models_dir).mkdir(exist_ok=True)       # saving model status\n",
    "Path(settings_dir).mkdir(exist_ok=True)     # for storing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if true, all input data will be read from a local file\n",
    "read_data_from_file = True\n",
    "save_data_to_file = False\n",
    "\n",
    "# choose a start and end date for querying \"normal traffic\" period\n",
    "normal_start = \"2020-02-10 00:00:00\"\n",
    "normal_end = \"2020-02-24 00:00:00\"\n",
    "\n",
    "# choose a start and end date for querying \"lockdown traffic\" period\n",
    "lockdown_start = \"2020-03-16 00:00:00\"\n",
    "lockdown_end = \"2020-03-30 00:00:00\"\n",
    "\n",
    "# columns to analyse\n",
    "columns = [\"n_vehicles_in_interval\"]\n",
    "\n",
    "# seeds\n",
    "gpflow.config.set_default_float(np.float64)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detector readings\n",
    "\n",
    "You can read scoot either from the DB or from a local file. Make sure you have set `read_data_from_file` and `save_data_to_file` correctly before running the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_fp = os.path.join(data_dir, \"normal_scoot.csv\")\n",
    "lockdown_fp = os.path.join(data_dir, \"lockdown_scoot.csv\")\n",
    "\n",
    "if read_data_from_file:\n",
    "    # read data from csv\n",
    "    normal_df = pd.read_csv(normal_fp)\n",
    "    lockdown_df = pd.read_csv(lockdown_fp)\n",
    "else:\n",
    "    # create an object for querying from DB\n",
    "    SQ = ScootQuery(secretfile=secretfile)\n",
    "    # read the data from DB\n",
    "    normal_df = SQ.get_all_readings(\n",
    "        start_datetime=normal_start,\n",
    "        end_datetime=normal_end\n",
    "    )\n",
    "    lockdown_df = SQ.get_all_readings(\n",
    "        start_datetime=lockdown_start,\n",
    "        end_datetime=lockdown_end\n",
    "    )\n",
    "    # save the data to csv if required\n",
    "    if save_data_to_file:\n",
    "        normal_df.to_csv(normal_fp)\n",
    "        lockdown_df.to_csv(lockdown_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "    - Convert Datetime to epoch\n",
    "    - Add normalised/standardised columns\n",
    "    - Get a dataframe for only a subset of sensors and for given time range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(x):\n",
    "    \"\"\"Standardize all columns individually\"\"\"\n",
    "    return (x - np.mean(x, axis=0)) / np.std(x, axis=0)\n",
    "\n",
    "def denormalise(x, wrt_y):\n",
    "    \"\"\"Denormalize x given the original data it was standardized to\"\"\"\n",
    "    return ( x * np.std(wrt_y, axis=0) ) + np.mean(wrt_y, axis=0)\n",
    "\n",
    "def clean_and_normalise_df(df: pd.DataFrame):\n",
    "    \"\"\"Normalise lat, lon, epoch.\"\"\"\n",
    "    df['measurement_start_utc'] = pd.to_datetime(df['measurement_start_utc'])\n",
    "    df['weekday'] = df['measurement_start_utc'].dt.dayofweek\n",
    "    df['weekend'] = (df.weekday // 5 == 1).astype(float)\n",
    "    df['epoch'] = df['measurement_start_utc'].astype('int64')//1e9 #convert to epoch\n",
    "    df['epoch_norm'] = normalise(df['epoch'])\n",
    "    df['lat_norm'] = normalise(df['lat'])\n",
    "    df['lon_norm'] = normalise(df['lon'])\n",
    "    return df\n",
    "\n",
    "def filter_df(df: pd.DataFrame, detector_list: list, start: str, end: str):\n",
    "    \"\"\"\n",
    "    Return a dataframe that only contains sensors in the list\n",
    "    and only contains observations between the start and end datetime.\n",
    "    \"\"\"\n",
    "    return df.loc[\n",
    "        (df['detector_id'].isin(detector_list)) &\n",
    "        (df[\"measurement_start_utc\"] >= start) &\n",
    "        (df[\"measurement_start_utc\"] < end)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        Unnamed: 0 detector_id       lon        lat measurement_start_utc  \\\n57022        57022   N13/016a1 -0.002975  51.551367   2020-02-10 07:00:00   \n140560      140560   N13/016a1 -0.002975  51.551367   2020-02-10 18:00:00   \n90750        90750   N00/002g1 -0.102058  51.513892   2020-02-10 12:00:00   \n\n        measurement_end_utc  n_vehicles_in_interval  occupancy_percentage  \\\n57022   2020-02-10 08:00:00                      62              1.285546   \n140560  2020-02-10 19:00:00                      87              2.191358   \n90750   2020-02-10 13:00:00                     258              8.726597   \n\n        congestion_percentage  saturation  weekday  weekend         epoch  \\\n57022                0.000000   32.150000        0      0.0  1.581318e+09   \n140560              -0.037037   44.333333        0      0.0  1.581358e+09   \n90750                1.533333  123.266667        0      0.0  1.581336e+09   \n\n        epoch_norm  lat_norm  lon_norm  \n57022    -1.692983  0.773122  0.892927  \n140560   -1.578419  0.773122  0.892927  \n90750    -1.640908  0.137691  0.155247  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>detector_id</th>\n      <th>lon</th>\n      <th>lat</th>\n      <th>measurement_start_utc</th>\n      <th>measurement_end_utc</th>\n      <th>n_vehicles_in_interval</th>\n      <th>occupancy_percentage</th>\n      <th>congestion_percentage</th>\n      <th>saturation</th>\n      <th>weekday</th>\n      <th>weekend</th>\n      <th>epoch</th>\n      <th>epoch_norm</th>\n      <th>lat_norm</th>\n      <th>lon_norm</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>57022</th>\n      <td>57022</td>\n      <td>N13/016a1</td>\n      <td>-0.002975</td>\n      <td>51.551367</td>\n      <td>2020-02-10 07:00:00</td>\n      <td>2020-02-10 08:00:00</td>\n      <td>62</td>\n      <td>1.285546</td>\n      <td>0.000000</td>\n      <td>32.150000</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1.581318e+09</td>\n      <td>-1.692983</td>\n      <td>0.773122</td>\n      <td>0.892927</td>\n    </tr>\n    <tr>\n      <th>140560</th>\n      <td>140560</td>\n      <td>N13/016a1</td>\n      <td>-0.002975</td>\n      <td>51.551367</td>\n      <td>2020-02-10 18:00:00</td>\n      <td>2020-02-10 19:00:00</td>\n      <td>87</td>\n      <td>2.191358</td>\n      <td>-0.037037</td>\n      <td>44.333333</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1.581358e+09</td>\n      <td>-1.578419</td>\n      <td>0.773122</td>\n      <td>0.892927</td>\n    </tr>\n    <tr>\n      <th>90750</th>\n      <td>90750</td>\n      <td>N00/002g1</td>\n      <td>-0.102058</td>\n      <td>51.513892</td>\n      <td>2020-02-10 12:00:00</td>\n      <td>2020-02-10 13:00:00</td>\n      <td>258</td>\n      <td>8.726597</td>\n      <td>1.533333</td>\n      <td>123.266667</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1.581336e+09</td>\n      <td>-1.640908</td>\n      <td>0.137691</td>\n      <td>0.155247</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "# choose a start and end datetime to filter by\n",
    "start_normal_interval = \"2020-02-10 00:00:00\"\n",
    "# end_normal_interval = \"2020-02-17 00:00:00\"\n",
    "end_normal_interval = \"2020-02-11 00:00:00\"\n",
    "start_lockdown_interval = \"2020-03-16 00:00:00\"\n",
    "end_lockdown_interval = \"2020-03-23 00:00:00\"\n",
    "\n",
    "# get list of detectors from json file to filter by\n",
    "detector_list = [\"N00/002e1\",\"N00/002g1\",\"N13/016a1\"]\n",
    "# detector_list = list(np.unique(normal_df['detector_id']))   # all scoot detectors\n",
    "\n",
    "# clean data and normalise\n",
    "# TODO: IMPORTANT - normalisation should be same for normal and lockdown periods.\n",
    "normal_df = clean_and_normalise_df(normal_df)\n",
    "lockdown_df = clean_and_normalise_df(lockdown_df)\n",
    "\n",
    "# filter normal and lockdown dataframes by interval dates and same detectors\n",
    "normal_interval_df = filter_df(normal_df, detector_list, start_normal_interval, end_normal_interval)\n",
    "lockdown_interval_df = filter_df(lockdown_df, detector_list, start_lockdown_interval, end_lockdown_interval)\n",
    "\n",
    "normal_interval_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = \"N00/002e1\"\n",
    "day_df = normal_interval_df.loc[normal_df.detector_id == id]\n",
    "day_df = day_df.drop(day_df.columns[0], axis=1)\n",
    "day_df.to_csv(os.path.join(\"/Users\",\"pohara\", \"Data\", \"scoot_profiles\", \"10Feb_\"+id.replace(\"/\",\"_\")+\".csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save kernel and data settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split up into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-47-7e1920ed2189>, line 4)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-47-7e1920ed2189>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    def get_X(df)\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# def get_X(df):\n",
    "#     return np.array(df[['epoch_norm', 'lon_norm', 'lat_norm', 'weekday', 'weekend']])\n",
    "\n",
    "def get_X(df)\n",
    "    return np.array(df[['epoch_norm', 'lon_norm', 'lat_norm']])\n",
    "\n",
    "def get_Y(df):\n",
    "    return np.array(df[['n_vehicles_in_interval']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_group = normal_interval_df.groupby(\"detector_id\")\n",
    "lockdown_group = lockdown_interval_df.groupby(\"detector_id\")\n",
    "\n",
    "# list of dfs for all sensors\n",
    "normal_df_list = [normal_group.get_group(id) for id in detector_list] \n",
    "lockdown_df_list = [lockdown_group.get_group(id) for id in detector_list]\n",
    "\n",
    "# get list of numpy arrays for each dataframe\n",
    "X_arr = [get_X(df) for df in normal_df_list] # |Number of scoot sensors| x N_i x D\n",
    "Y_arr = [get_Y(df) for df in normal_df_list] # |Number of scoot sensors| x N_i x 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit LGCP model for each sensor\n",
    "\n",
    "The input $X$ is time epoch, lat, lon and output $Y$ is the integer `n_vehicles_in_interval`.\n",
    "\n",
    "NOTE for 2 days of scoot data there are approx 400000 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimization functions - train the model for the given epochs\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "def optimization_step(model: gpflow.models.SVGP, X, Y):\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        obj = -model.elbo(X, Y)\n",
    "        grads = tape.gradient(obj, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "def simple_training_loop(X, Y, model: gpflow.models.SVGP, epochs: int = 1, logging_epoch_freq: int = 10, num_batches_per_epoch: int = 10):\n",
    "    tf_optimization_step = tf.function(optimization_step)\n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(num_batches_per_epoch):\n",
    "            tf_optimization_step(model, X, Y)\n",
    "\n",
    "        epoch_id = epoch + 1\n",
    "        if epoch_id % logging_epoch_freq == 0:\n",
    "            tf.print(f\"Epoch {epoch_id}: ELBO (train) {model.elbo(X,Y)}\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Given the data and the specific sensor this function optimise the ELBO and plot the results \n",
    "def train_sensor_model(X, Y, kernel, epochs = 100, logging_epoch_freq = 10, M=10, inducing_point_method=\"random\"):\n",
    "    \n",
    "    ## To remove newaxis when more features\n",
    "    num_features = X[:,0][:,np.newaxis].shape[0]\n",
    "    \n",
    "    X = tf.convert_to_tensor(X[:,0][:,np.newaxis])\n",
    "    Y = tf.convert_to_tensor(Y.astype(np.float64))\n",
    "\n",
    "    # ToDo : number of rows\n",
    "    if M == X.shape[0]:\n",
    "        ind_points = X\n",
    "    elif inducing_point_method == \"random\":\n",
    "        # randomly select \n",
    "        ind_points = tf.random.shuffle(X)[:M]\n",
    "    else:\n",
    "        # select of regular grid\n",
    "        ind_points = tf.expand_dims(\n",
    "            tf.linspace(np.min(X_arr[0][:,0]), np.max(X_arr[0][:,0]), M),1\n",
    "        )\n",
    "    \n",
    "    lik = gpflow.likelihoods.Poisson()\n",
    "    \n",
    "    ## Add code for inducing inputs - Needed when we run on the full data\n",
    "    model = gpflow.models.SVGP(kernel=kernel, likelihood=lik, inducing_variable=ind_points)\n",
    "    \n",
    "    ## Uncomment to see which variables are training and those that are not\n",
    "    #print_summary(model)\n",
    "    \n",
    "    simple_training_loop(X, Y, model, epochs = epochs, \n",
    "                         logging_epoch_freq = logging_epoch_freq)\n",
    "\n",
    "    return model,X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoot_settings = dict(\n",
    "  scoot_ids=detector_list,\n",
    "  lockdown_start=start_lockdown_interval,\n",
    "  lockdown_end=end_lockdown_interval,\n",
    "  normal_start=start_normal_interval,\n",
    "  normal_end=end_normal_interval,\n",
    "  columns=columns,\n",
    ")\n",
    "\n",
    "# periodic with 0.5, lengthscale ...\n",
    "# periodic with rbf with params from virgi\n",
    "# periodic x matern32/12 shouldn't work\n",
    "# periodic with matern52 play\n",
    "# periodic with 0.5 + rbf\n",
    "kernel_settings = {  \n",
    "    \"matern52_ls=0.1_v=0.1\": {\n",
    "        \"name\":\"matern52\",\n",
    "        \"hyperparameters\":{\n",
    "            \"lengthscale\":0.1,\n",
    "            \"variance\": 0.1\n",
    "        }\n",
    "    },\n",
    "    \"matern52_ls=0.1_v=1\": {\n",
    "        \"name\":\"matern52\",\n",
    "        \"hyperparameters\":{\n",
    "            \"lengthscale\":0.1,\n",
    "            \"variance\": 1\n",
    "        }\n",
    "    },\n",
    "    \"matern52_ls=1_v=0.1\": {\n",
    "        \"name\":\"matern52\",\n",
    "        \"hyperparameters\":{\n",
    "            \"lengthscale\":1,\n",
    "            \"variance\": 0.1\n",
    "        }\n",
    "    },\n",
    "    \"matern52_ls=1_v=1\": {\n",
    "        \"name\":\"matern52\",\n",
    "        \"hyperparameters\":{\n",
    "            \"lengthscale\":1,\n",
    "            \"variance\": 1\n",
    "        }\n",
    "    }\n",
    "    # \"matern52\": {\n",
    "    #     \"name\": \"matern52\",\n",
    "    #     \"hyperparameters\": {}\n",
    "    # },\n",
    "    # \"periodic\": {               # periodic with hand tuned params\n",
    "    #     \"name\": \"periodic\",\n",
    "    #     \"hyperparameters\": {\n",
    "    #         \"period\": 0.5,\n",
    "    #         \"lengthscale\": 0.7,\n",
    "    #         \"variance\": 4.5\n",
    "    #     }\n",
    "    # },\n",
    "    # \"matern12\": {\n",
    "    #     \"name\": \"matern12\",\n",
    "    #     \"hyperparameters\": {\n",
    "            \n",
    "    #     }\n",
    "    # }\n",
    "}\n",
    "\n",
    "# get the settings for kernels and scoot data\n",
    "try:\n",
    "    with open(os.path.join(settings_dir, \"kernel_settings.json\"), \"r+\") as kernel_file:\n",
    "        current_settings = json.load(kernel_file)\n",
    "        current_settings.update(kernel_settings)\n",
    "        kernel_file.seek(0)\n",
    "        json.dump(current_settings, kernel_file)\n",
    "except FileNotFoundError:\n",
    "    with open(os.path.join(settings_dir, \"kernel_settings.json\"), \"w\") as kernel_file:\n",
    "        json.dump(kernel_settings, kernel_file)\n",
    "with open(os.path.join(settings_dir, \"scoot_settings.json\"), \"w\") as scoot_file:\n",
    "    json.dump(scoot_settings, scoot_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run entire training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "rain) -166.46951184268812\nEpoch 700: ELBO (train) -162.78494177502927\nEpoch 800: ELBO (train) -159.3672413178486\nEpoch 900: ELBO (train) -156.0258902094483\nEpoch 1000: ELBO (train) -152.95495978297325\nEpoch 1100: ELBO (train) -150.3869083111983\nEpoch 1200: ELBO (train) -148.54496892506842\nEpoch 1300: ELBO (train) -147.1850314217212\nEpoch 1400: ELBO (train) -146.26474928471086\nEpoch 1500: ELBO (train) -145.6316260312551\nEpoch 1600: ELBO (train) -145.16211414679702\nEpoch 1700: ELBO (train) -144.82272117236727\nEpoch 1800: ELBO (train) -144.60044002989312\nEpoch 1900: ELBO (train) -144.27960767794937\nEpoch 2000: ELBO (train) -144.07901144524402\nmatern32\n{'lengthscale': 1, 'variance': 1}\nWARNING:tensorflow:AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbda5ffe50>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbc08a9d810>, <gast.gast.Return object at 0x7fbc08a9db90>]\n2020-04-02 16:55:25 WARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbda5ffe50>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbc08a9d810>, <gast.gast.Return object at 0x7fbc08a9db90>]\nWARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbda5ffe50>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbc08a9d810>, <gast.gast.Return object at 0x7fbc08a9db90>]\nWARNING:tensorflow:AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbda5ffe50>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbbce505050>, <gast.gast.Return object at 0x7fbbce505a90>]\n2020-04-02 16:55:25 WARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbda5ffe50>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbbce505050>, <gast.gast.Return object at 0x7fbbce505a90>]\nWARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbda5ffe50>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbbce505050>, <gast.gast.Return object at 0x7fbbce505a90>]\nEpoch 100: ELBO (train) -217.21716118512563\nEpoch 200: ELBO (train) -197.11805031346336\nEpoch 300: ELBO (train) -177.27356412043792\nEpoch 400: ELBO (train) -172.08263668479407\nEpoch 500: ELBO (train) -168.38055733625237\nEpoch 600: ELBO (train) -160.28154194394276\nEpoch 700: ELBO (train) -153.67423811301413\nEpoch 800: ELBO (train) -151.02825896321872\nEpoch 900: ELBO (train) -150.28092548969374\nEpoch 1000: ELBO (train) -149.76728051923487\nEpoch 1100: ELBO (train) -149.2829899232171\nEpoch 1200: ELBO (train) -148.82997425872253\nEpoch 1300: ELBO (train) -148.0049149542113\nEpoch 1400: ELBO (train) -146.37906549075558\nEpoch 1500: ELBO (train) -144.4823711293978\nEpoch 1600: ELBO (train) -143.93469007960758\nEpoch 1700: ELBO (train) -143.78208636693842\nEpoch 1800: ELBO (train) -143.83003262703167\nEpoch 1900: ELBO (train) -143.565937601367\nEpoch 2000: ELBO (train) -143.48137361112168\nmatern32\n{'lengthscale': 0.1, 'variance': 0.1}\nWARNING:tensorflow:AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbcce66310>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbc093a83d0>, <gast.gast.Return object at 0x7fbbbb95c350>]\n2020-04-02 16:55:36 WARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbcce66310>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbc093a83d0>, <gast.gast.Return object at 0x7fbbbb95c350>]\nWARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbcce66310>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbc093a83d0>, <gast.gast.Return object at 0x7fbbbb95c350>]\nWARNING:tensorflow:AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbcce66310>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbbce510410>, <gast.gast.Return object at 0x7fbbce510a10>]\n2020-04-02 16:55:36 WARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbcce66310>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbbce510410>, <gast.gast.Return object at 0x7fbbce510a10>]\nWARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbcce66310>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbbce510410>, <gast.gast.Return object at 0x7fbbce510a10>]\nEpoch 100: ELBO (train) -136.984154799208\nEpoch 200: ELBO (train) -133.63402014030908\nEpoch 300: ELBO (train) -130.06458717325074\nEpoch 400: ELBO (train) -128.28886810155407\nEpoch 500: ELBO (train) -126.36734334651518\nEpoch 600: ELBO (train) -123.94800942037844\nEpoch 700: ELBO (train) -122.13963857561446\nEpoch 800: ELBO (train) -119.72501410428362\nEpoch 900: ELBO (train) -117.7436666816841\nEpoch 1000: ELBO (train) -116.26873355779769\nEpoch 1100: ELBO (train) -111.6839147452906\nEpoch 1200: ELBO (train) -109.3541347857956\nEpoch 1300: ELBO (train) -108.5225173198108\nEpoch 1400: ELBO (train) -108.03348492196804\nEpoch 1500: ELBO (train) -107.7052187465818\nEpoch 1600: ELBO (train) -107.45524669003387\nEpoch 1700: ELBO (train) -107.29241716455272\nEpoch 1800: ELBO (train) -107.22161829714071\nEpoch 1900: ELBO (train) -107.02012942764532\nEpoch 2000: ELBO (train) -106.90272525306906\nmatern32\n{'lengthscale': 0.1, 'variance': 1}\nWARNING:tensorflow:AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbd9e0f950>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbc08a8ab90>, <gast.gast.Return object at 0x7fbc08a8ac10>]\n2020-04-02 16:55:47 WARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbd9e0f950>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbc08a8ab90>, <gast.gast.Return object at 0x7fbc08a8ac10>]\nWARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbd9e0f950>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbc08a8ab90>, <gast.gast.Return object at 0x7fbc08a8ac10>]\nWARNING:tensorflow:AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbd9e0f950>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbbce5172d0>, <gast.gast.Return object at 0x7fbbce517950>]\n2020-04-02 16:55:47 WARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbd9e0f950>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbbce5172d0>, <gast.gast.Return object at 0x7fbbce517950>]\nWARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbd9e0f950>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbbce5172d0>, <gast.gast.Return object at 0x7fbbce517950>]\nEpoch 100: ELBO (train) -120.22834966011392\nEpoch 200: ELBO (train) -118.74136681553533\nEpoch 300: ELBO (train) -118.32102104058688\nEpoch 400: ELBO (train) -117.97042105760437\nEpoch 500: ELBO (train) -116.7027051061998\nEpoch 600: ELBO (train) -115.87434309368886\nEpoch 700: ELBO (train) -115.30519940720347\nEpoch 800: ELBO (train) -114.77492489142139\nEpoch 900: ELBO (train) -114.15250613463924\nEpoch 1000: ELBO (train) -109.32677484089025\nEpoch 1100: ELBO (train) -108.00079889452928\nEpoch 1200: ELBO (train) -107.6434923695849\nEpoch 1300: ELBO (train) -107.42217531181763\nEpoch 1400: ELBO (train) -107.24407099584664\nEpoch 1500: ELBO (train) -107.11553816774219\nEpoch 1600: ELBO (train) -107.00360102555496\nEpoch 1700: ELBO (train) -106.91873406801737\nEpoch 1800: ELBO (train) -106.87421630066255\nEpoch 1900: ELBO (train) -106.78717523226314\nEpoch 2000: ELBO (train) -106.7453559374103\nmatern32\n{'lengthscale': 1, 'variance': 0.1}\nWARNING:tensorflow:AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbe966be90>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbbcd1fee90>, <gast.gast.Return object at 0x7fbbcd1fe710>]\n2020-04-02 16:55:57 WARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbe966be90>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbbcd1fee90>, <gast.gast.Return object at 0x7fbbcd1fe710>]\nWARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbe966be90>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbbcd1fee90>, <gast.gast.Return object at 0x7fbbcd1fe710>]\nWARNING:tensorflow:AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbe966be90>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbbce50c150>, <gast.gast.Return object at 0x7fbbce50c750>]\n2020-04-02 16:55:58 WARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbe966be90>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbbce50c150>, <gast.gast.Return object at 0x7fbbce50c750>]\nWARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbe966be90>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbbce50c150>, <gast.gast.Return object at 0x7fbbce50c750>]\nEpoch 100: ELBO (train) -138.43320031939865\nEpoch 200: ELBO (train) -135.99481738339944\nEpoch 300: ELBO (train) -133.5859601709383\nEpoch 400: ELBO (train) -130.20352028060643\nEpoch 500: ELBO (train) -127.24913385036916\nEpoch 600: ELBO (train) -123.9362134671409\nEpoch 700: ELBO (train) -121.45066356018681\nEpoch 800: ELBO (train) -119.11135607208055\nEpoch 900: ELBO (train) -116.91657585251743\nEpoch 1000: ELBO (train) -115.53071206138526\nEpoch 1100: ELBO (train) -110.336198556283\nEpoch 1200: ELBO (train) -108.9835413016557\nEpoch 1300: ELBO (train) -108.33100700422172\nEpoch 1400: ELBO (train) -107.97394085129429\nEpoch 1500: ELBO (train) -107.63068323038286\nEpoch 1600: ELBO (train) -107.46303427492064\nEpoch 1700: ELBO (train) -107.24470406608182\nEpoch 1800: ELBO (train) -107.1092101939206\nEpoch 1900: ELBO (train) -107.00186005963425\nEpoch 2000: ELBO (train) -106.90581971278917\nmatern32\n{'lengthscale': 1, 'variance': 1}\nWARNING:tensorflow:AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbbb945ed0>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbbffce1550>, <gast.gast.Return object at 0x7fbbffce11d0>]\n2020-04-02 16:56:08 WARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbbb945ed0>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbbffce1550>, <gast.gast.Return object at 0x7fbbffce11d0>]\nWARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbbb945ed0>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbbffce1550>, <gast.gast.Return object at 0x7fbbffce11d0>]\nWARNING:tensorflow:AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbbb945ed0>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbbce517750>, <gast.gast.Return object at 0x7fbbce517d90>]\n2020-04-02 16:56:08 WARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbbb945ed0>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbbce517750>, <gast.gast.Return object at 0x7fbbce517d90>]\nWARNING: AutoGraph could not transform <bound method Poisson.variational_expectations of <gpflow.likelihoods.likelihoods.Poisson object at 0x7fbbbb945ed0>> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7fbbce517750>, <gast.gast.Return object at 0x7fbbce517d90>]\nEpoch 100: ELBO (train) -139.44268300334468\nEpoch 200: ELBO (train) -130.22758349821547\nEpoch 300: ELBO (train) -122.01986638461452\nEpoch 400: ELBO (train) -120.89127602886124\nEpoch 500: ELBO (train) -120.3183610337989\nEpoch 600: ELBO (train) -118.75075559640757\nEpoch 700: ELBO (train) -116.66295466263006\nEpoch 800: ELBO (train) -115.55817114162691\nEpoch 900: ELBO (train) -114.78077864604536\nEpoch 1000: ELBO (train) -114.13128220092473\nEpoch 1100: ELBO (train) -110.7466217791231\nEpoch 1200: ELBO (train) -107.993610245504\nEpoch 1300: ELBO (train) -107.47951191858937\nEpoch 1400: ELBO (train) -107.23836516208624\nEpoch 1500: ELBO (train) -107.09694021240355\nEpoch 1600: ELBO (train) -106.98575278780993\nEpoch 1700: ELBO (train) -106.90108282887749\nEpoch 1800: ELBO (train) -106.81836743135992\nEpoch 1900: ELBO (train) -106.76147623912777\nEpoch 2000: ELBO (train) -106.6926079985067\n"
    }
   ],
   "source": [
    "# setup parameters\n",
    "epochs = 2000\n",
    "logging_epoch_freq = 100\n",
    "M = 24      # number of inducing points\n",
    "\n",
    "# loop through list of sensor. train model for each sensor\n",
    "for i in range(len(detector_list)):\n",
    "    for kernel_id in kernel_settings:\n",
    "        detector_id = detector_list[i]\n",
    "        # get a kernel from json/dict/list\n",
    "        kernel = parse_kernel(kernel_settings[kernel_id])\n",
    "\n",
    "        # train model\n",
    "        model, Xtest = train_sensor_model(\n",
    "            X_arr[i], Y_arr[i], kernel, epochs, logging_epoch_freq, M=M\n",
    "        )\n",
    "\n",
    "        # save model and processed data to file\n",
    "        save_model_to_file(model, name, kernel_id, detector_id, xp_root=xpfp)\n",
    "        save_processed_data_to_file(X_arr[i], Y_arr[i], name, detector_id, xp_root=xpfp)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cleanair",
   "language": "python",
   "name": "cleanair"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}