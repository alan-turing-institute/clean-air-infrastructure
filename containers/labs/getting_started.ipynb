{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with cleanair\n",
    "\n",
    "This is a quick startup guide to get hands on with the data, models and visualisation tools in the cleanair repo.\n",
    "\n",
    "We recommend you copy and paste code snippets from this notebook into your own notebook to run your models and evaluate the fits.\n",
    "\n",
    "## Installation\n",
    "\n",
    "The full installation (including docker) is given in the README of this repo, but here is a quick summary:\n",
    "\n",
    "### Clone the repository\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/alan-turing-institute/clean-air-infrastructure.git\n",
    "```\n",
    "\n",
    "### Install cleanair and dependencies\n",
    "\n",
    "Create a new python 3.7 conda/pyenv virtual environment. Install the requirements then install cleanair:\n",
    "```bash\n",
    "cd clean-air-infrastructure\n",
    "git checkout -b 182_dev\n",
    "git pull origin 182_dev\n",
    "pip install -r containers/requirements.txt\n",
    "pip install -e containers\n",
    "```\n",
    "\n",
    "> Please check that pip is using the virtual environment you have setup by running `which pip`.\n",
    "\n",
    "### Jupyterlab (optional)\n",
    "\n",
    "\n",
    "Add the jupyterlab extensions for plotly, dash and widgets:\n",
    "\n",
    "```bash\n",
    "jupyter labextension install jupyterlab-dash --no-build\n",
    "jupyter labextension install jupyterlab-plotly --no-build\n",
    "jupyter labextension install @jupyter-widgets/jupyterlab-manager --no-build\n",
    "jupyter labextension install plotlywidget\n",
    "```\n",
    "\n",
    "Also check that you have [nodejs installed](https://treehouse.github.io/installation-guides/mac/node-mac.html):\n",
    "\n",
    "```bash\n",
    "node -v\n",
    "```\n",
    "\n",
    "### Test install\n",
    "\n",
    "Run the import statements below to test everything has installed.\n",
    "\n",
    "> Ignore the tensorflow warnings. We are currently using an old version of TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-28 14:42:09  WARNING: From /opt/conda/lib/python3.7/site-packages/gpflow/session_manager.py:31: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2020-02-28 14:42:09  WARNING: From /opt/conda/lib/python3.7/site-packages/gpflow/misc.py:27: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "2020-02-28 14:42:09  WARNING: From /opt/conda/lib/python3.7/site-packages/gpflow/training/tensorflow_optimizer.py:169: The name tf.train.AdadeltaOptimizer is deprecated. Please use tf.compat.v1.train.AdadeltaOptimizer instead.\n",
      "\n",
      "2020-02-28 14:42:09  WARNING: From /opt/conda/lib/python3.7/site-packages/gpflow/training/tensorflow_optimizer.py:156: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "2020-02-28 14:42:09  WARNING: From /opt/conda/lib/python3.7/site-packages/gpflow/training/tensorflow_optimizer.py:169: The name tf.train.AdagradDAOptimizer is deprecated. Please use tf.compat.v1.train.AdagradDAOptimizer instead.\n",
      "\n",
      "2020-02-28 14:42:09  WARNING: From /opt/conda/lib/python3.7/site-packages/gpflow/training/tensorflow_optimizer.py:169: The name tf.train.AdagradOptimizer is deprecated. Please use tf.compat.v1.train.AdagradOptimizer instead.\n",
      "\n",
      "2020-02-28 14:42:09  WARNING: From /opt/conda/lib/python3.7/site-packages/gpflow/training/tensorflow_optimizer.py:169: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "2020-02-28 14:42:09  WARNING: From /opt/conda/lib/python3.7/site-packages/gpflow/training/tensorflow_optimizer.py:169: The name tf.train.FtrlOptimizer is deprecated. Please use tf.compat.v1.train.FtrlOptimizer instead.\n",
      "\n",
      "2020-02-28 14:42:09  WARNING: From /opt/conda/lib/python3.7/site-packages/gpflow/training/tensorflow_optimizer.py:169: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n",
      "2020-02-28 14:42:09  WARNING: From /opt/conda/lib/python3.7/site-packages/gpflow/training/tensorflow_optimizer.py:169: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n",
      "\n",
      "2020-02-28 14:42:09  WARNING: From /opt/conda/lib/python3.7/site-packages/gpflow/training/tensorflow_optimizer.py:169: The name tf.train.ProximalAdagradOptimizer is deprecated. Please use tf.compat.v1.train.ProximalAdagradOptimizer instead.\n",
      "\n",
      "2020-02-28 14:42:09  WARNING: From /opt/conda/lib/python3.7/site-packages/gpflow/training/tensorflow_optimizer.py:169: The name tf.train.ProximalGradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.ProximalGradientDescentOptimizer instead.\n",
      "\n",
      "2020-02-28 14:42:09  WARNING: From /opt/conda/lib/python3.7/site-packages/gpflow/training/tensorflow_optimizer.py:169: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\n",
      "2020-02-28 14:42:09  WARNING: From /opt/conda/lib/python3.7/site-packages/gpflow/saver/coders.py:80: The name tf.data.Iterator is deprecated. Please use tf.compat.v1.data.Iterator instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check that all of your imports are working\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import ipywidgets as ipw\n",
    "import logging\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from cleanair.models import ModelData\n",
    "from cleanair.models import SVGP\n",
    "from cleanair import metrics\n",
    "from cleanair.dashboard import timeseries\n",
    "from cleanair.dashboard.components import ModelFitComponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We need to add some files and configs before you can start running files.\n",
    "\n",
    "### DB credentials\n",
    "\n",
    "You will need to create a local secrets file. Run the following to create a file with the database secrets:\n",
    "```bash\n",
    "mkdir -p terraform/.secrets\n",
    "touch terraform/.secrets/db_secrets.json\n",
    "echo '{\n",
    "    \"username\": \"<db_admin_username>@<db_server_name>\",\n",
    "    \"password\": \"<db_admin_password>\",\n",
    "    \"host\": \"<db_server_name>.postgres.database.azure.com\",\n",
    "    \"port\": 5432,\n",
    "    \"db_name\": \"<dbname>\",\n",
    "    \"ssl_mode\": \"require\"\n",
    "}' >> terraform/.secrets/db_secrets.json\n",
    "```\n",
    "\n",
    "Open the file and replace the <> with the secret values which can be found in the keyvault in the `RG_CLEANAIR_INFRASTRUCTURE` Azure resource group. If you don't have access to the vault, ask someone in the cleanair team to help you out.\n",
    "\n",
    "> At this point you should have enough to start the `run_model_fitting.py` entrypoint\n",
    "\n",
    "### Get some data\n",
    "\n",
    "Ask Patrick to send you a sample of data.\n",
    "\n",
    "### Parser config\n",
    "\n",
    "We recommend you store some default settings when you intend to run models locally. Put these settings in the `config.json` file in your secrets folder:\n",
    "```bash\n",
    "touch terraform/.secrets/config.json\n",
    "echo '{\n",
    "    \"config_dir\": \"<DATA_DIRECTORY>\",\n",
    "    \"results_dir\": \"<DATA_DIRECTORY>\",\n",
    "    \"no_db_write\": true,\n",
    "    \"predict_write\": true,\n",
    "    \"local_read\": true,\n",
    "    \"local_write\": true,\n",
    "    \"tag\": \"<INSERT_YOUR_TAG>\",\n",
    "    \"return_y\": true,\n",
    "    \"predict_training\": false,\n",
    "    \"predict_read_local\": true\n",
    "}' >> terraform/.secrets/config.json\n",
    "```\n",
    "\n",
    "Make sure to change `<DATA_DIRECTORY>` and `<INSERT_YOUR_TAG>`. The data directory should be the absolute filepath to your data store. The tag should be a name that you give your model fits (useful for later when uploading results to the database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to your secrets directory\n",
    "secrets_dir = \"../../.secrets/\"\n",
    "\n",
    "# open the parser config\n",
    "with open(os.path.join(secrets_dir, \"config.json\"), \"r\") as filepath:\n",
    "    parser_config = json.load(filepath)\n",
    "\n",
    "# setup your filepaths\n",
    "data_dir = parser_config[\"config_dir\"]\n",
    "results_dir = parser_config[\"results_dir\"]\n",
    "secretfile=os.path.join(secrets_dir, \"db_secrets.json\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ModelData class\n",
    "\n",
    "The ModelData class is the interface between your data and your model. It has methods and constructors for reading and writing data/predictions from files and databases. It abstracts away a lot of the details so that you (hopefully) don't have to worry about data processing.\n",
    "\n",
    "The below code snippet shows how to initialise a ModelData object by reading from a data directory called `config_dir` and from the `db_secrets.json` file. Make sure your `config.json` file correctly initialised before running the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-28 14:42:09     INFO: Database connection information loaded from None\n"
     ]
    }
   ],
   "source": [
    "# read input data from a directory instead of the database\n",
    "if parser_config[\"local_read\"]:\n",
    "    model_data = ModelData(config_dir=data_dir, secretfile=secretfile)\n",
    "else:\n",
    "    raise NotImplementedError(\"Reading from database is not supported in this notebook yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model\n",
    "\n",
    "Below is an example of a simple SVGP (that doesn't perform very well). You can access and change the parameters of the model by changing the `model_params` attribute (which can also be passed as an argument to the constructor).\n",
    "\n",
    "All models in the cleanair repo inherit from the base class `cleanair.models.Model`. This base class is documented and describes the data format passed to the `fit` and `predict` methods. \n",
    "\n",
    "If you want to change the SVGP, then you should be able to create a new class that inherits from SVGP and overwrites the appropriate methods. The `cleanair.models.SVGP` class inherits from the `Model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default model params:\n",
      "{\n",
      "    \"jitter\": 1e-05,\n",
      "    \"likelihood_variance\": 0.1,\n",
      "    \"minibatch_size\": 100,\n",
      "    \"n_inducing_points\": 2000,\n",
      "    \"restore\": false,\n",
      "    \"train\": true,\n",
      "    \"model_state_fp\": null,\n",
      "    \"maxiter\": 100,\n",
      "    \"kernel\": {\n",
      "        \"name\": \"mat32+linear\",\n",
      "        \"variance\": 0.1,\n",
      "        \"lengthscale\": 0.1\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# run with the default SVGP\n",
    "model = SVGP()\n",
    "\n",
    "# get the default parameters\n",
    "print(\"Default model params:\")\n",
    "print(json.dumps(model.model_params, indent=4))\n",
    "\n",
    "# change a parameter\n",
    "model.model_params[\"maxiter\"] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laqn []\n",
      "0 :  48\n",
      "1 :  48\n",
      "2 :  48\n",
      "3 :  48\n",
      "4 :  48\n",
      "5 :  48\n",
      "6 :  48\n",
      "7 :  48\n",
      "8 :  48\n",
      "9 :  48\n",
      "10 :  48\n",
      "11 :  48\n",
      "12 :  48\n",
      "13 :  48\n",
      "14 :  48\n",
      "15 :  48\n",
      "16 :  48\n",
      "17 :  48\n",
      "18 :  48\n",
      "19 :  48\n",
      "20 :  48\n",
      "21 :  48\n",
      "22 :  48\n",
      "23 :  48\n",
      "24 :  48\n",
      "25 :  48\n",
      "26 :  48\n",
      "27 :  48\n",
      "28 :  48\n",
      "29 :  48\n",
      "30 :  48\n",
      "31 :  48\n",
      "32 :  48\n",
      "33 :  48\n",
      "34 :  48\n",
      "35 :  48\n",
      "36 :  48\n",
      "37 :  48\n",
      "38 :  48\n",
      "39 :  48\n",
      "40 :  48\n",
      "41 :  48\n",
      "42 :  48\n",
      "43 :  48\n",
      "44 :  48\n",
      "45 :  48\n",
      "46 :  48\n",
      "47 :  48\n",
      "{'laqn': {'NO2': {'mean': array([[0.53808959],\n",
      "       [0.61048998],\n",
      "       [0.68293151],\n",
      "       ...,\n",
      "       [2.55365583],\n",
      "       [2.62612424],\n",
      "       [2.69859265]]), 'var': array([[1.04601679],\n",
      "       [1.07076113],\n",
      "       [1.09427337],\n",
      "       ...,\n",
      "       [2.40804219],\n",
      "       [2.46603607],\n",
      "       [2.52486315]])}}}\n"
     ]
    }
   ],
   "source": [
    "# get the data into the right format (dicts)\n",
    "train_dict = model_data.get_training_data_arrays()\n",
    "pred_dict = model_data.get_pred_data_arrays()\n",
    "x_train, y_train = train_dict['X'], train_dict['Y']\n",
    "x_test = pred_dict['X']\n",
    "\n",
    "# fit the model on training set\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# predict on testing set\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the predictions\n",
    "\n",
    "Finally, we need to update the ModelData object with our predictions then write the results to a pickle file.\n",
    "\n",
    "In the validation notebook, we will show how to visualise the results of your model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to update the model data object with the predictions\n",
    "model_data.update_training_df_with_preds(y_pred, datetime.now())\n",
    "\n",
    "# we could write the results to the database, but for now we are going to write to a file\n",
    "pred_filepath = os.path.join(parser_config[\"results_dir\"], \"test_pred.pickle\")\n",
    "with open(pred_filepath, \"wb\") as handle:\n",
    "    pickle.dump(y_pred, handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
