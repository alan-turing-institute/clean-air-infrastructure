{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with cleanair\n",
    "\n",
    "This is a quick startup guide to get hands on with the data, models and visualisation tools in the cleanair repo.\n",
    "\n",
    "We recommend you copy and paste code snippets from this notebook into your own notebook to run your models and evaluate the fits.\n",
    "\n",
    "## Installation\n",
    "\n",
    "The full installation (including docker) is given in the README of this repo, but here is a quick summary:\n",
    "\n",
    "### Clone the repository\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/alan-turing-institute/clean-air-infrastructure.git\n",
    "```\n",
    "\n",
    "### Install cleanair and dependencies\n",
    "\n",
    "Create a new python 3.7 conda/pyenv virtual environment. Install the requirements then install cleanair:\n",
    "```bash\n",
    "cd clean-air-infrastructure\n",
    "git checkout -b 182_dev\n",
    "git pull origin 182_dev\n",
    "pip install -r containers/requirements.txt\n",
    "pip install -e containers\n",
    "```\n",
    "\n",
    "> Please check that pip is using the virtual environment you have setup by running `which pip`.\n",
    "\n",
    "### Jupyterlab (optional)\n",
    "\n",
    "\n",
    "Add the jupyterlab extensions for plotly, dash and widgets:\n",
    "\n",
    "```bash\n",
    "jupyter labextension install jupyterlab-dash --no-build\n",
    "jupyter labextension install jupyterlab-plotly --no-build\n",
    "jupyter labextension install @jupyter-widgets/jupyterlab-manager --no-build\n",
    "jupyter labextension install plotlywidget\n",
    "```\n",
    "\n",
    "Also check that you have [nodejs installed](https://treehouse.github.io/installation-guides/mac/node-mac.html):\n",
    "\n",
    "```bash\n",
    "node -v\n",
    "```\n",
    "\n",
    "### Test install\n",
    "\n",
    "Run the import statements below to test everything has installed.\n",
    "\n",
    "> Ignore the tensorflow warnings. We are currently using an old version of TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that all of your imports are working\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "from cleanair.models import ModelData\n",
    "from cleanair.models import SVGP\n",
    "from cleanair import metrics\n",
    "from cleanair.dashboard import timeseries\n",
    "from cleanair.dashboard.components import ModelFitComponent\n",
    "from cleanair.instance import ValidationInstance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We need to add some files and configs before you can start running files.\n",
    "\n",
    "### DB credentials\n",
    "\n",
    "You will need to create a local secrets file. Run the following to create a file with the database secrets:\n",
    "```bash\n",
    "mkdir -p terraform/.secrets\n",
    "touch terraform/.secrets/db_secrets.json\n",
    "echo '{\n",
    "    \"username\": \"<db_admin_username>@<db_server_name>\",\n",
    "    \"password\": \"<db_admin_password>\",\n",
    "    \"host\": \"<db_server_name>.postgres.database.azure.com\",\n",
    "    \"port\": 5432,\n",
    "    \"db_name\": \"<dbname>\",\n",
    "    \"ssl_mode\": \"require\"\n",
    "}' >> terraform/.secrets/db_secrets.json\n",
    "```\n",
    "\n",
    "Open the file and replace the <> with the secret values which can be found in the keyvault in the `RG_CLEANAIR_INFRASTRUCTURE` Azure resource group. If you don't have access to the vault, ask someone in the cleanair team to help you out.\n",
    "\n",
    "> At this point you should have enough to start the `run_model_fitting.py` entrypoint\n",
    "\n",
    "### Get some data\n",
    "\n",
    "Ask Patrick to send you a sample of data. Alternatively if you have access to the DB then you can request your own data.\n",
    "\n",
    "### Parser config\n",
    "\n",
    "We recommend you store some default settings when you intend to run models locally. Put these settings in the `config.json` file in your secrets folder:\n",
    "```bash\n",
    "touch terraform/.secrets/config.json\n",
    "echo '{\n",
    "    \"secretfile\": \"../../terraform/.secrets/db_secrets.json\",\n",
    "    \"config_dir\": <DATA_DIRECTORY>,\n",
    "    \"results_dir\": <RESULTS_DIRECTORY>,\n",
    "    \"model_dir\": <MODEL_DIRECTORY>,\n",
    "    \"no_db_write\": false,\n",
    "    \"predict_write\": true,\n",
    "    \"local_read\": false,\n",
    "    \"local_write\": false,\n",
    "    \"write_model_params\": false,\n",
    "    \"read_model_params\": false,\n",
    "    \"tag\": \"validation\",\n",
    "    \"predict_training\": false,\n",
    "    \"predict_read_local\": false,\n",
    "    \"include_prediction_y\": false,\n",
    "    \"model_name\": \"svgp\",\n",
    "    \"cluster_id\": <CLUSTER_ID>,\n",
    "    \"trainend\": \"2020-02-19T00:00:00\",\n",
    "    \"predstart\": \"2020-02-19T00:00:00\"\n",
    "}' >> terraform/.secrets/config.json\n",
    "```\n",
    "\n",
    "Make sure to change `<DATA_DIRECTORY>`, `<RESULTS_DIRECTORY>` and `<MODEL_DIRECTORY>` to valid directories if you wish to read and write from files. The filepaths should be absolute. Also change `<CLUSTER_ID>` to the name of the machine you are running the models on, e.g. `patrick_macbookpro`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to your secrets directory\n",
    "secrets_dir = \"../../terraform/.secrets/\"\n",
    "\n",
    "# open the parser config\n",
    "with open(os.path.join(secrets_dir, \"config.json\"), \"r\") as filepath:\n",
    "    parser_config = json.load(filepath)\n",
    "\n",
    "# setup your filepaths\n",
    "data_dir = parser_config[\"config_dir\"]\n",
    "results_dir = parser_config[\"results_dir\"]\n",
    "secretfile=os.path.join(secrets_dir, \"db_secrets.json\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an instance\n",
    "\n",
    "An instance is a model + data + parameters + settings. You can quickly create an instance object by passing the `model_params`, `experiment_config` and `data_config` dictionaries and selecting a model with the `model_name` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-23 11:48:28     INFO: Database connection information loaded from None\n"
     ]
    }
   ],
   "source": [
    "# create a sparse variational GP\n",
    "model_name = \"svgp\"\n",
    "\n",
    "# change the parameters of the model\n",
    "model_params = dict(\n",
    "    ValidationInstance.DEFAULT_MODEL_PARAMS,\n",
    "    maxiter=10,   # you can change or add individual params\n",
    ")\n",
    "\n",
    "# change the settings for loading data\n",
    "data_config = dict(\n",
    "    ValidationInstance.DEFAULT_DATA_CONFIG,\n",
    "    include_satellite=False,   # turn off satellite data\n",
    ")\n",
    "\n",
    "# update the experiment confing with settings store in your config file\n",
    "experiment_config = ValidationInstance.DEFAULT_EXPERIMENT_CONFIG.copy()\n",
    "experiment_config.update(parser_config)\n",
    "\n",
    "# create the instance using the dictionaries\n",
    "instance = ValidationInstance(\n",
    "    data_config=data_config,\n",
    "    experiment_config=experiment_config,\n",
    "    model_params=model_params,\n",
    "    model_name=model_name,\n",
    "    tag=\"validation\",\n",
    "    cluster_id=parser_config[\"cluster_id\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the instance is created you can setup the model, load the data, train the model, predict on the test set and save the results all by calling the `run()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_start_date': datetime.datetime(2020, 1, 29, 0, 0), 'train_end_date': datetime.datetime(2020, 1, 30, 0, 0), 'pred_start_date': datetime.datetime(2020, 1, 30, 0, 0), 'pred_end_date': datetime.datetime(2020, 1, 31, 0, 0), 'include_satellite': False, 'include_prediction_y': True, 'train_sources': ['laqn'], 'pred_sources': ['laqn'], 'train_interest_points': 'all', 'train_satellite_interest_points': 'all', 'pred_interest_points': 'all', 'species': ['NO2'], 'features': ['value_1000_total_a_road_length', 'value_500_total_a_road_length', 'value_500_total_a_road_primary_length', 'value_500_total_b_road_length'], 'norm_by': 'laqn'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-23 11:48:34     INFO: Setting up model.\n",
      "2020-03-23 11:48:34     INFO: Writing model parameters to json file.\n",
      "2020-03-23 11:48:34     INFO: Writing model parameters to a json file.\n",
      "2020-03-23 11:48:34     INFO: Inserting 1 row into the model table.\n",
      "2020-03-23 11:48:34     INFO: Reading from local file.\n",
      "2020-03-23 11:48:34     INFO: Database connection information loaded from None\n",
      "2020-03-23 11:48:37     INFO: State files saved to /Users/pohara/documents/tests/laqn_test_instance/\n",
      "2020-03-23 11:48:38     INFO: State files saved to /Users/pohara/documents/tests/laqn_test_instance/\n",
      "2020-03-23 11:48:38     INFO: Inserting 1 row into data config table.\n",
      "2020-03-23 11:48:38     INFO: Training started.\n",
      "2020-03-23 11:48:38     INFO: Training the model for 10 iterations.\n",
      "/Users/pohara/.pyenv/versions/3.7.6/envs/cleanair3.7/lib/python3.7/site-packages/scipy/cluster/vq.py:579: UserWarning:\n",
      "\n",
      "One of the clusters is empty. Re-run kmeans with a different initialization.\n",
      "\n",
      "2020-03-23 11:48:40     INFO: Training ended.\n",
      "2020-03-23 11:48:40     INFO: Inserting 1 record into the instance table.\n",
      "2020-03-23 11:48:40     INFO: Starting prediction.\n",
      "2020-03-23 11:48:40     INFO: Prediction started.\n",
      "2020-03-23 11:48:40     INFO: Start batch prediction.\n",
      "2020-03-23 11:48:41     INFO: Prediction finished.\n",
      "2020-03-23 11:48:41     INFO: Writing predictions to file.\n",
      "2020-03-23 11:48:41     INFO: Writing predictions to the database.\n",
      "2020-03-23 11:48:41     INFO: Inserting 4800 records into the result table.\n",
      "2020-03-23 11:48:42     INFO: Tag is validation\n",
      "2020-03-23 11:48:42     INFO: Model name is svgp\n",
      "2020-03-23 11:48:42     INFO: Param id is 4fd528a7397b321fc2d89cdd9f665113638fe80ecdc442761cd2439d8e901a95\n",
      "2020-03-23 11:48:42     INFO: Data id is 1f1fcf2c2c5288dfc87c0ec6210011638b6dbd6c916d6b1e2803015ab5934bf4\n",
      "2020-03-23 11:48:42     INFO: Instance id is 5f8acaa55ab363f34b19faebd1c75564f4fb448b511d74ff6bdf014d987a6cfa\n",
      "2020-03-23 11:48:42     INFO: Cluster id is patrick_laptop\n"
     ]
    }
   ],
   "source": [
    "print(instance.data_config)\n",
    "instance.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
